// Copyright 2024-2025 The Jule Programming Language.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

// This file contains the source code of the built-in map type.
// The built-in map type implementation is typically a hashmap.
// It is not lock-free in terms of concurrency, that is,
// it does not offer a thread-safe implementation.
// Uses the [maphash] function to hash keys.
// An empty initialization literal is valid and equals to nil map.
// To make it pass-by-reference, compiler implements map instances using with smart pointers.
// So, typically a nil map actually is a nil smart pointer.
//
// Implementation adopted from Go port of the Abseil's SwissTable.
// Source repository: https://github.com/dolthub/swiss, commit [f4b2bab].
// But the implementation is not same as repository.
// Optimized for the Jule runtime and compiler.
//
//   Copyright 2024 The Jule Programming Language
//
//   Licensed under the Apache License, Version 2.0 (the "License");
//   you may not use this file except in compliance with the License.
//   You may obtain a copy of the License at
//
//       http://www.apache.org/licenses/LICENSE-2.0
//
//   Unless required by applicable law or agreed to in writing, software
//   distributed under the License is distributed on an "AS IS" BASIS,
//   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
//   See the License for the specific language governing permissions and
//   limitations under the License.
//
// Google's C++ implementation:
//
//	https://github.com/abseil/abseil-cpp/blob/master/absl/container/internal/raw_hash_set.h
//
//	See also:
//		https://abseil.io/about/design/swisstables.
//		https://faultlore.com/blah/hashbrown-tldr/.
//

use "std/math/bits"

const (
	groupSize       = 8
	maxAvgGroupLoad = 7

	loBits = 0x0101010101010101
	hiBits = 0x8080808080808080
)

// 57-bit hash prefix
type h1 = u64

// 7-bit hash suffix
type h2 = i8

// This is a single control byte, which can have one of three
// states: empty, deleted, and full (which has an associated seven-bit h2 value). They have the following bit patterns:
//
//      empty: 1 0 0 0 0 0 0 0
//    deleted: 1 1 1 1 1 1 1 0
//       full: 0 h h h h h h h  // h represents the hash bits.
type ctrl: i8

// A bitset array value which is sets all ctrl elements to empty.
// For empty ctrl bits, see the documentation of ctrl.
const bitsetEmpty = 0b10000000_10000000_10000000_10000000_10000000_10000000_10000000_10000000

// The h2 bitset array for a group. Aka [8]ctrl type in bits.
// Find operations first probe the controls bytes
// to filter candidates before matching keys.
struct bitset {
	bits: u64
}

impl bitset {
	static fn hasZeroByte(x: u64): bitset {
		ret bitset{((x - loBits) & ^(x)) & hiBits}
	}

	// Returns length.
	fn len(self): u32 { ret 8 }

	// Returns the ctrl by offset i.
	fn at(mut self, i: u32): ctrl {
		ret unsafe { *((*ctrl)(&self.bits) + uint(i)) }
	}

	// Sets ctrl to c by offset i.
	fn set(mut self, i: u32, c: ctrl) {
		unsafe { *((*ctrl)(&self.bits) + uint(i)) = c }
	}

	// Sets all ctrls to empty.
	fn clear(mut self) {
		self.bits = bitsetEmpty
	}

	// Returns the set of bits which are full and for which the 7-bit hash
	// matches the given value. May return false positives.
	fn metaMatchH2(self, h: h2): bitset {
		// For the technique, see:
		// http://graphics.stanford.edu/~seander/bithacks.html##ValueInWord
		// (Determine if a word has a byte equal to n).
		//
		// NB: This generic matching routine produces false positive matches when
		// h is 2^N and the control bytes have a seq of 2^N followed by 2^N+1. For
		// example: if ctrls==0x0302 and h=02, we'll compute v as 0x0100. When we
		// subtract off 0x0101 the first 2 bytes we'll become 0xffff and both be
		// considered matches of h. The false positive matches are not a problem,
		// just a rare inefficiency. Note that they only occur if there is a real
		// match and never occur on empty, or tombstone. The subsequent key
		// comparisons ensure that there is no correctness issue.
		v := self.bits ^ (loBits * u64(h))
		ret bitset.hasZeroByte(v)
	}

	fn nextMatch(mut self): u32 {
		s := u32(bits::TrailingZeros64(self.bits))
		self.bits &= ^(1 << s) // clear bit |s|
		ret s >> 3 // div by 8
	}

	// Returns the set of bits in the group that are empty.
	fn metaMatchEmpty(self): bitset {
		//    empty: 1 0 0 0 0 0 0 0
		//  deleted: 1 1 1 1 1 1 1 0
		//     full: 0 h h h h h h h  // h represents the hash bits.
		ret bitset.hasZeroByte(self.bits ^ hiBits)
	}
}

fn initBitsets(mut bs: []bitset) {
	for i in bs {
		bs[i].bits = bitsetEmpty
	}
}

// Returns the minimum number of groups needed to store |n| elems.
fn numGroups(n: u32): (groups: u32) {
	groups = (n + maxAvgGroupLoad - 1) / maxAvgGroupLoad
	if groups == 0 {
		groups = 1
	}
	ret
}

fn splitHash(h: u64): (h1, h2) {
	ret h1((h & h1Mask) >> 7), h2(h & h2Mask)
}

fn probeStart(hi: h1, groups: int): u32 {
	ret fastModN(u32(hi), u32(groups))
}

const (
	h1Mask          = 0xffff_ffff_ffff_ff80
	h2Mask          = 0x0000_0000_0000_007f
	empty:     ctrl = -128 // 0b1000_0000
	tombstone: ctrl = -2   // 0b1111_1110
)

// Default initial size of a map.
const mapInitialSize = 16

// Group of 16 key-value pairs.
struct group[K: comparable, V] {
	keys:   [groupSize]K
	values: [groupSize]V
}

// Implementation of the built-in map type of Jule.
// Instantiated by compiler for each combination of key and value.
struct _Map[K: comparable, V] {
	seed:     uintptr
	ctrl:     []bitset
	groups:   []group[K, V]
	resident: u32
	dead:     u32
	limit:    u32
}

impl _Map {
	// Return non-nil map instance for [K, V] pair.
	static fn make(): &_Map[K, V] {
		ret &_Map[K, V]{
			seed: uintptr(rand()),
		}
	}

	fn initData(mut self, cap: u32) {
		groups := numGroups(cap)
		self.ctrl = make([]bitset, groups)
		initBitsets(self.ctrl)
		self.groups = make([]group[K, V], groups)
		self.limit = groups * maxAvgGroupLoad
	}

	// Returns hash for key.
	fn hash(self, &key: K): u64 {
		ret u64(maphash(key, self.seed))
	}

	fn rehash(mut self, n: u32) {
		if self.groups == nil {
			// no need for rehashing just handle nil data
			self.initData(mapInitialSize)
			ret
		}

		mut groups, ctrl := self.groups, self.ctrl
		self.groups = make([]group[K, V], n)
		self.ctrl = make([]bitset, n)
		initBitsets(self.ctrl)
		self.limit = n * maxAvgGroupLoad
		self.resident, self.dead = 0, 0
		for g in ctrl {
			mut ctrls := ctrl[g]
			mut s := u32(0)
			for s < ctrls.len(); s++ {
				c := ctrls.at(s)
				if c != empty && c != tombstone {
					self.uncheckedSet(groups[g].keys[s], groups[g].values[s])
				}
			}
		}
	}

	fn nextSize(self): (n: u32) {
		if self.groups == nil {
			ret mapInitialSize
		}
		n = u32(len(self.groups)) << 2
		if self.dead >= self.resident>>1 {
			n = u32(len(self.groups))
		}
		ret
	}

	// Returns the |value| mapped by |key| if one exists.
	// Sets |value| if pointer is not nil. If not found, sets to default value of V.
	// Same as the |ok| variable which reports whether |value| exist.
	fn lookup(mut &self, mut key: K, mut &value: V, mut &ok: bool) {
		// References are safe, but this method may used by the compiler unsafely.
		// And we have to support nil references for such a low-level use.
		// So, If the |value| and |ok| reference not implemented as is nil raw pointer
		// by the compiler, set it. Otherwise, do not touch it. Be safe.
		//
		// To determine whether |value| and |ok| is nil raw pointer behind the scene,
		// take raw pointer and compare it with nil. Since taking raw pointer of
		// the references will result as actual pointer of them, we can determine
		// whether the reference's raw pointer is nil.
		if self == nil || self.groups == nil {
			if &ok != nil {
				ok = false
			}
			if &value != nil {
				let mut def: V
				value = def
			}
			ret
		}
		hi, lo := splitHash(self.hash(key))
		mut g := probeStart(hi, len(self.groups))
		for { // inlined find loop
			mut matches := self.ctrl[g].metaMatchH2(lo)
			for matches.bits != 0 {
				s := matches.nextMatch()
				if key == self.groups[g].keys[s] {
					if &value != nil {
						value = self.groups[g].values[s]
					}
					if &ok != nil {
						ok = true
					}
					ret
				}
			}
			// |key| is not in group |g|, stop probing if we see an empty slot
			matches = self.ctrl[g].metaMatchEmpty()
			if matches.bits != 0 {
				if &ok != nil {
					ok = false
				}
				if &value != nil {
					let mut def: V
					value = def
				}
				ret
			}
			g++ // linear probing
			if g >= u32(len(self.groups)) {
				g = 0
			}
		}
		panic("unreachable")
	}

	// Returns value of key if exist, otherwise returns default value of value type.
	fn get(mut &self, mut key: K): (value: V) {
		if self != nil {
			mut ok := false
			self.lookup(key, value, ok)
		}
		ret
	}

	// Inserts |key| and |value|.
	// Assumes |key| is already not exist in the map and
	// we have enough space to add new entry.
	fn uncheckedSet(mut self, mut key: K, mut value: V) {
		hi, lo := splitHash(self.hash(key))
		mut g := probeStart(hi, len(self.groups))
		for { // inlined find loop
			// We assume |key| is not in group |g|,
			// stop probing if we see an empty slot.
			mut matches := self.ctrl[g].metaMatchEmpty()
			if matches.bits != 0 { // insert
				s := matches.nextMatch()
				self.groups[g].keys[s] = key
				self.groups[g].values[s] = value
				self.ctrl[g].set(s, ctrl(lo))
				self.resident++
				ret
			}
			g++ // linear probing
			if g >= u32(len(self.groups)) {
				g = 0
			}
		}
		panic("unreachable")
	}

	// Attempts to insert |key|.
	// Returns pointer to value of inserted or already exist |key|.
	fn set(mut self, mut key: K): (value: *V) {
		if self.resident >= self.limit {
			self.rehash(self.nextSize())
		}
		hi, lo := splitHash(self.hash(key))
		mut g := probeStart(hi, len(self.groups))
		for { // inlined find loop
			mut matches := self.ctrl[g].metaMatchH2(lo)
			for matches.bits != 0 {
				s := matches.nextMatch()
				if key == self.groups[g].keys[s] { // update
					self.groups[g].keys[s] = key
					value = &self.groups[g].values[s]
					ret
				}
			}
			// |key| is not in group |g|,
			// stop probing if we see an empty slot
			matches = self.ctrl[g].metaMatchEmpty()
			if matches.bits != 0 { // insert
				s := matches.nextMatch()
				self.groups[g].keys[s] = key
				value = &self.groups[g].values[s]
				self.ctrl[g].set(s, ctrl(lo))
				self.resident++
				ret
			}
			g++ // linear probing
			if g >= u32(len(self.groups)) {
				g = 0
			}
		}
		panic("unreachable")
	}

	// Attempts to remove |key|, returns true successful.
	fn del(mut &self, mut key: K): (ok: bool) {
		if self == nil || self.groups == nil {
			ret false
		}
		hi, lo := splitHash(self.hash(key))
		mut g := probeStart(hi, len(self.groups))
		for {
			mut matches := self.ctrl[g].metaMatchH2(lo)
			for matches.bits != 0 {
				s := matches.nextMatch()
				if key == self.groups[g].keys[s] {
					ok = true
					// optimization: if |self.ctrl[g]| contains any empty
					// metadata bytes, we can physically delete |key|
					// rather than placing a tombstone.
					// The observation is that any probes into group |g|
					// would already be terminated by the existing empty
					// slot, and therefore reclaiming slot |s| will not
					// cause premature termination of probes into |g|.
					if self.ctrl[g].metaMatchEmpty().bits != 0 {
						self.ctrl[g].set(s, empty)
						self.resident--
					} else {
						self.ctrl[g].set(s, tombstone)
						self.dead++
					}
					let mut defKey: K
					let mut defValue: V
					self.groups[g].keys[s] = defKey
					self.groups[g].values[s] = defValue
					if self.len() == 0 {
						// Reset the hash seed to make it more difficult for attackers to
						// repeatedly trigger hash collisions.
						// See: https://github.com/golang/go/issues/25237
						self.seed = uintptr(rand())
					}
					ret
				}
			}
			// |key| is not in group |g|,
			// stop probing if we see an empty slot
			matches = self.ctrl[g].metaMatchEmpty()
			if matches.bits != 0 { // |key| absent
				ok = false
				ret
			}
			g++ // linear probing
			if g >= u32(len(self.groups)) {
				g = 0
			}
		}
		panic("unreachable")
	}

	// Removes all elements from the Map.
	fn clear(mut &self) {
		if self == nil {
			ret
		}
		for i in self.ctrl {
			self.ctrl[i].clear()
		}
		let mut key: K
		let mut value: V
		for i in self.groups {
			mut &g := unsafe { *(&self.groups[i]) }
			for j in g.keys {
				g.keys[j] = key
				g.values[j] = value
			}
		}
		self.resident, self.dead = 0, 0
		// Reset the hash seed to make it more difficult for attackers to
		// repeatedly trigger hash collisions.
		// See: https://github.com/golang/go/issues/25237
		self.seed = uintptr(rand())
	}

	// Returns the number of elements in the map.
	fn len(&self): int {
		if self == nil {
			ret 0
		}
		ret int(self.resident - self.dead)
	}

	// Returns the number of additional elements
	// the can be added before resizing.
	fn cap(self): int { ret int(self.limit - self.resident) }

	fn iterator(mut &self): mapIterator[K, V] {
		mut iterator := mapIterator[K, V]{m: self}
		iterator.init()
		ret iterator
	}
}

// Iterates the elements of the map, returns pointer to the key and value.
// It guarantees that any key in the map will be visited only once, and
// for un-mutated maps, every key will be visited once. If the map is
// mutated during iteration, mutations will be reflected on return from
// iter, but the set of keys visited by iter is non-deterministic.
struct mapIterator[K: comparable, V] {
	m:      &_Map[K, V]
	ctrl:   []bitset
	groups: []group[K, V]
	n:      int
	g:      int
	s:      u32
}

impl mapIterator {
	fn init(mut self) {
		if self.m == nil {
			ret
		}
		// take a consistent view of the table in case
		// we rehash during iteration
		self.ctrl, self.groups = self.m.ctrl, self.m.groups
		if self.m.len() > 0 {
			// pick a random starting group
			self.g = int(rand() % u64(len(self.groups)))
		}
		self.n = 0
		self.s = 0
	}

	// Returns pointer to the key and value.
	// Returns nil pointer for both if iteration ended.
	fn next(mut self): (*K, *V) {
		for self.n < len(self.groups); self.n++ {
			mut ctrl := self.ctrl[self.g]
			for self.s < ctrl.len(); self.s++ {
				c := ctrl.at(self.s)
				if c == empty || c == tombstone {
					continue
				}
				mut key := &self.groups[self.g].keys[self.s]
				mut value := &self.groups[self.g].values[self.s]
				self.s++
				if self.s >= ctrl.len() {
					self.n++
					self.s = 0
					self.g++
					if self.g >= len(self.groups) {
						self.g = 0
					}
				}
				ret key, value
			}
			self.g++
			self.s = 0
			if self.g >= len(self.groups) {
				self.g = 0
			}
		}
		ret nil, nil
	}
}
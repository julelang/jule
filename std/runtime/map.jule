// Copyright 2024-2025 The Jule Programming Language.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

// This file contains the source code of the built-in map type.
// The built-in map type implementation is typically a hashmap.
// It is not lock-free in terms of concurrency, that is,
// it does not offer a thread-safe implementation.
// Uses the [maphash] function to hash keys.
// An empty initialization literal is valid and equals to nil map.
// To make it pass-by-reference, compiler implements map instances using with smart pointers.
// So, typically a nil map actually is a nil smart pointer.
//
// Implementation adopted from Go port of the Abseil's SwissTable (licensed under Apache 2.0 terms).
// Source repository: https://github.com/cockroachdb/swiss, commit [d6e517c].
// But the implementation is not same as repository.
// Optimized for the Jule runtime and compiler.
//
// Swiss Tables as described in
// https://abseil.io/about/design/swisstables. See also:
// https://faultlore.com/blah/hashbrown-tldr/.
//
// Google's C++ implementation:
//
//	https://github.com/abseil/abseil-cpp/blob/master/absl/container/internal/raw_hash_set.h
//
// # Swiss Tables
//
// Swiss tables are hash tables that map keys to values.
// Swiss tables use open-addressing rather than chaining to
// handle collisions. If you're not familiar with open-addressing see
// https://en.wikipedia.org/wiki/Open_addressing. A hybrid between linear and
// quadratic probing is used - linear probing within groups of small fixed
// size and quadratic probing at the group level. The key design choice of
// Swiss tables is the usage of a separate metadata array that stores 1 byte
// per slot in the table. 7-bits of this "control byte" are taken from
// hash(key) and the remaining bit is used to indicate whether the slot is
// empty, full, or deleted. The metadata array allows quick probes. The Google
// implementation of Swiss tables uses SIMD on x86 CPUs in order to quickly
// check 16 slots at a time for a match. Neon on arm64 CPUs is apparently too
// high latency, but the generic version is still able to compare 8 bytes at
// time through bit tricks (SWAR, SIMD Within A Register).
//
// Google's Swiss Tables layout is N-1 slots where N is a power of 2 and
// N+groupSize control bytes. The [N:N+groupSize] control bytes mirror the
// first groupSize control bytes so that probe operations at the end of the
// control bytes array do not have to perform additional checks. The
// separation of control bytes and slots implies 2 cache misses for a large
// map (larger than L2 cache size) or a cold map. The hmap implementation
// differs from Google's layout: it groups together 8 control bytes and 8
// slots which often results in 1 cache miss for a large or cold map rather
// than separate accesses for the controls and slots. The mirrored control
// bytes are no longer needed and and groups no longer start at arbitrary slot
// index, but only at those that are multiples of 8.
//
// Probing is done by taking the top 57 bits of hash(key)%N as the index into
// the groups slice and then performing a check of the groupSize control bytes
// within the group. Probing walks through groups in the table using quadratic
// probing until it finds a group that has at least one empty slot. See the
// comments on probeSeq for more details on the order in which groups are
// probed and the guarantee that every group is examined which means that in
// the worst case probing will end when an empty slot is encountered (the map
// can never be 100% full).
//
// Deletion is performed using tombstones (ctrlDeleted) with an optimization
// to mark a slot as empty if we can prove that doing so would not violate the
// probing behavior that a group of full slots causes probing to continue. It
// is invalid to take a group of full slots and mark one as empty as doing so
// would cause subsequent lookups to terminate at that group rather than
// continue to probe. We prove a slot was never part of a full group by
// looking for whether any of the groupSize-1 neighbors to the left and right
// of the deleting slot are empty which indicates that the slot was never part
// of a full group.
//
// # Extendible Hashing
//
// The Swiss table design has a significant caveat: resizing of the table is
// done all at once rather than incrementally. This can cause long-tail
// latency blips in some use cases. To address this caveat, extendible hashing
// (https://en.wikipedia.org/wiki/Extendible_hashing) is applied on top of the
// Swiss table foundation. In extendible hashing, there is a top-level
// directory containing entries pointing to tables. In hmap each table
// is a Swiss table as described above.
//
// The high bits of hash(key) are used to index into the table directory
// which is effectively a trie. The number of bits used is the globalDepth,
// resulting in 2^globalDepth directory entries. Adjacent entries in the
// directory are allowed to point to the same table which enables resizing to
// be done incrementally, one table at a time. Each table has a localDepth
// which is less than or equal to the globalDepth. If the localDepth for a
// table equals the globalDepth then only a single directory entry points to
// the table. Otherwise, more than one directory entry points to the table.
//
// The diagram below shows one possible scenario for the directory and
// tables. With a globalDepth of 2 the directory contains 4 entries. The
// first 2 entries point to the same table which has a localDepth of 1, while
// the last 2 entries point to different tables.
//
//	 dir(globalDepth=2)
//	+----+
//	| 00 | --\
//	+----+    +--> table[localDepth=1]
//	| 01 | --/
//	+----+
//	| 10 | ------> table[localDepth=2]
//	+----+
//	| 11 | ------> table[localDepth=2]
//	+----+
//
// The index into the directory is "hash(key) >> (64 - globalDepth)".
//
// When a table gets too large (specified by a configurable threshold) it is
// split. When a table is split its localDepth is incremented. If its
// localDepth is less than or equal to its globalDepth then the newly split
// table can be installed in the directory. If the table's localDepth is
// greater than the globalDepth then the globalDepth is incremented and the
// directory is reallocated at twice its current size. In the diagram above,
// consider what happens if the table at dir[3] is split:
//
//	 dir(globalDepth=3)
//	+-----+
//	| 000 | --\
//	+-----+    \
//	| 001 | ----\
//	+-----+      +--> table[localDepth=1]
//	| 010 | ----/
//	+-----+    /
//	| 011 | --/
//	+-----+
//	| 100 | --\
//	+-----+    +----> table[localDepth=2]
//	| 101 | --/
//	+-----+
//	| 110 | --------> table[localDepth=3]
//	+-----+
//	| 111 | --------> table[localDepth=3]
//	+-----+
//
// Note that the diagram above is very unlikely with a good hash function as
// the tables will tend to fill at a similar rate.
//
// The split operation redistributes the records in a table into two tables.
// This is done by walking over the records in the table to be split,
// computing hash(key) and using localDepth to extract the bit which
// determines whether to leave the record in the current table or to move it
// to the new table.
//
// hmaps containing only a single table are optimized to avoid the directory
// indexing resulting in performance that is equivalent to a Swiss table
// without extendible hashing.
//
// In order to avoid a level of indirection when accessing a table, the
// table directory points to tables by value rather than by pointer.
// Adjacent table[K, V]'s which share are logically the same table share the
// table.groups slice and have the same values for
// table.{groupMask,localDepth,index}. The other fields of a table are only
// valid for tables where &hmap.dir[table.index] = &table (i.e. the first
// table in the directory with the specified index). During get/lookup operations,
// any of the tables with the same index may be used for retrieval. During
// set and delete operations an additional indirection is performed, though
// the common case is that this indirection is within the same cache line as
// it is to the immediately preceding table in the directory.

use "std/internal/runtime"
use "std/math/bits"
use "std/unsafe"

const (
	groupSize       = 8
	maxAvgGroupLoad = 7

	ctrlEmpty:   ctrl = 0b10000000
	ctrlDeleted: ctrl = 0b11111110

	bitsetLSB     = 0x0101010101010101
	bitsetMSB     = 0x8080808080808080
	bitsetEmpty   = bitsetLSB * u64(ctrlEmpty)
	bitsetDeleted = bitsetLSB * u64(ctrlDeleted)

	// The default maximum capacity a table is allowed to
	// grow to before it will be split.
	defaultMaxTableCapacity: u32 = 8192

	// Used to optimize code generation for hmap.table(), hmap.tableCount(),
	// and tableStep(). This technique was lifted from the Go runtime's
	// runtime/map.go:bucketShift() routine
	shiftMask = runtime::PtrBits - 1
)

// Represents a set of slots within a group.
//
// The underlying representation uses one byte per slot, where each byte is
// either 0x80 if the slot is part of the set or 0x00 otherwise. This makes it
// convenient to calculate for an entire group at once (e.g. see matchEmpty).
type bitset: u64

impl bitset {
	// Assumes that only the MSB of each control byte can be set (e.g. bitset
	// is the result of matchEmpty or similar) and returns the relative index of the
	// first control byte in the group that has the MSB set.
	//
	// Returns 8 if the bitset is 0.
	// Returns groupSize if the bitset is empty.
	#disable nilptr
	fn first(*self): u32 {
		ret u32(bits::TrailingZeros64(u64(*self))) >> 3
	}

	// Removes the first set bit (that is, resets the least significant set bit to 0).
	#disable nilptr
	fn removeFirst(*self): bitset {
		ret *self & (*self - 1)
	}
}

// Each slot in the hash table has a control byte which can have one of three
// states: empty, deleted, and full. They have the following bit patterns:
//
//	  empty: 1 0 0 0 0 0 0 0
//	deleted: 1 1 1 1 1 1 1 0
//	   full: 0 h h h h h h h  // h represents the H1 hash bits
type ctrl: u8

// Fixed size array of groupSize control bytes stored in a u64.
type ctrlGroup: u64

impl ctrlGroup {
	// Get returns the i-th control byte.
	#disable nilptr
	fn get(*self, mut i: u32): ctrl {
		const match {
		| LittleEndian:
			ret unsafe { *((*ctrl)(self) + uint(i)) }
		| BigEndian:
			i = i ^ (groupSize - 1) // equivalent to (groupSize-1-i).
			ret unsafe { *((*ctrl)(self) + uint(i)) }
		|:
			panic("unreachable")
		}
	}

	// Set sets the i-th control byte.
	#disable nilptr
	fn set(mut *self, mut i: u32, c: ctrl) {
		const match {
		| LittleEndian:
			unsafe { *((*ctrl)(self) + uint(i)) = c }
		| BigEndian:
			i = i ^ (groupSize - 1) // equivalent to (groupSize-1-i).
			unsafe { *((*ctrl)(self) + uint(i)) = c }
		|:
			panic("unreachable")
		}
	}

	// Sets all the control bytes to empty.
	#disable nilptr
	fn setEmpty(mut *self) {
		*self = ctrlGroup(bitsetEmpty)
	}

	// Returns the set of slots which are full and for which the 7-bit hash
	// matches the given value. May return false positives.
	#disable nilptr
	fn matchH2(*self, h: uintptr): bitset {
		// NB: This generic matching routine produces false positive matches when
		// h is 2^N and the control bytes have a seq of 2^N followed by 2^N+1. For
		// example: if ctrls==0x0302 and h=02, we'll compute v as 0x0100. When we
		// subtract off 0x0101 the first 2 bytes we'll become 0xffff and both be
		// considered matches of h. The false positive matches are not a problem,
		// just a rare inefficiency. Note that they only occur if there is a real
		// match and never occur on ctrlEmpty, or ctrlDeleted. The subsequent key
		// comparisons ensure that there is no correctness issue.
		v := u64(*self) ^ (bitsetLSB * u64(h))
		ret bitset(((v - bitsetLSB) & ^v) & bitsetMSB)
	}

	// Returns the set of slots in the group that are empty.
	#disable nilptr
	fn matchEmpty(*self): bitset {
		// An empty slot is   1000 0000
		// A deleted slot is  1111 1110
		// A full slot is     0??? ????
		//
		// A slot is empty iff bit 7 is set and bit 1 is not. We could select any
		// of the other bits here (e.g. v << 1 would also work).
		v := u64(*self)
		ret bitset((v & ^(v << 6)) & bitsetMSB)
	}

	// Returns the set of slots in the group that are empty or deleted.
	#disable nilptr
	fn matchEmptyOrDeleted(*self): bitset {
		// An empty slot is  1000 0000
		// A deleted slot is 1111 1110
		// A full slot is    0??? ????
		//
		// A slot is empty or deleted iff bit 7 is set and bit 0 is not.
		v := u64(*self)
		ret bitset((v & ^(v << 7)) & bitsetMSB)
	}

	// Converts deleted control bytes in a group to empty control bytes,
	// and control bytes indicating full slots to deleted control bytes.
	#disable nilptr
	fn convertNonFullToEmptyAndFullToDeleted(mut *self) {
		// An empty slot is     1000 0000
		// A deleted slot is    1111 1110
		// A full slot is       0??? ????
		//
		// We select the MSB, invert, add 1 if the MSB was set and zero out the low
		// bit.
		//
		//  - if the MSB was set (i.e. slot was empty, or deleted):
		//     v:             1000 0000
		//     ^v:            0111 1111
		//     ^v + (v >> 7): 1000 0000
		//     &^ bitsetLSB:  1000 0000 = empty slot.
		//
		// - if the MSB was not set (i.e. full slot):
		//     v:             0000 0000
		//     ^v:            1111 1111
		//     ^v + (v >> 7): 1111 1111
		//     &^ bitsetLSB:  1111 1110 = deleted slot.
		//
		v := u64(*self) & bitsetMSB
		*self = ctrlGroup((^v + (v >> 7)) & ^bitsetLSB)
	}
}

// Maintains the state for a probe sequence that iterates through the
// groups in a table. The sequence is a triangular progression of the form
//
//	p(i) := (i^2 + i)/2 + hash (mod mask+1)
//
// The sequence effectively outputs the indexes of *groups*. The group
// machinery allows us to check an entire group with minimal branching.
//
// It turns out that this probe sequence visits every group exactly once if
// the number of groups is a power of two, since (i^2+i)/2 is a bijection in
// Z/(2^m). See https://en.wikipedia.org/wiki/Quadratic_probing
struct probeSeq {
	mask:   u32
	offset: u32
	index:  u32
}

impl probeSeq {
	fn make(hash: uintptr, mask: u32): probeSeq {
		ret probeSeq{
			mask: mask,
			offset: u32(hash) & mask,
			index: 0,
		}
	}

	#disable nilptr
	fn next(*self): probeSeq {
		mut s := *self
		s.index++
		s.offset = (s.offset + s.index) & s.mask
		ret s
	}
}

// Extracts the H1 portion of a hash: the 57 upper bits.
fn h1(h: uintptr): uintptr {
	ret h >> 7
}

// Extracts the H2 portion of a hash: the 7 bits not used for h1.
//
// These are used as an occupied control byte.
fn h2(h: uintptr): uintptr {
	ret h & 0x7f
}

// Holds groupSize control bytes and slots (key:value) arrays.
// Each keys[i] and values[i] pair is a slot.
struct group[K: comparable, V] {
	ctrls:  ctrlGroup
	keys:   [groupSize]K
	values: [groupSize]V
}

// Implements Google's Swiss Tables hash table design. A map instance is
// composed of 1 or more tables that are addressed using extendible hashing.
struct table[K: comparable, V] {
	// groups is groupMask+1 in length and holds groupSize key/value slots and
	// their control bytes.
	groups: []group[K, V]

	// groupMask is the number of groups minus 1 which is used to quickly
	// compute i%N using a bitwise & operation. The groupMask only changes
	// when a table is resized.
	groupMask: u32

	// Capacity, used, and growthLeft are only updated on mutation operations.
	// Read operations only access the groups and groupMask fields.

	// The total number (always 2^N). Equal to `(groupMask+1)*groupSize`
	// (unless the table is empty, when capacity is 0).
	capacity: u32

	// The number of filled slots (i.e. the number of elements in the table).
	used: u32

	// The number of slots we can still fill without needing to rehash.
	//
	// This is stored separately due to tombstones: we do not include
	// tombstones in the growth capacity because we'd like to rehash when the
	// table is filled with tombstones as otherwise probe sequences might get
	// unacceptably long without triggering a rehash.
	growthLeft: u32

	// Number of high bits from hash(key) used to generate
	// an index for the global directory to locate this table. If it
	// is 0 this table is hmap.table0. It is only updated when a
	// table splits.
	localDepth: u32

	// The index of the table within hmap.dir. The tables in
	// hmap.dir[index:index+2^(globalDepth-localDepth)] all share the same
	// groups (and are logically the same table). Only the table at
	// hmap.dir[index] can be used for mutation operations (set, delete). The
	// other tables can be used for get operations. Index is only updated
	// when a table splits or the directory grows.
	index: u32
}

impl table {
	// Returns the number of deleted (tombstone) entries in the table.
	// A tombstone is a slot that has been deleted but is still considered
	// occupied so as not to violate the probing invariant.
	#disable nilptr
	fn tombstones(*self): u32 {
		ret (self.capacity*maxAvgGroupLoad)/groupSize - self.used - self.growthLeft
	}

	// Inserts an entry known not to be in the table. Used by set
	// after it has failed to find an existing entry to overwrite duration
	// insertion. Returns pointer to the value.
	//
	// We handle key as reference pointer to avoid creating copy of it.
	// Copy may significantly reduce performance especially on large types.
	#disable boundary nilptr
	fn uncheckedSet(mut *self, h: uintptr, mut &key: *K): *V {
		// Given key and its hash hash(key), to insert it, we construct a
		// probeSeq, and use it to find the first group with an unoccupied (empty
		// or deleted) slot. We place the key/value into the first such slot in
		// the group and mark it as full with key's H2.
		mut seq := probeSeq.make(h1(h), self.groupMask)
		for ; seq = seq.next() {
			mut g := &self.groups[seq.offset]
			unsafe {
				_match := g.ctrls.matchEmptyOrDeleted()
				if _match != 0 {
					i := _match.first()
					g.keys[i] = *key
					if g.ctrls.get(i) == ctrlEmpty {
						self.growthLeft--
					}
					g.ctrls.set(i, ctrl(h2(h)))
					ret &g.values[i]
				}
			}
		}
	}

	fn rehash(mut *self, mut &m: *hmap[K, V]) {
		// Rehash in place if we can recover >= 1/3 of the capacity. Note that
		// this heuristic differs from Abseil's and was experimentally determined
		// to balance performance.
		//
		// Abseil notes that in the worst case it takes ~4 set/delete pairs to
		// create a single tombstone. Rehashing in place is significantly faster
		// than resizing because the common case is that elements remain in their
		// current location. The performance of rehashInPlace is dominated by
		// recomputing the hash of every key. We know how much space we're going
		// to reclaim because every tombstone will be dropped and we're only
		// called if we've reached the thresold of capacity/8 empty slots. So the
		// number of tomstones is capacity*7/8 - used.
		if self.capacity > groupSize && self.tombstones() >= self.capacity/3 {
			self.rehashInPlace(m)
			ret
		}

		// If the newCapacity is larger than the maxTableCapacity split the
		// table instead of resizing. Each of the new tables will be the same
		// size as the current table.
		newCapacity := self.capacity << 1
		if newCapacity > m.maxTableCapacity {
			self.split(m)
			ret
		}

		self.resize(m, newCapacity)
	}

	#disable boundary nilptr
	fn init(mut *self, mut newCapacity: u32) {
		if newCapacity < groupSize {
			newCapacity = groupSize
		}

		self.capacity = newCapacity
		self.groupMask = self.capacity/groupSize - 1
		self.groups = make([]group[K, V], int(self.groupMask+1))

		mut i := u32(0)
		for i <= self.groupMask; i++ {
			self.groups[i].ctrls.setEmpty()
		}

		self.resetGrowthLeft()
	}

	// Resize the capacity of the table by allocating a bigger array and
	// uncheckedSetting each element of the table into the new array (we know that
	// no insertion here will set an already-present value), and discard the old
	// backing array.
	#disable boundary nilptr
	fn resize(mut *self, &m: *hmap[K, V], newCapacity: u32) {
		mut oldGroups := self.groups
		oldGroupMask := self.groupMask
		oldCapacity := self.capacity
		self.init(newCapacity)

		if oldCapacity > 0 {
			mut i := u32(0)
			for i <= oldGroupMask; i++ {
				unsafe {
					mut g := &oldGroups[i]
					mut j := u32(0)
					for j < groupSize; j++ {
						if (g.ctrls.get(j)&ctrlEmpty) == ctrlEmpty {
							continue
						}
						mut &key := &g.keys[j]
						h := m.hash(key)
						*self.uncheckedSet(h, key) = g.values[j]
					}
				}
			}
		}
	}

	// Divides the entries in a table between the receiver and a new table
	// of the same size, and then installs the new table into the tables
	// directory, growing the tables directory if necessary.
	#disable boundary nilptr
	fn split(mut *self, mut &m: *hmap[K, V]) {
		mut t := self
		unsafe {
			// Create the new table as a clone of the table being split. If we're
			// splitting table0 we need to allocate a *table[K, V] for scratch
			// space. Otherwise we use table0 as the scratch space.
			let mut newt: *table[K, V]
			let mut zero: table[K, V]
			if m.globalShift == 0 {
				newt = &zero
			} else {
				newt = &m.table0
			}
			*newt = table[K, V]{
				localDepth: t.localDepth,
				index: t.index,
			}
			newt.init(t.capacity)

			// Divide the records between the 2 tables (t and newt). This is done by
			// examining the new bit in the hash that will be added to the table
			// index. If that bit is 0 the record stays in table t. If that bit is 1
			// the record is moved to table newt. We're relying on the table t
			// staying earlier in the directory than newt after the directory is
			// grown.
			mask := uintptr(1) << (runtime::PtrBits - (t.localDepth + 1))
			mut i := u32(0)
			for i <= t.groupMask; i++ {
				mut g := &t.groups[i]
				mut j := u32(0)
				for j < groupSize; j++ {
					if (g.ctrls.get(j)&ctrlEmpty) == ctrlEmpty {
						continue
					}

					mut &key := &g.keys[j]
					h := m.hash(key)
					if (h&mask) == 0 {
						// Nothing to do, the record is staying in t.
						continue
					}

					mut &value := &g.values[j]
					// Insert the record into newt.
					*newt.uncheckedSet(h, key) = *value
					newt.used++

					// Delete the record from t.
					if g.ctrls.matchEmpty() != 0 {
						g.ctrls.set(j, ctrlEmpty)
						t.growthLeft++
					} else {
						g.ctrls.set(j, ctrlDeleted)
					}

					let mut keyZero: K
					let mut valueZero: V
					*key = keyZero
					*value = valueZero

					t.used--
				}
			}

			if newt.used == 0 {
				// We didn't move any records to the new table. Either
				// maxTableCapacity is too small and we got unlucky, or we have a
				// degenerate hash function (e.g. one that returns a constant in the
				// high bits).
				m.maxTableCapacity = m.maxTableCapacity << 1
				*newt = table[K, V]{}
				t.resize(m, t.capacity<<1)
				ret
			}

			if t.used == 0 {
				// We moved all of the records to the new table (note the two
				// conditions are equivalent and both are present merely for clarity).
				// Similar to the above, bump maxTableCapacity and resize the table
				// rather than splitting. We'll replace the old table with the new
				// table in the directory.
				m.maxTableCapacity = m.maxTableCapacity << 1
				newt = m.installTable(&*newt)
				newt.resize(m, newt.capacity<<1)
				ret
			}

			// We need to ensure table t, which we evacuated records from, has empty
			// slots as we may be inserting into it. We also want to drop any
			// tombstones that may have been left in table to ensure lookups for
			// non-existent keys don't have to traverse long probe chains. With a good
			// hash function, 50% of the entries in t should have been moved to newt,
			// so we should be able to drop tombstones corresponding to ~50% of the
			// entries.
			t.rehashInPlace(m)

			// Grow the directory if necessary.
			if t.localDepth >= m.globalDepth() {
				// When the directory grows t will be invalidated. We pass in t's
				// index so that growDirectory will return the new index it resides
				// at.
				k := m.growDirectory(t.localDepth+1, t.index)
				t = &m.dir[k]
			}

			// Complete the split by incrementing the local depth for the 2 tables
			// and installing the new table in the directory.
			t.localDepth++
			m.installTable(&(*t))
			newt.localDepth = t.localDepth
			newt.index = t.index + tableStep(m.globalDepth(), t.localDepth)
			m.installTable(&*newt)
			*newt = table[K, V]{}
		}
	}

	#disable boundary nilptr
	fn rehashInPlace(mut *self, mut &m: *hmap[K, V]) {
		if self.capacity == 0 {
			ret
		}

		// We want to drop all of the deletes in place. We first walk over the
		// control bytes and mark every DELETED slot as EMPTY and every FULL slot
		// as DELETED. Marking the DELETED slots as EMPTY has effectively dropped
		// the tombstones, but we fouled up the probe invariant. Marking the FULL
		// slots as DELETED gives us a marker to locate the previously FULL slots.

		// Mark all DELETED slots as EMPTY and all FULL slots as DELETED.
		mut i := u32(0)
		for i <= self.groupMask; i++ {
			self.groups[i].ctrls.convertNonFullToEmptyAndFullToDeleted()
		}

		// Now we walk over all of the DELETED slots (a.k.a. the previously FULL
		// slots). For each slot we find the first probe group we can place the
		// element in which reestablishes the probe invariant. Note that as this
		// loop proceeds we have the invariant that there are no DELETED slots in
		// the range [0, i). We may move the element at i to the range [0, i) if
		// that is where the first group with an empty slot in its probe chain
		// resides, but we never set a slot in [0, i) to DELETED.
		i = 0
		for i <= self.groupMask; i++ {
			unsafe {
				mut g := &self.groups[i]
				mut j := u32(0)
				for j < groupSize; j++ {
					if g.ctrls.get(j) != ctrlDeleted {
						continue
					}

					mut &key := &g.keys[j]
					h := m.hash(key)
					mut seq := probeSeq.make(h1(h), self.groupMask)
					desiredOffset := seq.offset

					let mut targetGroup: *group[K, V]
					let mut target: u32
					for ; seq = seq.next() {
						targetGroup = &self.groups[seq.offset]
						_match := targetGroup.ctrls.matchEmptyOrDeleted()
						if _match != 0 {
							target = _match.first()
							break
						}
					}

					match {
					| i == desiredOffset:
						// If the target index falls within the first probe group
						// then we don't need to move the element as it already
						// falls in the best probe position.
						g.ctrls.set(j, ctrl(h2(h)))
					| targetGroup.ctrls.get(target) == ctrlEmpty:
						mut &value := &g.values[j]
						// The target slot is empty. Transfer the element to the
						// empty slot and mark the slot at index i as empty.
						targetGroup.ctrls.set(target, ctrl(h2(h)))
						targetGroup.keys[target] = *key
						targetGroup.values[target] = *value
						let mut keyZero: K
						let mut valueZero: V
						*key = keyZero
						*value = valueZero
						g.ctrls.set(j, ctrlEmpty)
					| targetGroup.ctrls.get(target) == ctrlDeleted:
						mut &value := &g.values[j]
						// The slot at target has an element (i.e. it was FULL).
						// We're going to swap our current element with that
						// element and then repeat processing of index i which now
						// holds the element which was at target.
						targetGroup.ctrls.set(target, ctrl(h2(h)))
						mut &targetKey := &targetGroup.keys[target]
						mut &targetValue := &targetGroup.values[j]
						*key, *targetKey = *targetKey, *key
						*value, *targetValue = *targetValue, *value
						// Repeat processing of the j'th slot which now holds a
						// new key/value.
						j--
					|:
						panic("ctrl at position should be empty or deleted")
					}
				}
			}
		}

		self.resetGrowthLeft()
		self.growthLeft -= self.used
	}

	#disable nilptr
	fn resetGrowthLeft(mut *self) {
		let mut growthLeft: int
		if self.capacity <= groupSize {
			// If the map fits in a single group then we're able to fill all of
			// the slots except 1 (an empty slot is needed to terminate find
			// operations).
			growthLeft = int(self.capacity - 1)
		} else {
			growthLeft = int((self.capacity * maxAvgGroupLoad) / groupSize)
		}
		if growthLeft < 0 {
			growthLeft = 0
		}
		self.growthLeft = u32(growthLeft)
	}
}

// Implementation of the built-in map type of Jule.
// Instantiated by the compiler for each combination of key and value.
struct hmap[K: comparable, V] {
	// Seed for hashing.
	// It should be updated after speficic operations if needed,
	// for security reasons.
	seed: uintptr

	// table0 is always present and inlined in the hmap to avoid a pointer
	// indirection during the common case that the map contains a single
	// table. table0 is also used during split operations as a temporary
	// table to split into before the table is installed in the directory.
	table0: table[K, V]

	// The directory of tables. See the comment on table.index for details
	// on how the physical table values map to logical tables.
	dir: []table[K, V]

	// The number of filled slots across all tables (i.e. the number of
	// elements in the map).
	used: int

	// globalShift is the number of bits to right shift a hash value to
	// generate an index for the global directory. As a special case, if
	// globalShift==0 then table0 is used and the directory is not accessed.
	// Note that globalShift==(64-globalDepth). globalShift is used rather
	// than globalDepth because the shifting is the more common operation than
	// needing to compare globalDepth to a table's localDepth.
	globalShift: u32

	// The maximum capacity a table is allowed to grow to before it will be
	// split.
	maxTableCapacity: u32
}

impl hmap {
	// Initializes a map with the specified initial capacity. If
	// initialCapacity is 0 the map will start out with zero capacity and will
	// grow on the first insert. The zero value for a map is not usable and Init
	// must be called before using the map.
	#disable boundary nilptr
	fn make(initialCapacity: int): &hmap[K, V] {
		mut m := new(hmap[K, V])
		// The groups slice for table0 in an empty map points to a single
		// group where the controls are all marked as empty. This
		// simplifies the logic for probing. The empty controls will never match
		// a probe operation, and if insertion is performed growthLeft==0 will
		// trigger a resize of the table.
		*m = hmap[K, V]{
			seed: uintptr(rand()),
			table0: table[K, V]{
				groups: unsafe::Slice(unsafe { (*group[K, V])(&emptyCtrls[0]) }, len(emptyCtrls), len(emptyCtrls)),
			},
			maxTableCapacity: defaultMaxTableCapacity,
		}

		// Initialize the directory to point to table0.
		m.dir = unsafe::Slice(&m.table0, 1, 1)

		if m.maxTableCapacity < groupSize {
			m.maxTableCapacity = groupSize
		}
		m.maxTableCapacity = normalizeCapacity(m.maxTableCapacity)

		if initialCapacity > 0 {
			// We consider initialCapacity to be an indication from the caller
			// about the number of records the map should hold. The realized
			// capacity of a map is 7/8 of the number of slots, so we set the
			// target capacity to initialCapacity*8/7.
			targetCapacity := uintptr((initialCapacity * groupSize) / maxAvgGroupLoad)
			if targetCapacity <= uintptr(m.maxTableCapacity) {
				// Normalize targetCapacity to the smallest value of the form 2^k.
				m.table0.init(normalizeCapacity(u32(targetCapacity)))
			} else {
				// If targetCapacity is larger than maxTableCapacity we need to
				// size the directory appropriately. We'll size each table to
				// maxTableCapacity and create enough tables to hold
				// initialCapacity.
				nTables := (targetCapacity + uintptr(m.maxTableCapacity) - 1) / uintptr(m.maxTableCapacity)
				globalDepth := u32(bits::Len32(u32(nTables) - 1))
				m.growDirectory(globalDepth, 0) /* index */

				n := m.tableCount()
				mut i := u32(0)
				for i < n; i++ {
					unsafe {
						mut t := &m.dir[i]
						t.init(m.maxTableCapacity)
						t.localDepth = globalDepth
						t.index = i
					}
				}
			}
		}

		ret m
	}

	// Returns the number of entries in the map.
	// Implementation of the built-in len function for map types.
	fn len(*self): int {
		if self == nil {
			ret 0
		}
		ret self.used
	}

	// Returns the hash value of the key.
	#disable nilptr
	fn hash(*self, &key: *K): uintptr {
		ret maphash(key, self.seed)
	}

	// Returns the number of bits from the top of the hash to use for
	// indexing in the tables directory.
	#disable nilptr
	fn globalDepth(*self): u32 {
		if self.globalShift == 0 {
			ret 0
		}
		ret runtime::PtrBits - self.globalShift
	}

	// Returns the number of tables in the tables directory.
	#disable nilptr
	fn tableCount(*self): u32 {
		const shiftMask = 31
		ret u32(1) << (self.globalDepth() & shiftMask)
	}

	// Installs a table into the tables directory, overwriting
	// every index in the range of entries the table occupies.
	#disable boundary nilptr
	fn installTable(mut *self, mut &t: *table[K, V]): *table[K, V] {
		step := tableStep(self.globalDepth(), t.localDepth)
		mut i := u32(0)
		for i < step; i++ {
			self.dir[t.index+i] = *t
		}
		ret &self.dir[t.index]
	}

	// Grows the directory slice to 1<<newGlobalDepth tables. Grow
	// directory returns the new index location for the table specified by index.
	#disable boundary nilptr
	fn growDirectory(mut *self, newGlobalDepth: u32, index: u32): (newIndex: u32) {
		mut newDir := make([]table[K, V], 1<<newGlobalDepth)

		mut lastIndex := u32(u32.Max)
		mut setNewIndex := true
		mut i, mut j, mut n := u32(0), u32(0), self.tableCount()
		for i < n; i++ {
			unsafe {
				mut t := &self.dir[i]
				if t.index == lastIndex {
					continue
				}
				lastIndex = t.index

				if t.index == index && setNewIndex {
					newIndex = j
					setNewIndex = false
				}
				t.index = j
				step := tableStep(newGlobalDepth, t.localDepth)
				mut k := u32(0)
				for k < step; k++ {
					newDir[j+k] = *t
				}
				j += step
			}
		}

		// Zero out table0 if we're growing from 1 table (which uses table0) to
		// more than 1 table.
		if self.globalShift == 0 {
			self.table0 = table[K, V]{}
		}
		self.dir = newDir
		self.globalShift = runtime::PtrBits - newGlobalDepth

		ret newIndex
	}

	// Returns the table corresponding to hash value h.
	#disable boundary nilptr
	fn table(mut *self, h: uintptr): *table[K, V] {
		// NB: It is faster to check for the single table case using a
		// conditional than to index into the directory.
		if self.globalShift == 0 {
			ret &self.table0
		}
		ret &self.dir[h>>(self.globalShift&shiftMask)]
	}

	#disable boundary nilptr
	fn mutableTable(mut *self, h: uintptr): *table[K, V] {
		// NB: It is faster to check for the single table case using a
		// conditional than to to index into the directory.
		if self.globalShift == 0 {
			ret &self.table0
		}
		bidx := self.dir[h>>(self.globalShift&shiftMask)].index
		// The mutable table is the one located at self.dir[t.index]. This will
		// usually be either the current table t, or the immediately preceding
		// table which is usually in the same cache line.
		ret &self.dir[bidx]
	}

	// Inserts an entry into the map, overwriting an existing value if an
	// entry with the same key already exists. Returns pointer to the value.
	//
	// This function will be checked before at runtime before call.
	// The self receiver parameter must be nil-safe.
	#disable boundary nilptr
	fn set(mut *self, mut key: K): *V {
		unsafe {
			// set is find composed with uncheckedSet. We perform find to see if the
			// key is already present. If it is, we're done and overwrite the existing
			// value. If the value isn't present we perform an uncheckedSet which
			// inserts an entry known not to be in the table (violating this
			// requirement will cause the table to behave erratically).
			h := self.hash(&key)
			mut t := self.mutableTable(h)

			// NB: Unlike the abseil swiss table implementation which uses a common
			// find routine for get, set, and delete, we have to manually inline the
			// find routine for performance.
			mut seq := probeSeq.make(h1(h), t.groupMask)
			startOffset := seq.offset

			for ; seq = seq.next() {
				mut g := &t.groups[seq.offset]
				mut _match := g.ctrls.matchH2(h2(h))

				for _match != 0 {
					i := _match.first()
					if key == g.keys[i] {
						ret &g.values[i]
					}
					_match = _match.removeFirst()
				}

				_match = g.ctrls.matchEmpty()
				if _match != 0 {
					// Finding an empty slot means we've reached the end of the probe
					// sequence.

					// If there is room left to grow in the table and we're at the
					// start of the probe sequence we can just insert the new entry.
					if t.growthLeft > 0 && seq.offset == startOffset {
						i := _match.first()
						g.keys[i] = key
						g.ctrls.set(i, ctrl(h2(h)))
						t.growthLeft--
						t.used++
						self.used++
						ret &g.values[i]
					}

					// Find the first empty or deleted slot in the key's probe
					// sequence.
					mut seq2 := probeSeq.make(h1(h), t.groupMask)
					for ; seq2 = seq2.next() {
						g2 := &t.groups[seq2.offset]
						_match = g2.ctrls.matchEmptyOrDeleted()
						if _match != 0 {
							i := _match.first()
							// If there is room left to grow in the table or the slot
							// is deleted (and thus we're overwriting it and not
							// changing growthLeft) we can insert the entry here.
							// Otherwise we need to rehash the table.
							if t.growthLeft > 0 || g.ctrls.get(i) == ctrlDeleted {
								g.keys[i] = key
								if g.ctrls.get(i) == ctrlEmpty {
									t.growthLeft--
								}
								g.ctrls.set(i, ctrl(h2(h)))
								t.used++
								self.used++
								ret &g.values[i]
							}
							break
						}
					}

					t.rehash(self)

					// We may have split the table in which case we have to
					// re-determine which table the key resides on. This
					// determination is quick in comparison to rehashing, resizing,
					// and splitting, so just always do it.
					t = self.mutableTable(h)

					// Note that we don't have to restart the entire set process as we
					// know the key doesn't exist in the map.
					mut value := t.uncheckedSet(h, &key)
					t.used++
					self.used++
					ret value
				}
			}
		}
	}

	// Deletes the entry corresponding to the specified key from the map.
	// It is a noop to delete a non-existent key.
	// Reports whether a key removed.
	//
	// This function will no be checked before at runtime before call.
	// The self receiver parameter may be nil.
	#disable boundary nilptr
	fn delete(mut *self, key: K): bool {
		if self == nil || self.used == 0 {
			ret false
		}
		unsafe {
			// delete is find composed with "deleted at": we perform find(key), and
			// then delete at the resulting slot if found.
			h := self.hash(&key)
			mut t := self.mutableTable(h)

			// NB: Unlike the abseil swiss table implementation which uses a common
			// find routine for get, set, and delete, we have to manually inline the
			// find routine for performance.
			mut seq := probeSeq.make(h1(h), t.groupMask)
			for ; seq = seq.next() {
				mut g := &t.groups[seq.offset]
				mut _match := g.ctrls.matchH2(h2(h))

				for _match != 0 {
					i := _match.first()
					if key == g.keys[i] {
						t.used--
						self.used--
						if self.used == 0 {
							// Reset the hash seed to make it more difficult for attackers to
							// repeatedly trigger hash collisions. See issue
							// https://github.com/golang/go/issues/25237.
							self.seed = uintptr(rand())
						}
						let mut keyZero: K
						let mut valueZero: V
						g.keys[i] = keyZero
						g.values[i] = valueZero

						// Only a full group can appear in the middle of a probe
						// sequence (a group with at least one empty slot terminates
						// probing). Once a group becomes full, it stays full until
						// rehashing/resizing. So if the group isn't full now, we can
						// simply remove the element. Otherwise, we create a tombstone
						// to mark the slot as deleted.
						if g.ctrls.matchEmpty() != 0 {
							g.ctrls.set(i, ctrlEmpty)
							t.growthLeft++
						} else {
							g.ctrls.set(i, ctrlDeleted)
						}
						ret true
					}
					_match = _match.removeFirst()
				}

				_match = g.ctrls.matchEmpty()
				if _match != 0 {
					ret false
				}
			}
		}
	}

	// Deletes all entries from the map resulting in an empty map.
	//
	// This function will no be checked before at runtime before call.
	// The self receiver parameter may be nil.
	#disable boundary nilptr
	fn clear(mut *self) {
		if self == nil {
			ret
		}

		let mut keyZero: K
		let mut valueZero: V

		let mut tw: tablewalker[K, V]
		tw.init(0, self)
		for {
			unsafe {
				mut t := tw.next()
				if t == nil {
					break
				}

				mut i := u32(0)
				for i <= t.groupMask; i++ {
					mut g := &t.groups[i]
					g.ctrls.setEmpty()
					mut j := u32(0)
					for j < groupSize; j++ {
						g.keys[j] = keyZero
						g.values[j] = valueZero
					}
				}

				t.used = 0
				t.resetGrowthLeft()
			}
		}

		// Reset the hash seed to make it more difficult for attackers to
		// repeatedly trigger hash collisions. See issue
		// https://github.com/golang/go/issues/25237.
		self.seed = uintptr(rand())
		self.used = 0
	}

	// Retrieves the value from the map for the specified key,
	// sets *value=zero, *ok=false if the key is not present.
	//
	// This function will no be checked before at runtime before call.
	// The self receiver parameter may be nil.
	#disable boundary nilptr
	fn lookup(mut *self, mut key: K, mut &value: *V, mut &ok: *bool) {
		if self == nil || self.used == 0 {
			goto NotExist
		}
		unsafe {
			h := self.hash(&key)
			t := self.table(h)

			// NB: Unlike the abseil swiss table implementation which uses a common
			// find routine for get, set, and delete, we have to manually inline the
			// find routine for performance.

			// To find the location of a key in the table, we compute hash(key). From
			// h1(hash(key)) and the capacity, we construct a probeSeq that visits
			// every group of slots in some interesting order.
			//
			// We walk through these indices. At each index, we select the entire group
			// starting with that index and extract potential candidates: occupied slots
			// with a control byte equal to h2(hash(key)). If we find an empty slot in the
			// group, we stop and return an error. The key at candidate slot y is compared
			// with key; if key == hmap.slots[y].key we are done and return y; otherwise we
			// continue to the next probe index. Tombstones (ctrlDeleted) effectively
			// behave like full slots that never match the value we're looking for.
			//
			// The h2 bits ensure when we compare a key we are likely to have actually
			// found the object. That is, the chance is low that keys compare false. Thus,
			// when we search for an object, we are unlikely to call == many times. This
			// likelyhood can be analyzed as follows (assuming that h2 is a random enough
			// hash function).
			//
			// Let's assume that there are k "wrong" objects that must be examined in a
			// probe sequence. For example, when doing a find on an object that is in the
			// table, k is the number of objects between the start of the probe sequence
			// and the final found object (not including the final found object). The
			// expected number of objects with an h2 match is then k/128. Measurements and
			// analysis indicate that even at high load factors, k is less than 32,
			// meaning that the number of false positive comparisons we must perform is
			// less than 1/8 per find.
			mut seq := probeSeq.make(h1(h), t.groupMask)
			for ; seq = seq.next() {
				mut g := &t.groups[seq.offset]
				mut _match := g.ctrls.matchH2(h2(h))

				for _match != 0 {
					i := _match.first()
					if key == g.keys[i] {
						if value != nil {
							*value = g.values[i]
						}
						if ok != nil {
							*ok = true
						}
						ret
					}
					_match = _match.removeFirst()
				}

				_match = g.ctrls.matchEmpty()
				if _match != 0 {
					goto NotExist
				}
			}
		}
	NotExist:
		if value != nil {
			let mut zeroValue: V
			*value = zeroValue
		}
		if ok != nil {
			*ok = false
		}
	}

	// Returns value of key if exist, otherwise returns default value of value type.
	//
	// This function will no be checked before at runtime before call.
	// The self receiver parameter may be nil.
	#disable nilptr
	fn get(mut *self, mut key: K): (value: V) {
		// The lookup function is safe for nil receiver parameters.
		self.lookup(key, &value, nil)
		ret
	}

	fn iterator(mut *self): hmapiterator[K, V] {
		let mut iterator: hmapiterator[K, V]
		iterator.init(self)
		ret iterator
	}
}

struct hmapiterator[K: comparable, V] {
	bw:        tablewalker[K, V]
	groups:    []group[K, V]
	offset:    u32
	groupMask: u32
	i:         u32
	j:         u32
}

impl hmapiterator {
	#disable nilptr
	fn init(mut *self, mut &m: *hmap[K, V]) {
		// Randomize iteration order by starting iteration at a random table and
		// within each table at a random offset.
		offset := uintptr(rand())
		self.bw.init(offset>>32, m)
		self.offset = u32(offset)
		self.i = 0
		self.j = 0
	}

	#disable boundary nilptr
	fn next(mut *self): (*K, *V) {
	Over:
		if self.j >= groupSize {
			self.i++
			self.j = 0
		}
		if self.i > self.groupMask {
			self.groups = nil
		}
		unsafe {
			if self.groups == nil {
				for {
					mut t := self.bw.next()
					if t == nil {
						ret nil, nil
					}
					if t.used == 0 {
						continue
					}
					// Snapshot the groups, and groupMask so that iteration remains valid
					// if the map is resized during iteration.
					self.groups = t.groups
					self.groupMask = t.groupMask
					self.i = 0
					self.j = 0
					break
				}
			}
			mut g := &self.groups[(self.i+self.offset)&self.groupMask]
			k := (self.j + self.offset) & (groupSize - 1)
			self.j++
			// Match full entries which have a high-bit of zero.
			if (g.ctrls.get(k)&ctrlEmpty) != ctrlEmpty {
				ret &g.keys[k], &g.values[k]
			}
		}
		goto Over
	}
}

// Walks sequentially for each table in the map.
struct tablewalker[K: comparable, V] {
	m:                *hmap[K, V]
	startIndex:       u32
	startGlobalDepth: u32
	index:            u32
	first:            bool

	originalGlobalDepth: u32
	originalLocalDepth:  u32
	originalIndex:       u32
}

impl tablewalker {
	// Initialize tablewalker for the map m.
	// The offset argument specifies the table to start iteration at (used to randomize iteration order).
	#disable boundary nilptr
	fn init(mut *self, offset: uintptr, mut &m: *hmap[K, V]) {
		self.m = m
		self.index = m.dir[offset&uintptr(m.tableCount()-1)].index

		// Loop termination is handled by remembering the start table index and
		// exiting when it is reached again. Note that the startIndex needs to be
		// adjusted to account for the directory growing during iteration (i.e.
		// due to a mutation), so we remember the starting global depth as well in
		// order to perform that adjustment. Whenever the directory grows by
		// doubling, every existing table index will be doubled.
		self.startIndex = self.index
		self.startGlobalDepth = m.globalDepth()
		self.first = true
	}

	#disable boundary nilptr
	unsafe fn next(mut *self): *table[K, V] {
		if self.m == nil {
			ret nil
		}

		// We iterate over the first table in a logical group of tables (i.e.
		// tables which share table.groups). The first table has the accurate
		// table.used field and those are also the tables that are stepped
		// through using tableStep().
		let mut t: *table[K, V]

		if self.first {
			t = &self.m.dir[self.index]
			self.first = false
		} else {
			// The size of the directory can grow if the yield function mutates
			// the map.  We want to iterate over each table once, and if a table
			// splits while we're iterating over it we want to skip over all of
			// the tables newly split from the one we're iterating over. We do
			// this by snapshotting the table's local depth and using the
			// snapshotted local depth to compute the table step.
			//
			// Note that t.index will also change if the directory grows. Consider
			// the directory below with a globalDepth of 2 containing 4 tables,
			// each of which has a localDepth of 2.
			//
			//    dir   t.index   t.localDepth
			//	+-----+---------+--------------+
			//	|  00 |       0 |            2 |
			//	+-----+---------+--------------+
			//	|  01 |       1 |            2 |
			//	+-----+---------+--------------+
			//	|  10 |       2 |            2 | <--- iteration point
			//	+-----+---------+--------------+
			//	|  11 |       3 |            2 |
			//	+-----+---------+--------------+
			//
			// If the directory grows during iteration, the index of the table
			// we're iterating over will change. If the table we're iterating
			// over split, then the local depth will have increased. Notice how
			// the table that was previously at index 1 now is at index 2 and is
			// pointed to by 2 directory entries: 010 and 011. The table being
			// iterated over which was previously at index 2 is now at index 4.
			// Iteration within a table takes a snapshot of the controls and
			// slots to make sure we don't miss keys during iteration or iterate
			// over keys more than once. But we also need to take care of the case
			// where the table we're iterating over splits. In this case, we need
			// to skip over the table at index 5 which can be done by computing
			// the tableStep using the table's depth prior to calling yield
			// which in this example will be 1<<(3-2)==2.
			//
			//    dir   t.index   t.localDepth
			//	+-----+---------+--------------+
			//	| 000 |       0 |            2 |
			//	+-----+         |              |
			//	| 001 |         |              |
			//	+-----+---------+--------------+
			//	| 010 |       2 |            2 |
			//	+-----+         |              |
			//	| 011 |         |              |
			//	+-----+---------+--------------+
			//	| 100 |       4 |            3 |
			//	+-----+---------+--------------+
			//	| 101 |       5 |            3 |
			//	+-----+---------+--------------+
			//	| 110 |       6 |            2 |
			//	+-----+         |              |
			//	| 111 |         |              |
			//	+-----+---------+--------------+

			// After iteration step, t is no longer valid. We determine the next
			// table to iterate over using the t.index we cached before calling
			// yield and adjusting for any directory growth that happened during
			// the iteration step.
			mut i := adjustTableIndex(self.originalIndex, self.m.globalDepth(), self.originalGlobalDepth)
			i += tableStep(self.m.globalDepth(), self.originalLocalDepth)
			i &= (self.m.tableCount() - 1)

			// Similar to the adjustment for t's index, we compute the starting
			// table's new index accounting for directory growth.
			adjustedStartIndex := adjustTableIndex(self.startIndex, self.m.globalDepth(), self.startGlobalDepth)
			if i == adjustedStartIndex {
				self.m = nil
				ret nil
			} else {
				self.index = i
			}

			t = &self.m.dir[self.index]
		}

		self.originalGlobalDepth = self.m.globalDepth()
		self.originalLocalDepth = t.localDepth
		self.originalIndex = t.index

		ret t
	}
}

// Number of tables to step over in the tables directory
// to reach the next different table. A table occupies 1 or more contiguous
// entries in the tables directory specified by the range:
//
//	[t.index:t.index+tableStep(m.globalDepth(), t.localDepth)]
fn tableStep(globalDepth: u32, localDepth: u32): u32 {
	const shiftMask = 31
	ret u32(1) << ((globalDepth - localDepth) & shiftMask)
}

// Adjusts the index of a table to account for the growth
// of the directory where index was captured at originalGlobalDepth and we're
// computing where that index will reside in the directory at
// currentGlobalDepth.
fn adjustTableIndex(index: u32, currentGlobalDepth: u32, originalGlobalDepth: u32): u32 {
	ret index * (1 << (currentGlobalDepth - originalGlobalDepth))
}

// Rounds capacity to the next power of 2.
fn normalizeCapacity(capacity: u32): u32 {
	ret u32(1) << min(bits::Len32(capacity-1), 31)
}

// Singleton for a single empty groupSize set of controls.
let emptyCtrls: [groupSize]group[int, int] = [group[int, int]{ctrls: ctrlGroup(bitsetEmpty)}, ...]
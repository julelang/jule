// Copyright 2025 The Jule Project Contributors. All rights reserved.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

use "std/internal/runtime"
use "std/internal/runtime/atomic"

type eventpollError: str

const (
	netpollClosing:     eventpollError = "descriptor is closed"
	netpollNotPollable: eventpollError = "not pollable"
	netpollDeadline:    eventpollError = "deadline exceeded"
)

struct polldescqp {
	pd:   &polldesc
	prev: &polldescqp
	next: &polldescqp
}

// A polldesc linked-list.
struct polldescq {
	head: &polldescqp
	tail: &polldescqp
}

impl polldescq {
	// Appends poll descriptor pd to the queue.
	#disable nilptr
	fn push(mut *self, mut pd: &polldesc): &polldescqp {
		mut qc := new(polldescqp)
		qc.pd = pd
		if self.tail == nil {
			self.head = qc
			self.tail = qc
		} else {
			qc.next = self.head
			self.head.prev = qc
			self.head = qc
		}
		ret qc
	}

	// Removes poll descriptor pd from the queue.
	#disable nilptr
	fn remove(mut *self, mut pd: &polldescqp) {
		if pd.prev != nil {
			pd.prev.next = pd.next
		} else {
			self.head = pd.next
		}
		if pd.next != nil {
			pd.next.prev = pd.prev
		} else {
			self.tail = pd.prev
		}
		pd.prev = nil
		pd.next = nil
	}
}

// The done indicates whether the I/O operation has already been completed
// by another thread before the worker enters the poll/wait phase.
//
// This flag exists to handle a fundamental race in multi-threaded schedulers:
// an I/O event may be delivered and processed by a different thread
// between the moment an operation decides it must wait and the moment
// it actually enters the poll mechanism.
//
// This design prevents missed wake-ups and incorrect blocking in the presence
// of concurrent event handling across multiple scheduler threads.
let mut _c = c{}
let mut pollrace = unsafe { (&c)(&_c) }

// Represents a pollable file descriptor and its runtime state.
// It encapsulates coroutine coordination, lifetime management, deadlines,
// and linkage for internal poll descriptor queues.
//
// A polldesc is typically owned by the poller and may be referenced by
// multiple coroutines concurrently. Its lifetime is governed by reference
// counting to ensure safe reuse and deferred cleanup.
struct polldesc {
	fd:    u64     // File descriptor associated with this poll descriptor.
	fdseq: uintptr // Atomic seq protects against stale pollDesc.

	mu:        mutex       // Mutex protecting all mutable fields below.
	rcp:       &c          // A fake smart-pointer for stack allocated coroutine waiting for read readiness.
	wcp:       &c          // A fake smart-pointer for stack allocated coroutine waiting for write readiness.
	pp:        &p          // Owning P.
	procq:     &polldescqp // The queue node.
	eventerr:  bool
	closing:   bool
	rdeadline: i64 // Deadline (absolute timestamp) for read operations.
	wdeadline: i64 // Deadline (absolute timestamp) for write operations.
	deadline:  i64 // General deadline applying to both read and write operations.
	link:      &polldesc
}

impl polldesc {
	#disable nilptr
	fn setDeadline(mut *self, mut deadline: i64) {
		self.mu.lock()
		if deadline == 0 {
			self.deadline = 0
			self.mu.unlock()
			ret
		}
		deadline = nanotime() + deadline
		if deadline <= 0 {
			// Overflows the size of the integer, use maximum possible value.
			deadline = 1<<63 - 1
		}
		self.deadline = deadline
		self.mu.unlock()
		mut m := gett()
		m.pp.timers.push(&timer{when: deadline})
	}

	#disable nilptr
	fn setReadDeadline(mut *self, mut deadline: i64) {
		self.mu.lock()
		if deadline == 0 {
			self.rdeadline = 0
			self.mu.unlock()
			ret
		}
		deadline = nanotime() + deadline
		if deadline <= 0 {
			// Overflows the size of the integer, use maximum possible value.
			deadline = 1<<63 - 1
		}
		self.rdeadline = deadline
		self.mu.unlock()
		mut m := gett()
		m.pp.timers.push(&timer{when: deadline})
	}

	#disable nilptr
	fn setWriteDeadline(mut *self, mut deadline: i64) {
		self.mu.lock()
		if deadline == 0 {
			self.wdeadline = 0
			self.mu.unlock()
			ret
		}
		deadline = nanotime() + deadline
		if deadline <= 0 {
			// Overflows the size of the integer, use maximum possible value.
			deadline = 1<<63 - 1
		}
		self.wdeadline = deadline
		self.mu.unlock()
		mut m := gett()
		m.pp.timers.push(&timer{when: deadline})
	}

	#disable nilptr boundary
	fn evict(mut *self) {
		let mut batch: [2]c
		mut n := u32(0)
		self.mu.lock()
		eventpollunblock(self, 'r', &batch[0], &n)
		eventpollunblock(self, 'w', &batch[1], &n)
		self.mu.unlock()
		mut m := gett()
		runqputbatch(m.pp, unsafe { &(*(*[prunqsize]c)(&batch)) }, 0, n)
	}
}

#disable nilptr
async fn eventpollwait(mut pd: &polldesc, mode: i32)! {
	if mode != 'r' && mode != 'w' {
		panic("runtime: invalid poll mode")
	}
	mut m := gett()
	pd.mu.lock()
	// Check for deadline because scheduler will remove it
	// from the timers after expiration.
	now := nanotime()
	match {
	| pd.deadline != 0 && pd.deadline <= now:
		pd.mu.unlock()
		error(netpollDeadline)
	| mode == 'r' && pd.rdeadline != 0 && pd.rdeadline <= now:
		pd.mu.unlock()
		error(netpollDeadline)
	| mode == 'w' && pd.wdeadline != 0 && pd.wdeadline <= now:
		pd.mu.unlock()
		error(netpollDeadline)
	}
	// Check for completion signals first.
	if mode == 'r' && pd.rcp == pollrace {
		pd.rcp = nil
		pd.mu.unlock()
		ret
	}
	if mode == 'w' && pd.wcp == pollrace {
		pd.wcp = nil
		pd.mu.unlock()
		ret
	}
	// Check for closing FD.
	if pd.closing {
		pd.mu.unlock()
		error(netpollClosing)
	}
	// Report an event scanning error only on a read event.
	// An error on a write event will be captured in a subsequent
	// write call that is able to report a more specific error.
	if mode == 'r' && pd.eventerr {
		pd.mu.unlock()
		error(netpollNotPollable)
	}
	mut mc := m.c
	match mode {
	| 'r':
		pd.rcp = unsafe { (&c)(&mc) }
	| 'w':
		pd.wcp = unsafe { (&c)(&mc) }
	}
	sched.enterpoll()
	park(&mc, uintptr(&pd.mu), reasonNA).await
	sched.exitpoll()
	eventpollgeterr(pd, mode)?
}

#disable nilptr
async fn eventpollwaitcanceled(mut pd: &polldesc, mode: i32) {
	// This function is used only on windows after a failed attempt to cancel
	// a pending async IO operation. Wait for ioready, ignore closing or timeouts.
	if mode != 'r' && mode != 'w' {
		panic("runtime: invalid poll mode")
	}
	mut m := gett()
	pd.mu.lock()
	// Check for completion signals first.
	if mode == 'r' && pd.rcp == pollrace {
		pd.rcp = nil
		pd.mu.unlock()
		ret
	}
	if mode == 'w' && pd.wcp == pollrace {
		pd.wcp = nil
		pd.mu.unlock()
		ret
	}
	sched.enterpoll()
	for {
		mut tc := m.c
		match mode {
		| 'r':
			pd.rcp = unsafe { (&c)(&tc) }
		| 'w':
			pd.wcp = unsafe { (&c)(&tc) }
		}
		park(&tc, uintptr(&pd.mu), reasonNA).await
		pd.mu.lock()
		// Check for relevant coroutine.
		// If any deadline exist on this pd, a spurious wakeup may occur.
		if mode == 'r' && pd.rcp == nil {
			pd.mu.unlock()
			break
		}
		if mode == 'w' && pd.wcp == nil {
			pd.mu.unlock()
			break
		}
	}
	sched.exitpoll()
}

// Throws error if needed after polling for an event.
#disable nilptr
fn eventpollgeterr(mut pd: &polldesc, mode: i32)! {
	if mode == 'r' && pd.rcp != nil && pd.rcp != pollrace {
		pd.rcp = nil
		error(netpollDeadline)
	}
	if mode == 'w' && pd.wcp != nil && pd.wcp != pollrace {
		pd.wcp = nil
		error(netpollDeadline)
	}
}

#disable nilptr
fn eventpollunblock(mut &pd: *polldesc, mode: i32, mut &cp: *c, mut &delta: *u32) {
	mut &pdcp := &pd.rcp
	if mode == 'w' {
		unsafe { *(&pdcp) = &pd.wcp }
	}
	if *pdcp != nil && *pdcp != pollrace {
		*cp = **pdcp
		if delta != nil {
			*delta++
		}
		*pdcp = nil
	} else {
		*pdcp = pollrace
	}
}

// Adds notified coroutines of pd to toRun according to mode.
// It assumes pd mutex is locked.
#disable nilptr boundary
fn pollnotified(mut &pd: *polldesc, mode: i32, mut &toRun: *[prunqsize]c, mut &i: *u32) {
	match mode {
	| 'r':
		eventpollunblock(pd, 'r', &(*toRun)[*i], i)
	| 'w':
		eventpollunblock(pd, 'w', &(*toRun)[*i], i)
	| 'r' + 'w':
		eventpollunblock(pd, 'r', &(*toRun)[*i], i)
		eventpollunblock(pd, 'w', &(*toRun)[*i], i)
	|:
		panic("unimplemented case")
	}
}

// Returns a polldesc for the fd.
#disable nilptr
fn eventpollnew(fd: u64)!: &polldesc {
	mut pd := pollcache.alloc()
	eventpollopen(fd, &(*pd)) else {
		pollcache.free(pd)
		error(error)
	}
	pd.fd = fd
	mut m := gett()
	pd.pp = m.pp
	pd.pp.pdsmu.lock()
	pd.procq = pd.pp.pds.push(pd)
	pd.pp.pdsmu.unlock()
	ret pd
}

// This function should be called when an fd associated with pd,
// previously obtained from eventpollnew, is closed.
#disable nilptr
fn eventpollfree(mut pd: &polldesc) {
	pollcache.free(pd)
	eventpollclose(pd.fd)
}

struct pollcachepool {
	mu:    mutex
	first: &polldesc
	// polldesc objects must be type-stable,
	// because we can get ready notification from epoll/kqueue
	// after the descriptor is closed/reused.
	// Stale notifications are detected using seq variable,
	// seq is incremented when descriptor is reused.
}

impl pollcachepool {
	#disable nilptr
	fn alloc(mut *self): &polldesc {
		self.mu.lock()
		if self.first == nil {
			mut pd := new(polldesc)
			unsafe {
				// Must be in non-GC memory because can be referenced
				// only from epoll/kqueue internals.
				// Remove RC pointer and disable GC for allocation.
				// Allocated polldescs are never will be deallocated.
				// Some tools like Valgrind may detect as memory leak,
				// but this was done on purpose.
				mut p := (*runtime::Smartptr[polldesc])(&pd)
				_RCFree(p.Ref)
				p.Ref = nil
			}
			pd.link = self.first
			self.first = pd
		}
		mut pd := self.first
		pd.eventerr = false
		pd.closing = false
		self.first = pd.link
		self.mu.unlock()
		ret pd
	}

	#disable nilptr
	fn free(mut *self, mut pd: &polldesc) {
		pd.mu.lock()

		// Set closing state.
		pd.closing = true

		// Unpark waiter coroutines, if any.
		if pd.rcp != nil && pd.rcp != pollrace {
			unpark(&(*pd.rcp))
		}
		if pd.wcp != nil && pd.wcp != pollrace {
			unpark(&(*pd.wcp))
		}
		pd.rcp = nil
		pd.wcp = nil

		// Increment the fdseq field, so that any currently
		// running eventpoll calls will not mark pd as ready.
		mut fdseq := atomic::Load(&pd.fdseq, atomic::Acquire)
		fdseq = (fdseq + 1) & (1<<taggedPointerBits - 1)
		atomic::Store(&pd.fdseq, fdseq, atomic::Release)

		// Remove processor.
		if pd.pp != nil {
			pd.pp.pdsmu.lock()
			pd.pp.pds.remove(pd.procq)
			pd.pp.pdsmu.unlock()
			pd.pp = nil
		}

		// Clean deadlines.
		pd.rdeadline = 0
		pd.wdeadline = 0
		pd.deadline = 0

		pd.mu.unlock()

		self.mu.lock()
		pd.link = self.first
		self.first = pd
		self.mu.unlock()
	}
}

// General eventpoll cache for pds.
let mut pollcache = pollcachepool{}
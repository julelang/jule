// Copyright 2025 The Jule Project Contributors. All rights reserved.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

use "std/internal/cpu"
use "std/internal/runtime/atomic"

// Holds one slot's sequence and the value.
// Vyukov protocol:
//	- Initially: sequence == index (EMPTY for the first pass)
//	- Producer at position pos writes value, then sequence = pos+1 (FULL)
//	- Consumer at position pos reads value, then sequence = pos+capacity (EMPTY for next wrap)
struct mpmcCell[T] {
	seq: u64
	val: T
}

// Padded counters to reduce false sharing between hot fields.
struct paddedU64 {
	v: u64
	_: [cpu::CacheLinePadSize - 8]byte
}

// Bounded, lock-free MPMC ring buffer.
// Primarily designed for channels.
// Implementation based on the Dmitry Vyukov's MPMC queue.
struct mpmcQueue[T] {
	cap:    u64
	act:    u64
	closed: u32
	buf:    []mpmcCell[T]

	// Hot cursors on separate cache lines
	_:          [cpu::CacheLinePadSize]byte
	enqueuePos: paddedU64
	_:          [cpu::CacheLinePadSize]byte
	dequeuePos: paddedU64
	_:          [cpu::CacheLinePadSize]byte
}

impl mpmcQueue {
	#disable boundary
	fn new(mut cap: u64): (q: mpmcQueue[T]) {
		q.act = cap
		q.cap = cap
		if q.cap < 2 {
			q.cap = 2
		}
		q.buf = make([]mpmcCell[T], q.cap)

		// Initialize per-slot sequence numbers to their indices
		mut i := u64(0)
		for i < q.cap; i++ {
			q.buf[i].seq = i
		}
		ret
	}

	#disable boundary nilptr
	fn enqueue(mut *self, mut &val: *T): (ok: bool, closed: bool, full: bool) {
		for {
			pos := atomic::Load(&self.enqueuePos.v, atomic::Relaxed)
			mut &c := unsafe { &(*(&self.buf[pos%self.cap])) }
			seq := atomic::Load(&c.seq, atomic::Acquire)
			dif := i64(seq - pos)

			if dif == 0 {
				// Try to claim slot
				if atomic::CompareAndSwapWeak(&self.enqueuePos.v, pos, pos+1, atomic::AcqRel, atomic::Relaxed) {
					// Slot is ours — now check closed
					if closed {
						// Roll back publication by marking slot empty again
						atomic::Store(&c.seq, pos, atomic::Release)
						ret false, true, false
					}

					c.val = *val
					atomic::Store(&c.seq, pos+1, atomic::Release)
					ret true, false, false
				}
				continue
			}

			if dif < 0 {
				// Slot not ready — MAY be full, confirm via cursors
				deq := atomic::Load(&self.dequeuePos.v, atomic::Acquire)
				if pos-deq >= self.cap {
					ret false, false, true // FULL is guaranteed
				}
				// Not guaranteed full → retry
				osyield()
				continue
			}

			// dif > 0 → another producer ahead
			osyield()
		}
	}

	// Attempts to pop one element; returns (zero, false) if empty.
	// Non-blocking; uses polite backoff on contention.
	#disable boundary nilptr
	fn dequeue(mut *self, mut &val: *T): (ok: bool, closed: bool) {
		for {
			// do not immediately return even if isclosed is true
			// we have to consume all queued data even if it is closed
			closed = atomic::Load(&self.closed, atomic::Relaxed) != 0
			pos := atomic::Load(&self.dequeuePos.v, atomic::Relaxed)
			mut &c := unsafe { &(*(&self.buf[pos%self.cap])) }
			seq := atomic::Load(&c.seq, atomic::Acquire)
			dif := i64(seq - (pos + 1))

			if dif == 0 {
				// slot is FULL for this consumer position: try to claim it
				if atomic::CompareAndSwapWeak(&self.dequeuePos.v, pos, pos+1, atomic::AcqRel, atomic::Relaxed) {
					// exclusive access to this slot
					*val = c.val
					// help GC: clear the slot's value
					let mut zero: T
					c.val = zero
					// mark EMPTY for the next wrap-around pass
					atomic::Store(&c.seq, pos+self.cap, atomic::Release)
					ret true, closed
				}
				// lost race → retry
			} else if dif < 0 {
				// seq < pos+1 ⇒ producer hasn't published yet; queue appears EMPTY
				ret false, closed
			} else {
				// another consumer is ahead / not our turn yet → polite backoff
				osyield()
			}
		}
	}

	#disable nilptr
	fn close(mut *self) {
		atomic::Store(&self.closed, 1, atomic::Release)
	}

	// Returns length of the availabe elements.
	#disable nilptr
	fn len(*self): int {
		h := atomic::Load(&self.dequeuePos.v, atomic::Acquire)
		t := atomic::Load(&self.enqueuePos.v, atomic::Acquire)
		ret int(t - h)
	}
}
// Copyright 2025 The Jule Project Contributors. All rights reserved.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

// SCHEDULER
//
// This scheduler implements a *cooperative*, *single-threadâ€“owned* coroutine
// execution loop for the Jule runtime. It is designed as a low-level runtime
// component, not a user-facing abstraction.
//
// The scheduler operates similarly to an event loop: it repeatedly polls a
// runnable coroutine, resumes it, and re-integrates it into the system based on
// its post-resume state.
//
// The scheduler does NOT implement coroutines itself. Instead, it operates on
// opaque coroutine handles provided by the runtime ABI.
//
// The scheduler is designed according to the C:M:P model:
//
//	C (Coroutine)
//		A suspendable state machine generated by the compiler.
//		Conceptually, it is an async function, but it is not part of the user's
//		ordinary control flow. It is detached and it may execute concurrently at
//		any time, and its scheduling is fully managed by the scheduler.
//		It behaves similarly to a typical thread.
//
//	M (Machine)
//		A real operating system thread.
//		It is responsible for executing a coroutine.
//		Only as many M instances may be created as permitted by COMAXPROCS.
//
//	P (Processor)
//		A scheduler processor. It owns its own local state.
//		An M must be paired with a P in order to execute a coroutine and perform
//		scheduling. An M can be paired with only one P at a time, and a P can be
//		paired with only one M at a time.
//
// Async functions may call other async functions and, through an await chain,
// form a chain of state machines. The scheduler does not take responsibility
// for lifetime management of each individual state machine. It is only
// responsible for managing the lifetime of coroutines (detached state machines).
//
// A coroutine may only be resumed by the scheduler.
// When a parked coroutine is woken up, it must be enqueued into the runnable
// coroutine queue. The scheduler decides when it will actually execute.
//
// STACK-ORIENTED COROUTINE HANDLING
//
// Jule runtime prioritizes avoiding heap allocations for coroutines.
// Each coroutine instance is used from the stack. When necessary,
// it can be copied or passed around via references/pointers.
// Since it is stack-oriented, it must be handled carefully.
// Otherwise, a stale coroutine copy may be used and cause critical issues.

use "std/internal/runtime"
use "std/internal/runtime/atomic"

extern fn __jule_retireDrain()
extern fn __jule_trampolineRun()

// Calls schedthread with P.
extern let __jule_trampoline_schedthread: *unsafe

// Returns the maximum number of CPUs that can be executing simultaneously.
fn COMAXPROCS(): int { ret numcpu }

const (
	// The maximum count of the runnable coroutine queue of a P.
	// Must be power of two.
	prunqsize = 256

	// The default budget of a C.
	// See [adjustbudget] and [p.budget] for more details.
	pbudget = 128

	// Indicates that an M is currently performing an eventpoll check.
	pollCheckIn = -1
)

// Implementation of the runnext field for a P.
struct runnext {
	used: bool // Indicates whether coro is meaningful.
	c:    c    // The coroutine.
}

// A coroutine processor.
struct p {
	// Prioritized coroutine, should run at next scheduler loop.
	runnext: runnext

	// A lock-free local list for the runnable coroutines.
	runqhead: u32
	runqtail: u32
	runq:     [prunqsize]c

	// Incremented on every scheduler call.
	schedtick: u32

	// Represents the scheduling quantum for the current C.
	// It is used to prevent Cs from monopolizing the execution M (starvation).
	//
	// Each time a C performs a productive or potentially blocking operation
	// (e.g., I/O, Mutex acquisition, channel communication), the budget is decremented.
	// When the budget reaches zero, the C is considered to have exhausted its
	// time slice and must yield execution back to the scheduler.
	// The scheduler resets this value when the C is re-scheduled for a new turn.
	budget: u32

	// Local timers of the P.
	timers: timerheap

	link: &p // Link to the next P.
}

// Attempts to decrement the current C's budget if possible and reports that
// yielding is not required (returns false). If the budget is already zero,
// it reports that yielding is required (returns true).
//
// Concurrency integrations that may cause starvation/fairness issues should use
// this function and integrate with the budget system.
//
// Typically, if adjustbudget reports that yielding is required, a yield call
// should be performed. In most cases this is lighter than using Yield,
// because yield pushes the current C to the tail of the current P's local queue,
// whereas Yield pushes it to the global queue. In most scenarios, yield is
// a sufficient solution.
//
// The following pattern is usually enough to integrate the budget system:
//
//	if adjustbudget() {
//		yield().await
//	}
//
// This integration is most commonly performed before the algorithm that
// requires it starts executing. For example, right before initiating an I/O
// read operation. This prevents a runaway read from being issued when the
// budget is already zero.
//
// Additionally, operations like I/O reads may block on eventpoll when no data
// is available, and when they are resumed, budgets will be refreshed. By
// checking the budget at the beginning of the algorithm, potential issues such
// as consuming a refreshed budget after unblocking are avoided proactively.
fn adjustbudget(): bool {
	mut m := gett()
	if m.pp.budget == 0 {
		ret true
	}
	m.pp.budget--
	ret false
}

// Scheduler is the global scheduling environment for the entire Jule program.
// It owns Ps, global run queues, and tracks system-wide
// polling and spinning states for load-balancing and sleep/wakeup decisions.
struct scheduler {
	mu: mutex

	// All P's of the runtime.
	// All Ps here remain at the count they were initially added with.
	// Each P is a real reference-counted smart pointer.
	allp: []&p

	pidle:  &p  // Idle M's waiting for work.
	npidle: i32 // Count of the pidle list.

	nm: i32 // Count of the all working Ms.

	midle:  &thread // Number of idle M's waiting for work.
	nmidle: i32     // Count of the midle list.

	// Global runnable queue.
	runq: &coroq

	// Stores the timestamp of the most recent eventpoll.
	// A value of -1 indicates that the next findRunnable invocation
	// must perform a non-blocking eventpoll if possible.
	// A value of -2 indicates that a coroutine is checking eventpoll.
	lastpoll: i64

	// Number of worker threads currently spinning (actively searching
	// for runnable coroutines instead of sleeping).
	nmspinning:   i32
	needspinning: i32

	// Number of coroutines currently polling for I/O.
	//
	// Must be accessed atomically when used by scheduler logic.
	_ncpoll: i32

	// Number of coroutines currently in blocking call.
	//
	// Must be accessed atomically when used by scheduler logic.
	_ncblock: int

	// Number of coroutines currently working.
	//
	// Must be accessed atomically when used by scheduler logic.
	_ncrun: int
}

impl scheduler {
	// Returns the number of coroutines currently polling for I/O.
	// Used by the scheduler to make global scheduling decisions.
	fn ncpolling(*self): i32 {
		ret atomic::Load(&self._ncpoll, atomic::Acquire)
	}

	// Marks that a coroutine is about to enter an I/O polling state.
	// Must be called immediately before the coroutine blocks on I/O.
	fn enterblock(mut *self) {
		atomic::Add(&self._ncblock, 1, atomic::Relaxed)
	}

	// Marks that a coroutine has exited an I/O polling state.
	// Must be called immediately after the coroutine finishes polling.
	fn exitblock(mut *self) {
		atomic::Add(&self._ncblock, -1, atomic::Relaxed)
	}

	// Returns the number of coroutines currently polling for I/O.
	// Used by the scheduler to make global scheduling decisions.
	fn ncblocking(*self): int {
		ret atomic::Load(&self._ncblock, atomic::Relaxed)
	}

	// Marks that a worker thread is entering a coroutine runner state.
	fn enterrun(mut *self) {
		atomic::Add(&self._ncrun, 1, atomic::Relaxed)
	}

	// Marks that a worker thread has exited the runner state.
	fn exitrun(mut *self) {
		atomic::Add(&self._ncrun, -1, atomic::Relaxed)
	}

	// Returns the number of worker threads currently runs a coroutine.
	fn ncrunning(*self): int {
		ret atomic::Load(&self._ncrun, atomic::Relaxed)
	}
}

// A global scheduler instance of the program.
// Just a declaration, compiler will not initialize it by default.
// It will be initialized by the runtime.
let mut sched = scheduler{}

// Puts P on the pidle list.
//
// This releases ownership of P.
// Once sched.lock is released it is no longer safe to use P.
//
// sched.lock must be held.
#disable nilptr
fn pidleput(mut pp: &p) {
	pp.link = sched.pidle
	sched.pidle = pp
	atomic::Add(&sched.npidle, 1, atomic::Relaxed)
}

// Tries to get a P from the pidle list, acquiring ownership.
//
// sched.lock must be held.
#disable nilptr
fn pidleget(): &p {
	mut pp := sched.pidle
	if pp != nil {
		sched.pidle = pp.link
		atomic::Add(&sched.npidle, -1, atomic::Relaxed)
	}
	ret pp
}

// Tries to get a P from the pidle list, acquiring ownership.
// This is called by spinning Ms (or callers than need a spinning M) that have
// found work. If no P is available, this must synchronized with non-spinning
// Ms that may be preparing to drop their P without discovering this work.
//
// sched.lock must be held.
fn pidlegetSpinning(): &p {
	mut pp := pidleget()
	if pp == nil {
		// We found work that we cannot take, we must synchronize with non-spinning
		// Ms that may be preparing to drop their P.
		atomic::Store(&sched.needspinning, 1, atomic::Release)
		ret nil
	}

	ret pp
}

// Tries to add one more P to execute C's.
// Called when a C is made runnable (newproc, ready).
fn wakep() {
	// Be conservative about spinning threads,
	// only start one if none exist already.
	if atomic::Load(&sched.nmspinning, atomic::Relaxed) != 0 ||
		!atomic::CompareAndSwap(&sched.nmspinning, 0, 1, atomic::Relaxed, atomic::Relaxed) {
		ret
	}
	sched.mu.lock()
	mut pp := pidlegetSpinning()
	if pp == nil {
		if atomic::Add(&sched.nmspinning, -1, atomic::Relaxed) < 0 {
			panic("wakep: negative nmspinning")
		}
		sched.mu.unlock()
		ret
	}
	sched.mu.unlock()

	startm(pp, true, false)
}

// Try get a batch of C's from the global runnable queue.
// sched.runqmu must be held and will not be released before return.
fn globrunqget(mut pp: &p, max: i32, mut &cp: *c): bool {
	if sched.runq.len == 0 {
		ret false
	}

	mut n := sched.runq.len/COMAXPROCS() + 1
	if n > sched.runq.len {
		n = sched.runq.len
	}
	if max > 0 && n > int(max) {
		n = int(max)
	}
	if n > len(pp.runq)/2 {
		n = len(pp.runq) / 2
	}

	*cp = sched.runq.head.c
	sched.runq.remove(sched.runq.head)
	n--
	for n > 0; n-- {
		mut c := sched.runq.head.c
		sched.runq.remove(sched.runq.head)
		runqput(pp, &c, false)
	}
	ret true
}

// Tries to put cp on the local runnable queue.
// If next is false, it adds cp to the tail of the runnable queue if runnext is used.
// If next is true, runqput puts cp in the pp.runnext slot.
// If the run queue is full, runnext puts cp on the global queue.
// Executed only by the owner P.
#disable nilptr boundary
fn runqput(mut pp: &p, mut &cp: *c, next: bool): (overflow: bool) {
	// pp is nil, the caller is probably a blocking thread pool worker thread.
	// Not an actual M, it is not have a P by design.
	// Nothing to do, overflow to the global runnable queue.
	if pp == nil {
		sched.mu.lock()
		sched.runq.push(cp)
		sched.mu.unlock()
		wakep()
		ret true
	}

	if next && !pp.runnext.used {
		// The runnext slot is empty, use it.
		pp.runnext.c = *cp
		pp.runnext.used = true
		ret false
	}
	// The runnext slot is not empty, but it is prioritized.
	// So we tried moving it to local queue and done.
	// Now we can use the runnext slot.
	let mut oldc: c
	if next {
		oldc = pp.runnext.c
		pp.runnext.c = *cp
		pp.runnext.used = true
		// Kick the old runnext out to the regular run queue.
		unsafe { *(&cp) = &oldc }
	}
	for {
		h := atomic::Load(&pp.runqhead, atomic::Acquire) // load-acquire, synchronize with consumers
		t := pp.runqtail
		if t-h < prunqsize {
			pp.runq[t&(prunqsize-1)] = *cp
			atomic::Store(&pp.runqtail, t+1, atomic::Release) // store-release, makes the item available for consumption
			ret false
		}
		if runqputslow(pp, cp, h, t) {
			ret true
		}
		// The queue is not full, now the put above must succeed.
	}
}

// Put cp and a batch of work from local runnable queue on global queue.
// Executed only by the owner P.
#disable nilptr boundary
fn runqputslow(mut pp: &p, mut &cp: *c, h: u32, t: u32): bool {
	let mut batch: [prunqsize/2 + 1]c

	// First, grab a batch from local queue.
	mut n := t - h
	n = n / 2
	if n != prunqsize/2 {
		panic("runqputslow: queue is not full")
	}
	mut i := u32(0)
	for i < n; i++ {
		batch[i] = pp.runq[(h+i)&(prunqsize-1)]
	}
	if !atomic::CompareAndSwap(&pp.runqhead, h, h+n, atomic::Release, atomic::Relaxed) { // cas-release, commits consume
		ret false
	}
	batch[n] = *cp

	// Put batch to global runnable queue.
	batchn := i32(n + 1)
	sched.mu.lock()
	mut bi := i32(0)
	for bi < batchn; bi++ {
		sched.runq.push(&batch[bi])
	}
	sched.mu.unlock()

	wakep()

	ret true
}

// Puts all the coroutines on batch on the local runnable queue.
// The queue must have enough space for batch.
// Batch is a ring buffer starting at batchHead.
// Executed only by the owner P.
#disable nilptr boundary
fn runqputbatch(mut pp: &p, mut &batch: *[prunqsize]c, batchHead: u32, mut bsize: u32) {
	h := atomic::Load(&pp.runqhead, atomic::Acquire)
	mut t := pp.runqtail
	mut n := batchHead
	for n < bsize && t-h < prunqsize {
		pp.runq[t&(prunqsize-1)] = (*batch)[n]
		t++
		n++
	}
	bsize -= n
	atomic::Store(&pp.runqtail, t, atomic::Release)
	if bsize != 0 {
		panic("batch size corruption")
	}
}

// Adds each runnable C on the batch to some run queue.
// If there is no current P, they are added to the global queue,
// and up to npidle M's are started to run them. Otherwise,
// for each idle P, this adds a C to the global queue and starts an M.
// Any remaining C's are added to the current P's local run queue.
// This may temporarily acquire sched.lock.
#disable nilptr boundary
fn injectclist(mut &batch: *[prunqsize]c, batchStart: u32, bsize: u32) {
	if batchStart >= bsize {
		ret
	}
	startIdle := fn(n: u32) {
		mut i := u32(0)
		for i < n; i++ {
			sched.mu.lock()

			mut pp := pidlegetSpinning()
			if pp == nil {
				sched.mu.unlock()
				break
			}

			startm(pp, false, true)
			sched.mu.unlock()
		}
	}

	mut m := gett()
	if m == nil || m.pp == nil {
		sched.mu.lock()
		mut i := batchStart
		for i < bsize; i++ {
			sched.runq.push(&(*batch)[i])
		}
		sched.mu.unlock()
		startIdle(bsize)
		ret
	}

	npidle := u32(atomic::Load(&sched.npidle, atomic::Relaxed))
	mut n := u32(0)
	mut i := batchStart
	for n < npidle && i < bsize {
		n++
		i++
	}
	if n > 0 {
		sched.mu.lock()
		i = u32(0)
		for i < n; i++ {
			sched.runq.push(&(*batch)[batchStart+i])
		}
		sched.mu.unlock()
		startIdle(n)
	}

	if n < bsize {
		runqputbatch(m.pp, batch, batchStart+n, bsize)
	}

	wakep()
	ret
}

// Gets a coroutine from local runnable queue and writes to cp.
// Executed only by the owner P.
#disable nilptr boundary
fn runqget(mut pp: &p, mut &cp: *c): bool {
	// If there's a runnext, it's the next coroutine to run.
	if pp.runnext.used {
		pp.runnext.used = false
		*cp = pp.runnext.c
		ret true
	}
	for {
		h := atomic::Load(&pp.runqhead, atomic::Acquire) // load-acquire, synchronize with other consumers
		t := pp.runqtail
		if t == h {
			ret false
		}
		// Must load *before* acquiring the slot as slot may be overwritten
		// immediately after acquiring. This load is NOT required to be
		// atomic even-though it may race with an overwrite as we only return
		// true if we win the race below guaranteeing we had no race during
		// our read. If we loose the race then `cp` could be corrupt due to
		// read-during-write race but as coroutine is trivially destructible
		// this does not matter.
		//
		// We cannot store `&coro` instead of `coro` because of the race.
		// Otherwise, RC counting may be corrupted and cause some critical
		// issues, such as use-after-free.
		*cp = pp.runq[h&(prunqsize-1)]
		if atomic::CompareAndSwap(&pp.runqhead, h, h+1, atomic::Release, atomic::Relaxed) { // cas-release, commits consume
			ret true
		}
	}
}

// Grabs a batch of coroutines from pp's runnable queue into batch.
// Batch is a ring buffer starting at batchHead.
// Returns number of grabbed coroutines.
// Can be executed by any P.
#disable nilptr boundary
fn runqgrab(mut pp: &p, mut &batch: *[prunqsize]c, batchHead: u32): u32 {
	for {
		h := atomic::Load(&pp.runqhead, atomic::Acquire) // load-acquire, synchronize with other consumers
		t := atomic::Load(&pp.runqtail, atomic::Acquire) // load-acquire, synchronize with the producer
		mut n := t - h
		n = n - n/2
		if n == 0 {
			ret 0
		}
		if n > prunqsize/2 { // read inconsistent h and t
			continue
		}
		mut i := u32(0)
		for i < n; i++ {
			// See comment of runqget about read-during-write race.
			(*batch)[(batchHead+i)&(prunqsize-1)] = pp.runq[(h+i)&(prunqsize-1)]
		}
		if atomic::CompareAndSwap(&pp.runqhead, h, h+n, atomic::Release, atomic::Relaxed) { // cas-release, commits consume
			ret n
		}
	}
}

// Steal half of elements from local runnable queue of pp2
// and put onto local runnable queue of pp.
// Writes one of the stolen coroutines to cp.
// Reports false if failed, otherwise true.
#disable nilptr boundary
fn runqsteal(mut pp: &p, mut pp2: &p, mut &cp: *c): bool {
	t := pp.runqtail
	mut n := runqgrab(pp2, &pp.runq, t)
	if n == 0 {
		ret false
	}
	n--
	*cp = pp.runq[(t+n)&(prunqsize-1)]
	if n == 0 {
		ret true
	}
	h := atomic::Load(&pp.runqhead, atomic::Acquire) // load-acquire, synchronize with consumers
	if t-h+n >= prunqsize {
		panic("runqsteal: runq overflow")
	}
	atomic::Store(&pp.runqtail, t+n, atomic::Release) // store-release, makes the item available for consumption
	ret true
}

// Tries to steal a batch of coroutines for pp.
// Writes one of the stolen coroutines to cp.
// Reports false if failed, otherwise true.
fn stealWork(mut pp: &p, mut &cp: *c): bool {
	const StealTries = 4
	mut n := 0
	pcount := COMAXPROCS()
	startIdx := int(rand() % u64(pcount))
	for n < StealTries; n++ {
		mut i := 0
		mut idx := startIdx
		for i < pcount; i, idx = i+1, (idx+1)%pcount {
			mut pp2 := sched.allp[idx]
			if pp == pp2 {
				continue
			}
			ok := runqsteal(pp, pp2, cp)
			if ok {
				ret true
			}
		}
	}
	ret false
}

// Selects a runnable coroutine for execution.
//
// This function is one of the core components of the coroutine scheduler.
// It attempts to find exactly one coroutine to run,
// following a strict priority order:
//
// The function guarantees that at most one coroutine is returned via
// `coro`; any additional runnable coroutines are enqueued back into
// appropriate run queues.
//
// We use `runqputbatch` instead of `injectclist` for eventpoll results.
//
// Using `injectclist` would push these coroutines onto the global run queue,
// requiring sched.lock and increasing contention under high connection
// counts (e.g. many concurrent network wakeups).
//
// runqputbatch places the goroutines directly onto the local run queue of
// the current P, preserving cache locality and avoiding global scheduler
// contention. This improves throughput for event-driven workloads without
// violating scheduler invariants.
//
// Sysmon and other scheduler systems intentionally will use the
// `injectclist`, as they operate without a P and must ensure global progress.
#disable nilptr boundary
fn findRunnable(): (coro: c, ok: bool) {
	mut m := gett()
	mut pp := m.pp

top:
	// Check the global runnable queue once in a while to ensure fairness.
	// Otherwise two coroutines can completely occupy the local runqueue
	// by constantly respawning each other.
	if pp.schedtick&63 == 0 && sched.runq.len > 0 {
		sched.mu.lock()
		ok = globrunqget(pp, 1, &coro)
		sched.mu.unlock()
		if ok {
			ret coro, true
		}
	}

	// Local runnable queue.
	ok = runqget(pp, &coro)
	if ok {
		ret coro, true
	}

	// Local queue empty: attempt to fetch a batch from the global run queue.
	//
	// The batch size is capped to prunqsize to preserve
	// locality and reduce contention on the global lock.
	if sched.runq.len > 0 {
		sched.mu.lock()
		ok = globrunqget(pp, 0, &coro)
		sched.mu.unlock()
		if ok {
			ret coro, true
		}
	}

	// Poll I/O.
	// This eventpoll is only an optimization before we resort to stealing.
	// We can safely skip it if there are no waiters or a thread is blocked
	// in eventpoll already.
	if sched.ncpolling() > 0 && atomic::Load(&sched.lastpoll, atomic::Acquire) != pollCheckIn {
		// eventpoll guarantees len(toRun) <= local run queue capacity.
		let mut toRun: [prunqsize]c
		bn := eventpoll(0, &toRun) // Use 0-delay to perform a non-blocking poll.
		// If any coroutines are ready, return the first and enqueue the rest.
		// Timers possibly expired here will be handled in a later iteration.
		if bn > 0 {
			coro = toRun[0]
			runqputbatch(pp, &toRun, 1, bn)
			eventpollAdjustWaiters(-i32(bn))
			ret coro, true
		}
	}

	// No work found locally or globally: attempt work stealing.
	//
	// Stealing is limited to a small fixed number of attempts to
	// avoid excessive contention and cache thrashing.
	if m.spinning || 2*atomic::Load(&sched.nmspinning, atomic::Relaxed) < i32(COMAXPROCS())-atomic::Load(&sched.npidle, atomic::Relaxed) {
		if !m.spinning {
			becomeSpinning(m)
		}
		ok = stealWork(pp, &coro)
		if ok {
			ret coro, true
		}
	}

	if !m.spinning && atomic::Load(&sched.needspinning, atomic::Acquire) == 1 {
		becomeSpinning(m)
		goto top
	}

	if m.spinning {
		m.spinning = false
		if atomic::Add(&sched.nmspinning, -1, atomic::Relaxed) < 0 {
			panic("findrunnable: negative nmspinning")
		}
	}

	// No runnable coroutine found yet.
	// Determine how long we can block based on timers and deadlines.
	// If pollUntil is -1, eventpoll will block indefinitely.
	mut timer, pollUntil := pp.timers.check()
	if timer != nil {
		if timer.ifunc != nil {
			// This timer is an runtime internal timer associated with
			// a raw function to be invoked when timer fired.
			// Invoke the function and try again to find a runnable coroutine.
			timer.unlockAndFire()
			goto top
		} else {
			// The timer associated with a coroutine,
			// let the scheduler handle this.
			mut &tc := unsafe { &(*(*c)(timer.arg)) }
			timer.mu.unlock()
			coro = *tc
			ok = true
			ret
		}
	}

	// Final step: eventpoll.
	if pollUntil != -1 || sched.ncpolling() > 0 && atomic::Swap(&sched.lastpoll, pollCheckIn, atomic::AcqRel) != pollCheckIn {
		// eventpoll guarantees len(toRun) <= local run queue capacity.
		let mut toRun: [prunqsize]c
		bn := eventpoll(pollUntil, &toRun)
		now := nanotime()
		atomic::Store(&sched.lastpoll, now, atomic::Release)
		if bn > 0 {
			coro = toRun[0]
			runqputbatch(pp, &toRun, 1, bn)
			eventpollAdjustWaiters(-i32(bn))
			ret coro, true
		}
	}

	ret coro, false
}

// A coroutine scheduler routine.
// Works like and event-loop, looks for a coroutine and executes it.
#disable nilptr
fn schedule() {
	mut m := gett()
Sched:
	m.pp.schedtick = 0
	for {
		// Poll a coroutine to run.
		mut c, ok := findRunnable()
		// findRunnable failed to find a runnable coroutine.
		if !ok {
			// There are coroutines polling for I/O and
			// there is no M looking for eventpoll.
			if sched.ncpolling() > 0 &&
				atomic::Load(&sched.lastpoll, atomic::Acquire) != pollCheckIn {
				continue
			}
			// There are timers in this worker, clean them.
			// Normally, eventpoll returns a timer if exist.
			// Somehow, a spurious wakeup occurred.
			m.pp.timers.mu.lock()
			ntimers := m.pp.timers.len()
			m.pp.timers.mu.unlock()
			if ntimers > 0 {
				continue
			}
			// We tried enough.
			// Nothing to do, break the loop.
			// Park the M.
			if m.spinning {
				m.spinning = false
				if atomic::Add(&sched.nmspinning, -1, atomic::Relaxed) < 0 {
					panic("findrunnable: negative nmspinning")
				}
			}
			break
		}
		// This thread is going to run a coroutine and is not spinning anymore,
		// so if it was marked as spinning we need to reset it now and potentially
		// start a new spinning M.
		if m.spinning {
			resetspinning()
		}

		// We will resume a coroutine, increase scheduler tick.
		m.pp.schedtick++

		// Refresh the budget for the new C.
		m.pp.budget = pbudget

		// Assign C to M.
		m.c = c
		m.c.state |= coroRunning

		// Run C.
		sched.enterrun()
		resume(&m.c)
		trampolinerun()
		sched.exitrun()
		retiredrain() // Destroy the retire coroutines.

		// In general, `m.c` will not be updated by `park`.
		// But empty select statements yield coroutine indefinitely,
		// so it will not be runnable again. To prevent memory leak,
		// because of coroutine handle allocations, it uses `m.c` as
		// the park coroutine. So we can catch it here.
		if m.c.state&reasonSelectEmpty == reasonSelectEmpty {
			close(&m.c)
		}
	}
	stopm(m)
	goto Sched
}

// Start point of a M.
// When a M is first started, it starts from here.
// This function called by the trampoline, with M.
// The mp is a pointer to the M.
#disable nilptr
#export "__jule_schedthread"
fn schedthread(mut mp: *unsafe) {
	// Set thread-local data before scheduling.
	mut m := unsafe { (&thread)((*thread)(mp)) }
	sett(m)

	schedule()
}

// A hint function for the compiler.
// At the call site of this function, the compiler emits a runtime park call
// and performs the parking operation for the cp. It also releases
// the mutex mu via `mutexunlock`. This is a low-level mechanism and must be
// used with great care.
async fn runtimepark(cp: *c, mu: u64) {}

// Suspends the current cp and yields the CPU.
// The cp should be the coroutine will be used for resume(cp).
// If the mu is not zero, assumes it already locked and releases before yield.
// The coroutine can be made runnable again by calling [unpark].
//
// The first parameters must be the resume-coroutime,
// where park handle will be written, and mutex address.
// Following parameters may be change.
//
// After park, the `gett().c` will not be affected.
// So, scheduler cannot reach to up-to-date information of the coroutine.
#disable nilptr
async fn park(mut &cp: *c, mu: uintptr, mut reason: u32) {
	cp.state |= coroSuspended | reason
	runtimepark(cp, u64(mu)).await
}

// Same as park, but takes a tagged pointer for mu.
#disable nilptr
async fn park2(mut &cp: *c, mu: taggedPointer, mut reason: u32) {
	cp.state |= coroSuspended | reason
	runtimepark(cp, u64(mu)).await
}

// Makes cp ready to be runnable.
// Like unpark, but it will not enqueue cp.
#disable nilptr
fn ready(mut &cp: *c) {
	cp.state &= ^reasonMask
}

// Makes cp runnable.
#disable nilptr
fn unpark(mut &cp: *c) {
	ready(cp)
	mut m := gett()
	runqput(m.pp, cp, true)
	wakep()
}

// Puts the current coroutine sleep for duration dur.
#disable nilptr
async fn sleep(dur: sleepDuration) {
	mut m := gett()
	mut c := m.c
	when := nanotime() + dur
	mut t := timer.new(when, nil, uintptr(&c), 0)
	m.pp.timers.push(t)
	park(&c, 0, reasonSleep).await
}

// Destroys all retired coroutine frames for the current worker thread.
fn retiredrain() {
	extern.__jule_retireDrain()
}

// Run all queued coroutines until the queue is empty.
// This must be called by the scheduler to make progress after resume.
fn trampolinerun() {
	extern.__jule_trampolineRun()
}

// Yields the processor, allowing other coroutiens to run. It does not
// suspend the current coroutine, so execution resumes automatically.
#disable nilptr
async fn Yield() {
	// Push the coroutine to the global queue, otherwise the next
	// scheduler iteration after park may run it from the local queue.
	mut coroq := new(coroqc)
	coroq.c = gett().c
	sched.mu.lock()
	sched.runq.pushq(coroq)
	park(&coroq.c, uintptr(&sched.mu), reasonNA).await
}

// Like Yield, but puts the current C on the runq of
// the current P instead of the globrunq.
#disable nilptr
async fn yield() {
	mut m := gett()
	tg := taggedPointer(mutexunlockYield)
	park2(&m.c, tg, reasonNA).await
}

#disable nilptr boundary
fn schedinit() {
	// Initialize the global scheduler instance.
	sched = scheduler{}
	sched.runq = new(coroq)
	sched.nm = 1 // The main program thread.

	// We mutate sched here but mutex is not need to be acquired.
	// Because we are in the program initialize state, no concurrency risk.

	// Initialize Ps.
	maxprocs := i32(COMAXPROCS())
	sched.allp = make([]&p, maxprocs)
	sched.npidle = maxprocs - sched.nm
	mut i := i32(0)
	for i < maxprocs; i++ {
		mut pp := new(p)
		unsafe {
			// Must be in non-GC memory because can be referenced
			// from any runtime system during program lifetime.
			// Remove RC pointer and disable GC for allocation.
			// Allocated Ps are never will be deallocated.
			// Some tools like Valgrind may detect as memory leak,
			// but this was done on purpose.
			mut pptr := (*runtime::Smartptr[thread])(&pp)
			_RCFree(pptr.Ref)
			pptr.Ref = nil
		}
		sched.allp[i] = pp
		if i == 0 {
			// Set P of the main program thread.
			mainm.pp = pp
		} else {
			pp.link = sched.pidle
			sched.pidle = pp
		}
	}
}

// The sysmon (system-monitor) thread.
//
// This function handled like an anonymous function.
// Therefore the compiler will add *unsafe argument to
// the head of the argument list implicitly for anonymous function context.
// The operating system thread API passes a pointer to the function when calling it.
// In the final stage, this function's signature fits with the OS thread API.
//
// Since this function never returns, there should be no risk about return value.
#disable nilptr
fn sysmon() {
	for {
		// Sleep for the next period.
		threadsleep(_Millisecond * 10)
		now := nanotime()
		// eventpoll if not polled for more than 10ms and
		// there is a coroutine waiting for eventpoll.
		// If eventpoll is not triggered, all coroutines may accumulate in
		// local and global queues, leading to starvation.
		// The eventpoll must be checked regularly.
		if sched.ncpolling() > 0 {
			lastpoll := atomic::Load(&sched.lastpoll, atomic::Acquire)
			if lastpoll >= 0 && now-lastpoll > _Millisecond*10 {
				atomic::CompareAndSwap(&sched.lastpoll, lastpoll, now, atomic::AcqRel, atomic::Relaxed)
				let mut toRun: [prunqsize]c
				bn := eventpoll(0, &toRun)
				if bn > 0 {
					injectclist(&toRun, 0, bn)
					eventpollAdjustWaiters(-i32(bn))
				}
			}
		}
	}
}

// Schedules some M to run the P (creates an M if necessary).
// If P==nil, the behavior is undefined. P is always should be a valid pointer.
// If spinning is set, the caller has incremented nmspinning and must provide a
// P. startm will set m.spinning of the newly started M.
//
// Argument lockheld indicates whether the caller already acquired the
// scheduler lock. Callers holding the lock when making the call must pass
// true. The lock might be temporarily dropped, but will be reacquired before
// returning.
#disable nilptr
fn startm(mut pp: &p, spinning: bool, lockheld: bool) {
	if !lockheld {
		sched.mu.lock()
	}
	sched.nm++
	mut m := mget()
	if !lockheld {
		sched.mu.unlock()
	}
	if m == nil {
		m = newThread(roleM)
		m.pp = pp
		m.spinning = spinning
		unsafe {
			// Must be in non-GC memory because can be referenced
			// from any runtime system during program lifetime.
			// Remove RC pointer and disable GC for allocation.
			// Allocated threads are never will be deallocated.
			// Some tools like Valgrind may detect as memory leak,
			// but this was done on purpose.
			mut mptr := (*runtime::Smartptr[thread])(&m)
			_RCFree(mptr.Ref)
			mptr.Ref = nil
		}
		ok := unsafe { threadSpawn((*unsafe)(uintptr(extern.__jule_trampoline_schedthread)), (*thread)(m)) }
		if !ok {
			panic("runtime: M creation failed")
		}
		ret
	}
	m.pp = pp
	m.spinning = spinning
	m.parker.unpark()
}

fn checkdead() {
	// There is still working M.
	if sched.nm > 0 {
		ret
	}
	// Some coroutines are polling for event.
	// sysmon will wake a P.
	if sched.ncpolling() > 0 {
		ret
	}
	// Some coroutines working some blocking tasks.
	// When task complete, program can continue to run.
	if sched.ncblocking() > 0 {
		ret
	}
	panic("runtime: all coroutines are asleep - deadlock!")
}

// Stops execution of the M until new work is available.
// Returns with acquired P.
#disable nilptr
fn stopm(mut m: &thread) {
	sched.mu.lock()
	sched.nm--
	checkdead()
	pidleput(m.pp)
	mput(m)
	sched.mu.unlock()
	m.parker.park()
}

// Put M on midle list.
// sched.lock must be held.
#disable nilptr
fn mput(mut m: &thread) {
	m.link = sched.midle
	sched.midle = m
	sched.nmidle++
}

// Try to get an M from midle list.
// sched.lock must be held.
#disable nilptr
fn mget(): &thread {
	mut m := sched.midle
	if m != nil {
		sched.midle = m.link
		sched.nmidle--
	}
	ret m
}

#disable nilptr
fn becomeSpinning(mut m: &thread) {
	m.spinning = true
	atomic::Add(&sched.nmspinning, 1, atomic::Relaxed)
	atomic::Store(&sched.needspinning, 0, atomic::Release)
}

#disable nilptr
fn resetspinning() {
	mut m := gett()
	if !m.spinning {
		panic("resetspinning: not a spinning m")
	}
	m.spinning = false
	nmspinning := atomic::Add(&sched.nmspinning, -1, atomic::Release)
	if nmspinning < 0 {
		panic("findrunnable: negative nmspinning")
	}
	// M wakeup policy is deliberately somewhat conservative, so check if we
	// need to wakeup another P here.
	wakep()
}
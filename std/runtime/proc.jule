// Copyright 2025 The Jule Project Contributors. All rights reserved.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

// SCHEDULER
//
// This scheduler implements a *cooperative*, *single-threadâ€“owned* coroutine
// execution loop for the Jule runtime. It is designed as a low-level runtime
// component, not a user-facing abstraction.
//
// The scheduler operates similarly to an event loop: it repeatedly polls a
// runnable coroutine, resumes it, and re-integrates it into the system based on
// its post-resume state.
//
// The scheduler does NOT implement coroutines itself. Instead, it operates on
// opaque coroutine handles provided by the runtime ABI.
//
// The scheduler is designed according to the C:M:P model:
//
//	C (Coroutine)
//		A suspendable state machine generated by the compiler.
//		Conceptually, it is an async function, but it is not part of the user's
//		ordinary control flow. It is detached and it may execute concurrently at
//		any time, and its scheduling is fully managed by the scheduler.
//		It behaves similarly to a typical thread.
//
//	M (Machine)
//		A real operating system thread.
//		It is responsible for executing a coroutine.
//		Only as many M instances may be created as permitted by COMAXPROCS.
//
//	P (Processor)
//		A scheduler processor. It owns its own local state.
//		An M must be paired with a P in order to execute a coroutine and perform
//		scheduling. An M can be paired with only one P at a time, and a P can be
//		paired with only one M at a time.
//
// Async functions may call other async functions and, through an await chain,
// form a chain of state machines. The scheduler does not take responsibility
// for lifetime management of each individual state machine. It is only
// responsible for managing the lifetime of coroutines (detached state machines).
//
// A coroutine may only be resumed by the scheduler.
// When a parked coroutine is woken up, it must be enqueued into the runnable
// coroutine queue. The scheduler decides when it will actually execute.
//
// STACK-ORIENTED COROUTINE HANDLING
//
// Jule runtime prioritizes avoiding heap allocations for coroutines.
// Each coroutine instance is used from the stack. When necessary,
// it can be copied or passed around via references/pointers.
// Since it is stack-oriented, it must be handled carefully.
// Otherwise, a stale coroutine copy may be used and cause critical issues.

use "std/internal/runtime/atomic"

extern fn __jule_retireDrain()
extern fn __jule_trampolineRun()

// Returns the maximum number of CPUs that can be executing simultaneously.
fn COMAXPROCS(): int { ret numcpu }

const (
	// The maximum count of the runnable coroutine queue of a coroutine processor.
	// Must be power of two.
	coroprocRunQueueSize = 256
)

// Implementation of the runnext field for a coroutine processor.
struct runnext {
	used: bool // Indicates whether coro is meaningful.
	coro: coro // The coroutine.
}

// A coroutine processor.
struct coroproc {
	// Prioritized coroutine, should run at next scheduler loop.
	runnext: runnext

	// List of the runnable coroutines.
	runq: localrunq

	// Poll descriptors that are still alive.
	pdsmu: mutex
	pds:   polldescq

	// Local timers of the coroutine processor.
	timers: timerheap
}

const (
	pollCheckRequested = -1 // Indicates that an eventpoll check has been explicitly prioritized.
	pollCheckIn        = -2 // Indicates that an M is currently performing an eventpoll check.
)

// Scheduler is the global scheduling environment for the entire Jule program.
// It owns coroutine processors, global run queues, and tracks system-wide
// polling and spinning states for load-balancing and sleep/wakeup decisions.
struct scheduler {
	// All coroutine processors (logical execution units).
	// Each processor is responsible for running coroutines on a worker thread.
	procs: []&coroproc

	// Queue of idle coroutine processors that are not currently executing work.
	// Used to quickly assign work without scanning all processors.
	idleprocs: mpmcQueue[&coroproc]

	// Global runnable coroutine queue.
	// Protected by runqmu to synchronize cross-processor scheduling.
	runqmu: mutex
	runq:   &coroq

	// Stores the timestamp of the most recent eventpoll.
	// A value of -1 indicates that the next coropoll invocation
	// must perform a non-blocking eventpoll if possible.
	// A value of -2 indicates that a coroutine is checking eventpoll.
	lastpoll: i64

	// Number of coroutines currently polling for I/O.
	// This is used by the scheduler to decide whether to block,
	// spin, or wake worker threads.
	//
	// Must be accessed atomically when used by scheduler logic.
	_ncpoll: int

	// Number of worker threads currently spinning (actively searching
	// for runnable coroutines instead of sleeping).
	//
	// Must be accessed atomically when used by scheduler logic.
	_nmspin: int
}

impl scheduler {
	// Marks that a coroutine is about to enter an I/O polling state.
	// Must be called immediately before the coroutine blocks on I/O.
	fn enterpoll(mut *self) {
		atomic::Add(&self._ncpoll, 1, atomic::Relaxed)
	}

	// Marks that a coroutine has exited an I/O polling state.
	// Must be called immediately after the coroutine finishes polling.
	fn exitpoll(mut *self) {
		atomic::Add(&self._ncpoll, -1, atomic::Relaxed)
	}

	// Returns the number of coroutines currently polling for I/O.
	// Used by the scheduler to make global scheduling decisions.
	fn ncpolling(*self): int {
		ret atomic::Load(&self._ncpoll, atomic::Relaxed)
	}

	// Marks that a worker thread is entering a spin state.
	// Spinning means actively searching for runnable coroutines
	// instead of blocking or sleeping.
	fn enterspin(mut *self) {
		atomic::Add(&self._nmspin, 1, atomic::Relaxed)
	}

	// Marks that a worker thread has exited the spin state.
	// Must be called when the thread stops actively searching for work.
	fn exitspin(mut *self) {
		atomic::Add(&self._nmspin, -1, atomic::Relaxed)
	}

	// Returns the number of worker threads currently spinning
	// while searching for runnable coroutines.
	fn nmspinning(*self): int {
		ret atomic::Load(&self._nmspin, atomic::Relaxed)
	}
}

// A global scheduler instance of the program.
// Just a declaration, compiler will not initialize it by default.
// It will be initialized by the runtime.
let mut sched = scheduler{}

// Should be called when a corotuine enqueued to the global runnable queue.
#disable nilptr
fn onglobalrunqput() {
	// There is not actively spinning worker thread.
	// Try to wake or create worker thread, if an idle coroutine processor available.
	if sched.nmspinning() < 1 {
		let mut rproc: &coroproc
		ok, _ := sched.idleprocs.dequeue(&rproc)
		if ok {
			// We found an idle coroutine processor.
			// Wake a thread or spawn a new one with the coroutine processor.
			threadMutex.lock()
			newm(rproc)
			ret
		}
	}
}

// Tries to put coroutine c to the
// runnable coroutine queue of the current processor.
#disable nilptr boundary
fn runqputl(mut &c: *coro): (ok: bool) {
	mut t := gett()
	ok = t.proc.runq.enqueue(&(*c))
	if ok {
		ret
	}
LoadBalance:
	// Load balancing.
	// Move half of the local queue to the global queue, if possible.
	let mut batch: [coroprocRunQueueSize / 2]coro
	mut n := 0
	for n < len(batch); n++ {
		ok = t.proc.runq.dequeue(&batch[n])
		if !ok {
			break
		}
	}
	sched.runqmu.lock()
	mut i := 0
	for i < n; i++ {
		sched.runq.push(&batch[i])
	}
	sched.runqmu.unlock()

	// The queue is not full, now the enqueue must succeed.
	ok = t.proc.runq.enqueue(&(*c))
	if !ok {
		goto LoadBalance
	}

	onglobalrunqput()
	ret
}

// Puts coroutine c to the runnable coroutine queue.
// If next is true, runnext field of the current processor is prioritized.
#disable nilptr
fn runqput(mut &c: *coro, next: bool) {
	// We have a coroutine processor on the current worker thread.
	mut t := gett()
	if t != nil && t.proc != nil {
		if !t.proc.runnext.used {
			// The runnext slot is empty, use it.
			t.proc.runnext.coro = *c
			t.proc.runnext.used = true
			ret
		} else if next && runqputl(&t.proc.runnext.coro) {
			// The runnext slot is not empty, but it is prioritized.
			// So we tried moving it to local queue and done.
			// Now we can use the runnext slot.
			t.proc.runnext.coro = *c
			t.proc.runnext.used = true
			ret
		}
		// The runnext slot is full, try the local queue.
		if runqputl(c) {
			ret
		}
	}

	// No other option, push it directly to the global queue.
	sched.runqmu.lock()
	sched.runq.push(c)
	sched.runqmu.unlock()

	onglobalrunqput()
}

// A coroutine scheduler routine.
// Works like and event-loop, looks for a coroutine and executes it.
#disable nilptr
fn schedule() {
	sett(acquireThread())
	threadMutex.unlock()
	mut t := gett()
Sched:
	// If poll is set, attempt a non-blocking eventpoll first.
	for {
		// Poll a coroutine to run.
		mut c, ok := coropoll()
		// coropoll failed to find a runnable coroutine.
		if !ok {
			// There are timers in this worker, clean them.
			// Normally, eventpoll returns a timer if exist.
			// Somehow, a spurious wakeup occurred.
			if t.proc.timers.len() > 0 {
				continue
			}
			// There are coroutines polling for I/O.
			if sched.ncpolling() > 0 {
				continue
			}
			// No coroutine to run, no timer, not poll.
			// Nothing to do, break the loop.
			// Park or destroy the worker thread.
			break
		}

		// This coroutine is done, poll a new one.
		if done(&c) {
			continue
		}

		t.coro = c
		t.coro.state |= coroRunning
		resume(&t.coro)
		trampolinerun()
		retiredrain() // Destroy the retire coroutines.

		// In general, `t.coro` will not be updated by `park`.
		// But empty select statements yield coroutine indefinitely,
		// so it will not be runnable again. To prevent memory leak,
		// because of coroutine handle allocations, it uses `t.coro` as
		// the park coroutine. So we can catch it here.
		if t.coro.state&reasonSelectEmpty == reasonSelectEmpty {
			close(&t.coro)
		}
	}
	schedAgain := closeThread(t)
	if schedAgain {
		// We must have a coroutine processor by waker.
		goto Sched
	}
}

// Start point of a scheduler thread.
//
// This function handled like an anonymous function.
// Therefore the compiler will add *unsafe argument to
// the head of the argument list implicitly for anonymous function context.
// The operating system thread API passes a pointer to the function when calling it.
// In the final stage, this function's signature fits with the OS thread API.
fn schedthread() {
	schedule()
}

// A hint function for the compiler.
// At the call site of this function, the compiler emits a runtime park call
// and performs the parking operation for the coroutine c. It also releases
// the mutex mu via `mutexunlock`. This is a low-level mechanism and must be
// used with great care.
async fn runtimepark(c: *coro, mu: u64) {}

// Suspends the current coroutine c and yields the CPU.
// The coroutine c should be the coroutine will be used for resume(c).
// If the mu is not zero, assumes it already locked and releases before yield.
// The coroutine can be made runnable again by calling [unpark].
//
// The first parameters must be the resume-coroutine,
// where park handle will be written, and mutex address.
// Following parameters may be change.
//
// After park, the `gett().coro` will not be affected.
// So, scheduler cannot reach to up-to-date information of the coroutine.
#disable nilptr
async fn park(mut &c: *coro, mu: uintptr, mut reason: u32) {
	c.state |= coroSuspended | reason
	runtimepark(c, u64(mu)).await
}

// Same as park, but takes a tagged pointer for mu.
#disable nilptr
async fn park2(mut &c: *coro, mu: taggedPointer, mut reason: u32) {
	c.state |= coroSuspended | reason
	runtimepark(c, u64(mu)).await
}

// Makes coroutine c runnable.
#disable nilptr
fn unpark(mut &c: *coro) {
	c.state &= ^reasonMask
	runqput(c, true)
}

// Puts the current coroutine sleep for duration dur.
#disable nilptr
async fn sleep(dur: sleepDuration) {
	mut t := gett()
	mut timer := new(timer)
	timer.data.coro = t.coro
	timer.data.used = true
	timer.when = nanotime() + dur
	t.proc.timers.push(timer)
	park(&timer.data.coro, 0, reasonSleep).await
}

// Destroys all retired coroutine frames for the current worker thread.
fn retiredrain() {
	extern.__jule_retireDrain()
}

// Run all queued coroutines until the queue is empty.
// This must be called by the scheduler to make progress after resume.
fn trampolinerun() {
	extern.__jule_trampolineRun()
}

// Yields the processor, allowing other coroutines to run. It does not
// suspend the current coroutine, so execution resumes automatically.
async fn Yield() {
	// Push the coroutine to the global queue, otherwise the next
	// scheduler iteration after park may run it from the local queue.
	mut coroq := new(coroqc)
	coroq.coro = gett().coro
	sched.runqmu.lock()
	sched.runq.pushq(coroq)
	// Do not call onglobalrunqput here.
	// We are doing this out of necessity because
	// when it gets stuck in the local queue,
	// it can be immediately retrieved and resumed.
	// We need to prevent that. This is not an actual overflow.
	park(&coroq.coro, uintptr(&sched.runqmu), reasonNA).await
}

#disable nilptr boundary
fn schedinit() {
	// Initialize the global scheduler instance.
	sched = scheduler{}
	sched.runq = new(coroq)

	// We mutate sched here but mutex is not need to be acquired.
	// Because we are in the program initialize state, no concurrency risk.

	// Initialize corotuine processors.
	maxprocs := u64(COMAXPROCS())
	sched.procs = make([]&coroproc, maxprocs)
	sched.idleprocs = mpmcQueue[&coroproc].new(maxprocs)
	mut i := u64(0)
	for i < maxprocs; i++ {
		mut p := new(coroproc)
		p.runq.init(coroprocRunQueueSize)
		sched.procs[i] = p
		if i == 0 {
			// Set proc of the main program thread.
			threadHead.proc = p
		} else {
			sched.idleprocs.enqueue(&p)
		}
	}
}

// The sysmon (system-monitor) thread.
//
// This function handled like an anonymous function.
// Therefore the compiler will add *unsafe argument to
// the head of the argument list implicitly for anonymous function context.
// The operating system thread API passes a pointer to the function when calling it.
// In the final stage, this function's signature fits with the OS thread API.
fn sysmon() {
	for {
		// Sleep for the next period.
		threadsleep(_Millisecond * 10)
		// eventpoll if not polled for more than 10ms and
		// there is a coroutine waiting for eventpoll.
		// If eventpoll is not triggered, all coroutines may accumulate in
		// local and global queues, leading to starvation.
		// The eventpoll must be checked regularly.
		if sched.ncpolling() > 0 {
			lastpoll := atomic::Load(&sched.lastpoll, atomic::Acquire)
			if lastpoll >= 0 && nanotime()-lastpoll > _Millisecond*10 {
				atomic::CompareAndSwap(&sched.lastpoll, lastpoll, pollCheckRequested, atomic::AcqRel, atomic::Relaxed)
			}
		}
	}
}
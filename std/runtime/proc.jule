// Copyright 2025 The Jule Project Contributors. All rights reserved.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

use "std/internal/runtime/atomic"

// Destroys all retired coroutine frames for the current worker thread.
extern fn __jule_retireDrain()

// Returns the maximum number of CPUs that can be executing simultaneously.
fn COMAXPROCS(): int { ret numcpu }

const (
	// The maximum count of the runnable coroutine queue of a coroutine process.
	// Must be power of two.
	coroprocRunQueueSize = 256
)

// A coroutine process.
struct coroproc {
	// Prioritized coroutine, should run at next scheduler loop.
	runnext: &coro

	// List of the runnable coroutines.
	runq: deque

	// Waiting poll descriptors.
	pds: polldescq

	// Local timers of the coroutine process.
	timers: timerheap

	//Ttime of last eventpoll.
	lastpoll: i64

	// This coroutine processor is in use.
	owned: i32
}

impl coroproc {
	// Tries to take ownership.
	fn tryown(mut *self): bool {
		for {
			owned := atomic::Load(&self.owned, atomic::Acquire)
			if owned != 0 {
				ret false
			}
			if atomic::CompareAndSwap(&self.owned, owned, 1, atomic::AcqRel, atomic::Relaxed) {
				ret true
			}
		}
	}
}

// A scheduler envrionment for the whole Jule program.
struct scheduler {
	// All coroutine processes.
	procs: []&coroproc

	// Idle coroutine processes of the scheduler.
	idleprocs: mpmcQueue[&coroproc]

	// Global queue for runnable coroutines with guard.
	runqmu: mutex
	runq:   &coroq

	// Count of the coroutines actively polling for I/O.
	// Atomic if used with relevant scheduler functions.
	_ncpolling: int

	// Count of the worker threads actively polling for a coroutine.
	// Atomic if used with relevant scheduler functions.
	_nmspinning: int
}

impl scheduler {
	// A coroutine should call this before polling for I/O.
	fn enterpoll(mut *self) {
		atomic::Add(&self._ncpolling, 1, atomic::Relaxed)
	}

	// A coroutine should call this after polling for I/O.
	fn exitpoll(mut *self) {
		atomic::Add(&self._ncpolling, -1, atomic::Relaxed)
	}

	// Count of the coroutines actively polling for I/O.
	fn ncpolling(*self): int {
		ret atomic::Load(&self._ncpolling, atomic::Relaxed)
	}

	// A worker thread should call this before polling for a corouitine.
	fn enterspin(mut *self) {
		atomic::Add(&self._nmspinning, 1, atomic::Relaxed)
	}

	// A worker thread should call this after polled a corouitine.
	fn exitspin(mut *self) {
		atomic::Add(&self._nmspinning, -1, atomic::Relaxed)
	}

	// Returns count of the worker threads actively polling for a coroutine.
	fn nmspinning(*self): int {
		ret atomic::Load(&self._nmspinning, atomic::Relaxed)
	}

	// Returns a coroutine processor with ownership, if possible.
	fn getproc(mut *self): (rproc: &coroproc) {
		for {
			ok, empty := gsched.idleprocs.dequeue(&rproc)
			if !ok {
				if !empty {
					// Queue is not empty, try again.
					continue
				}
				ret nil
			}
			// We found an idle coroutine process.
			// Try to take ownership.
			if rproc.tryown() {
				ret
			}
			// Lost race, try again.
		}
	}
}

// A global scheduler instance of the program.
// Just a declaration, compiler will not initialize it by default.
// It will be initalized by the runtime.
let mut gsched = scheduler{}

// Should be called when a corotuine enqueued to the global runnable queue.
#disable nilptr
fn onglobalrunqput() {
	// There is not actively spinning worker thread.
	// Try to wake or create worker thread, if an idle coroutine process avaiable.
	if gsched.nmspinning() < 1 {
		let mut rproc: &coroproc
		ok, _ := gsched.idleprocs.dequeue(&rproc)
		if ok {
			// We found an idle coroutine process.
			// Wake a thread or spawn a new one with the coroutine process.
			threadMutex.lock()
			spawnSchedThread(rproc)
			ret
		}
	}
}

// Tries to put coroutine c to the
// runnable coroutine queue of the current process.
#disable nilptr boundary
fn runqputl(mut &c: *&coro): (ok: bool) {
	mut t := gett()
	ok = t.proc.runq.push(*c)
	if ok {
		ret
	}
LoadBalance:
	// Load balancing.
	// Move half of the local queue to the global queue, if possible.
	let mut batch: [coroprocRunQueueSize / 2]&coro
	mut n := 0
	for n < len(batch); n++ {
		batch[n], ok = t.proc.runq.pop()
		if !ok {
			break
		}
	}
	gsched.runqmu.lock()
	mut i := 0
	for i < n; i++ {
		gsched.runq.push(batch[i])
	}
	gsched.runqmu.unlock()

	// The queue is not full, now the enqueue must succeed.
	ok = t.proc.runq.push(*c)
	if !ok {
		goto LoadBalance
	}

	onglobalrunqput()
	ret
}

// Puts coroutine c to the runnable coroutine queue.
// If next is true, runnext field of the current process is prioritized.
#disable nilptr
fn runqput(mut &c: *&coro, next: bool) {
	// We have a coroutine process on the current worker thread.
	mut t := gett()
	if t != nil && t.proc != nil {
		if t.proc.runnext == nil {
			// The runnext slot is empty, use it.
			t.proc.runnext = *c
			ret
		} else if next && runqputl(&t.proc.runnext) {
			// The runnext slot is not empty, but it is prioritized.
			// So we tried moving it to local queue and done.
			// Now we can use the runnext slot.
			t.proc.runnext = *c
			ret
		}
		// The runnext slot is full, try the local queue.
		if runqputl(c) {
			ret
		}
	}

	// No other option, push it directly to the global queue.
	gsched.runqmu.lock()
	gsched.runq.push(*c)
	gsched.runqmu.unlock()

	onglobalrunqput()
}

// A coroutine scheduler routine.
// Works like and event-loop, looks for a coroutine and execues it.
#disable nilptr
fn sched() {
	sett(acquireThread())
	threadMutex.unlock()
	mut t := gett()
Sched:
	for {
		// Poll a coroutine to run.
		mut c := coropoll()
		if c == nil {
			// There are timers in this worker, clean them.
			// Normally, eventpoll returns a timer if exist.
			// Somehow, a spurious wakeup occured.
			if t.proc.timers.len() > 0 {
				continue
			}
			// There are coroutines polling for I/O.
			if gsched.ncpolling() > 0 {
				continue
			}
			// No coroutine to run, no timer, not poll.
			// Nothing to do, break the loop.
			// Park or destroy the worker thread.
			break
		}

		// This coroutine is done, poll a new one.
		if done(c) {
			continue
		}

		t.coro = c
		c.state |= coroRunning
		resume(c)
		retiredrain() // Destroy the retire coroutines.
		t.coro = nil
	}
	schedAgain := closeThread(t)
	if schedAgain {
		// We must have a coroutine processor by waker.
		goto Sched
	}
}

// Start point of a scheduler thread.
//
// This function handled like an anonymous function.
// Therefore the compiler will add *unsafe argument to
// the head of the argument list implicitly for anonymous function context.
// The operating system thread API passes a pointer to the function when calling it.
// In the final stage, this function's signature fits with the OS thread API.
fn schedthread() {
	sched()
}

// Suspends the current coroutine and yields the CPU.
// If the mu is not zero, assumes it already locked and releases before yield.
// The coroutine can be made runnable again by calling [unpark].
//
// The coroutine rc is the resume coroutine.
// Compiler will write the handle to it, resume(rc) must be used to resume.
//
// This function will compile inlinely by the compiler.
// The function must implement additional behavior before the park.
// Compiler will park the coroutine at the placeholder comment.
//
// The first parameters must be the resume-coroutine,
// where park handle will be written, and mutex address.
// Following parameters may be change.
#disable nilptr
async fn park(mut rc: &coro, mu: uintptr, mut reason: u32) {
	mut c := gett().coro
	c.state |= coroSuspended | reason
	// park(c)
}

// Makes coroutine c runnable.
#disable nilptr
fn unpark(mut &c: *&coro) {
	(*c).state &= ^reasonMask
	runqput(c, true)
}

// Puts the current coroutine sleep for duration dur.
#disable nilptr
async fn sleep(dur: sleepDuration) {
	mut t := gett()
	mut timer := new(timer)
	timer.coro = t.coro
	timer.when = nanotime() + dur
	t.proc.timers.push(timer)
	park(t.coro, 0, reasonSleep).await
}

// Destroys all retired coroutine frames for the current worker thread.
fn retiredrain() {
	extern.__jule_retireDrain()
}

// Yields the processor, allowing other coroutines to run. It does not
// suspend the current coroutine, so execution resumes automatically.
async fn Yield() {
	mut coro := gett().coro
	coro.state &= ^coroRunning
	coro.state |= coroSuspended
	// Push the coroutine to the global queue, otherwise the next
	// scheduler iteration after park may run it from the local queue.
	gsched.runqmu.lock()
	gsched.runq.push(coro)
	onglobalrunqput()
	park(coro, uintptr(&gsched.runqmu), reasonNA).await
}

#disable nilptr boundary
fn schedinit() {
	// Initialize the global scheduler instance.
	gsched = scheduler{}
	gsched.runq = new(coroq)

	// We mutate gsched here but mutex is not need to be acquired.
	// Becase we are in the program initialize state, no concurreny risk.

	// Initialize corotuine processes.
	maxprocs := u64(COMAXPROCS())
	gsched.procs = make([]&coroproc, maxprocs)
	gsched.idleprocs = mpmcQueue[&coroproc].new(maxprocs)
	mut i := u64(0)
	for i < maxprocs; i++ {
		mut p := new(coroproc)
		p.runq.init(coroprocRunQueueSize)
		gsched.procs[i] = p
		if i == 0 {
			// Set proc of the main program thread.
			threadHead.proc = p
		} else {
			gsched.idleprocs.enqueue(&p)
		}
	}
}
// Copyright 2025 The Jule Project Contributors. All rights reserved.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

// SCHEDULER
//
// This scheduler implements a *cooperative*, *single-threadâ€“owned* coroutine
// execution loop for the Jule runtime. It is designed as a low-level runtime
// component, not a user-facing abstraction.
//
// The scheduler operates similarly to an event loop: it repeatedly polls a
// runnable coroutine, resumes it, and re-integrates it into the system based on
// its post-resume state.
//
// The scheduler does NOT implement coroutines itself. Instead, it operates on
// opaque coroutine handles provided by the runtime ABI.
//
// The scheduler is designed according to the C:M:P model:
//
//	C (Coroutine)
//		A suspendable state machine generated by the compiler.
//		Conceptually, it is an async function, but it is not part of the user's
//		ordinary control flow. It is detached and it may execute concurrently at
//		any time, and its scheduling is fully managed by the scheduler.
//		It behaves similarly to a typical thread.
//
//	M (Machine)
//		A real operating system thread.
//		It is responsible for executing a coroutine.
//		Only as many M instances may be created as permitted by COMAXPROCS.
//
//	P (Processor)
//		A scheduler processor. It owns its own local state.
//		An M must be paired with a P in order to execute a coroutine and perform
//		scheduling. An M can be paired with only one P at a time, and a P can be
//		paired with only one M at a time.
//
// Async functions may call other async functions and, through an await chain,
// form a chain of state machines. The scheduler does not take responsibility
// for lifetime management of each individual state machine. It is only
// responsible for managing the lifetime of coroutines (detached state machines).
//
// A coroutine may only be resumed by the scheduler.
// When a parked coroutine is woken up, it must be enqueued into the runnable
// coroutine queue. The scheduler decides when it will actually execute.
//
// STACK-ORIENTED COROUTINE HANDLING
//
// Jule runtime prioritizes avoiding heap allocations for coroutines.
// Each coroutine instance is used from the stack. When necessary,
// it can be copied or passed around via references/pointers.
// Since it is stack-oriented, it must be handled carefully.
// Otherwise, a stale coroutine copy may be used and cause critical issues.

use "std/internal/runtime/atomic"

extern fn __jule_retireDrain()
extern fn __jule_trampolineRun()

// Returns the maximum number of CPUs that can be executing simultaneously.
fn COMAXPROCS(): int { ret numcpu }

const (
	// The maximum count of the runnable coroutine queue of a P.
	// Must be power of two.
	prunqsize = 256
)

// Implementation of the runnext field for a P.
struct runnext {
	used: bool // Indicates whether coro is meaningful.
	c:    c    // The coroutine.
}

// A coroutine processor.
struct p {
	// Prioritized coroutine, should run at next scheduler loop.
	runnext: runnext

	// A lock-free local list for the runnable coroutines.
	runqhead: u32
	runqtail: u32
	runq:     [prunqsize]c

	// Poll descriptors that are still alive.
	pdsmu: mutex
	pds:   polldescq

	// Local timers of the P.
	timers: timerheap
}

const (
	pollCheckRequested = -1 // Indicates that an eventpoll check has been explicitly prioritized.
	pollCheckIn        = -2 // Indicates that an M is currently performing an eventpoll check.
)

// Scheduler is the global scheduling environment for the entire Jule program.
// It owns Ps, global run queues, and tracks system-wide
// polling and spinning states for load-balancing and sleep/wakeup decisions.
struct scheduler {
	// All Ps (logical execution units).
	// Each processor is responsible for running coroutines on a worker thread.
	pps: []&p

	// Queue of idle Ps that are not currently executing work.
	// Used to quickly assign work without scanning all processors.
	idlepps: mpmcQueue[&p]

	// Global runnable coroutine queue.
	// Protected by runqmu to synchronize cross-processor scheduling.
	runqmu: mutex
	runq:   &coroq

	// Stores the timestamp of the most recent eventpoll.
	// A value of -1 indicates that the next findRunnable invocation
	// must perform a non-blocking eventpoll if possible.
	// A value of -2 indicates that a coroutine is checking eventpoll.
	lastpoll: i64

	// Number of coroutines currently polling for I/O.
	//
	// Must be accessed atomically when used by scheduler logic.
	_ncpoll: int

	// Number of coroutines currently in blocking call.
	//
	// Must be accessed atomically when used by scheduler logic.
	_ncblock: int

	// Number of worker threads currently spinning (actively searching
	// for runnable coroutines instead of sleeping).
	//
	// Must be accessed atomically when used by scheduler logic.
	_nmspin: int

	// Number of coroutines currently working.
	//
	// Must be accessed atomically when used by scheduler logic.
	_ncrun: int
}

impl scheduler {
	// Marks that a coroutine is about to enter an I/O polling state.
	// Must be called immediately before the coroutine blocks on I/O.
	fn enterpoll(mut *self) {
		atomic::Add(&self._ncpoll, 1, atomic::Relaxed)
	}

	// Marks that a coroutine has exited an I/O polling state.
	// Must be called immediately after the coroutine finishes polling.
	fn exitpoll(mut *self) {
		atomic::Add(&self._ncpoll, -1, atomic::Relaxed)
	}

	// Returns the number of coroutines currently polling for I/O.
	// Used by the scheduler to make global scheduling decisions.
	fn ncpolling(*self): int {
		ret atomic::Load(&self._ncpoll, atomic::Relaxed)
	}

	// Marks that a coroutine is about to enter an I/O polling state.
	// Must be called immediately before the coroutine blocks on I/O.
	fn enterblock(mut *self) {
		atomic::Add(&self._ncblock, 1, atomic::Relaxed)
	}

	// Marks that a coroutine has exited an I/O polling state.
	// Must be called immediately after the coroutine finishes polling.
	fn exitblock(mut *self) {
		atomic::Add(&self._ncblock, -1, atomic::Relaxed)
	}

	// Returns the number of coroutines currently polling for I/O.
	// Used by the scheduler to make global scheduling decisions.
	fn ncblocking(*self): int {
		ret atomic::Load(&self._ncblock, atomic::Relaxed)
	}

	// Marks that a worker thread is entering a spin state.
	// Spinning means actively searching for runnable coroutines
	// instead of blocking or sleeping.
	fn enterspin(mut *self) {
		atomic::Add(&self._nmspin, 1, atomic::Relaxed)
	}

	// Marks that a worker thread has exited the spin state.
	// Must be called when the thread stops actively searching for work.
	fn exitspin(mut *self) {
		atomic::Add(&self._nmspin, -1, atomic::Relaxed)
	}

	// Returns the number of worker threads currently spinning
	// while searching for runnable coroutines.
	fn nmspinning(*self): int {
		ret atomic::Load(&self._nmspin, atomic::Relaxed)
	}

	// Marks that a worker thread is entering a coroutine runner state.
	fn enterrun(mut *self) {
		atomic::Add(&self._ncrun, 1, atomic::Relaxed)
	}

	// Marks that a worker thread has exited the runner state.
	fn exitrun(mut *self) {
		atomic::Add(&self._ncrun, -1, atomic::Relaxed)
	}

	// Returns the number of worker threads currently runs a coroutine.
	fn ncrunning(*self): int {
		ret atomic::Load(&self._ncrun, atomic::Relaxed)
	}
}

// A global scheduler instance of the program.
// Just a declaration, compiler will not initialize it by default.
// It will be initialized by the runtime.
let mut sched = scheduler{}

// Tries to add one more P to execute coroutines.
fn wakep() {
	// Be conservative about spinning threads,
	// only start one if none exist already.
	if sched.nmspinning() != 0 || !atomic::CompareAndSwap(&sched._nmspin, 0, 1, atomic::Relaxed, atomic::Relaxed) {
		ret
	}
	let mut pp: &p
	ok, _ := sched.idlepps.dequeue(&pp)
	if ok {
		// We found an idle P.
		// Wake a thread or spawn a new one with the P.
		threadMutex.lock()
		startm(pp)
	} else {
		sched.exitspin()
	}
}

// Tries to put cp on the local runnable queue.
// If next is false, it adds cp to the tail of the runnable queue if runnext is used.
// If next is true, runqput puts cp in the pp.runnext slot.
// If the run queue is full, runnext puts cp on the global queue.
// Executed only by the owner P.
#disable nilptr boundary
fn runqput(mut pp: &p, mut &cp: *c, next: bool): (overflow: bool) {
	// pp is nil, the caller is probably a blocking thread pool worker thread.
	// Not an actual M, it is not have a P by design.
	// Nothing to do, overflow to the global runnable queue.
	if pp == nil {
		sched.runqmu.lock()
		sched.runq.push(cp)
		sched.runqmu.unlock()
		ret true
	}

	if !pp.runnext.used {
		// The runnext slot is empty, use it.
		pp.runnext.c = *cp
		pp.runnext.used = true
		ret false
	}
	// The runnext slot is not empty, but it is prioritized.
	// So we tried moving it to local queue and done.
	// Now we can use the runnext slot.
	let mut oldc: c
	if next {
		oldc = pp.runnext.c
		pp.runnext.c = *cp
		pp.runnext.used = true
		// Kick the old runnext out to the regular run queue.
		unsafe { *(&cp) = &oldc }
	}
	for {
		h := atomic::Load(&pp.runqhead, atomic::Acquire) // load-acquire, synchronize with consumers
		t := pp.runqtail
		if t-h < prunqsize {
			pp.runq[t&(prunqsize-1)] = *cp
			atomic::Store(&pp.runqtail, t+1, atomic::Release) // store-release, makes the item available for consumption
			ret false
		}
		if runqputslow(pp, cp, h, t) {
			ret true
		}
		// The queue is not full, now the put above must succeed.
	}
}

// Put cp and a batch of work from local runnable queue on global queue.
// Executed only by the owner P.
#disable nilptr boundary
fn runqputslow(mut pp: &p, mut &cp: *c, h: u32, t: u32): bool {
	let mut batch: [prunqsize/2 + 1]c

	// First, grab a batch from local queue.
	mut n := t - h
	n = n / 2
	if n != prunqsize/2 {
		panic("runqputslow: queue is not full")
	}
	mut i := u32(0)
	for i < n; i++ {
		batch[i] = pp.runq[(h+i)&(prunqsize-1)]
	}
	if !atomic::CompareAndSwap(&pp.runqhead, h, h+n, atomic::Release, atomic::Relaxed) { // cas-release, commits consume
		ret false
	}
	batch[n] = *cp

	// Put batch to global runnable queue.
	batchn := i32(n + 1)
	sched.runqmu.lock()
	mut bi := i32(0)
	for bi < batchn; bi++ {
		sched.runq.push(&batch[bi])
	}
	sched.runqmu.unlock()

	ret true
}

// Puts all the coroutines on batch on the local runnable queue.
// The queue must have enough space for batch.
// Batch is a ring buffer starting at batchHead.
// Executed only by the owner P.
#disable nilptr boundary
fn runqputbatch(mut pp: &p, mut &batch: *[prunqsize]c, batchHead: u32, mut bsize: u32) {
	h := atomic::Load(&pp.runqhead, atomic::Acquire)
	mut t := pp.runqtail
	mut n := batchHead
	for n < bsize && t-h < prunqsize {
		pp.runq[t&(prunqsize-1)] = (*batch)[n]
		t++
		n++
	}
	bsize -= n
	atomic::Store(&pp.runqtail, t, atomic::Release)
	if bsize != 0 {
		panic("batch size corruption")
	}
}

// Gets a coroutine from local runnable queue and writes to cp.
// Executed only by the owner P.
#disable nilptr boundary
fn runqget(mut pp: &p, mut &cp: *c): bool {
	// If there's a runnext, it's the next coroutine to run.
	if pp.runnext.used {
		pp.runnext.used = false
		*cp = pp.runnext.c
		ret true
	}
	for {
		h := atomic::Load(&pp.runqhead, atomic::Acquire) // load-acquire, synchronize with other consumers
		t := pp.runqtail
		if t == h {
			ret false
		}
		// Must load *before* acquiring the slot as slot may be overwritten
		// immediately after acquiring. This load is NOT required to be
		// atomic even-though it may race with an overwrite as we only return
		// true if we win the race below guaranteeing we had no race during
		// our read. If we loose the race then `cp` could be corrupt due to
		// read-during-write race but as coroutine is trivially destructible
		// this does not matter.
		//
		// We cannot store `&coro` instead of `coro` because of the race.
		// Otherwise, RC counting may be corrupted and cause some critical
		// issues, such as use-after-free.
		*cp = pp.runq[h&(prunqsize-1)]
		if atomic::CompareAndSwap(&pp.runqhead, h, h+1, atomic::Release, atomic::Relaxed) { // cas-release, commits consume
			ret true
		}
	}
}

// Grabs a batch of coroutines from pp's runnable queue into batch.
// Batch is a ring buffer starting at batchHead.
// Returns number of grabbed coroutines.
// Can be executed by any P.
#disable nilptr boundary
fn runqgrab(mut pp: &p, mut &batch: *[prunqsize]c, batchHead: u32): u32 {
	for {
		h := atomic::Load(&pp.runqhead, atomic::Acquire) // load-acquire, synchronize with other consumers
		t := atomic::Load(&pp.runqtail, atomic::Acquire) // load-acquire, synchronize with the producer
		mut n := t - h
		n = n - n/2
		if n == 0 {
			ret 0
		}
		if n > prunqsize/2 { // read inconsistent h and t
			continue
		}
		mut i := u32(0)
		for i < n; i++ {
			// See comment of runqget about read-during-write race.
			(*batch)[(batchHead+i)&(prunqsize-1)] = pp.runq[(h+i)&(prunqsize-1)]
		}
		if atomic::CompareAndSwap(&pp.runqhead, h, h+n, atomic::Release, atomic::Relaxed) { // cas-release, commits consume
			ret n
		}
	}
}

// Steal half of elements from local runnable queue of pp2
// and put onto local runnable queue of pp.
// Writes one of the stolen coroutines to cp.
// Reports false if failed, otherwise true.
#disable nilptr boundary
fn runqsteal(mut pp: &p, mut pp2: &p, mut &cp: *c): bool {
	t := pp.runqtail
	mut n := runqgrab(pp2, &pp.runq, t)
	if n == 0 {
		ret false
	}
	n--
	*cp = pp.runq[(t+n)&(prunqsize-1)]
	if n == 0 {
		ret true
	}
	h := atomic::Load(&pp.runqhead, atomic::Acquire) // load-acquire, synchronize with consumers
	if t-h+n >= prunqsize {
		panic("runqsteal: runq overflow")
	}
	atomic::Store(&pp.runqtail, t+n, atomic::Release) // store-release, makes the item available for consumption
	ret true
}

// Tries to steal a batch of coroutines for pp.
// Writes one of the stolen coroutines to cp.
// Reports false if failed, otherwise true.
fn stealWork(mut pp: &p, mut &cp: *c): bool {
	const StealTries = 4
	mut n := 0
	pcount := COMAXPROCS()
	startIdx := int(rand() % u64(pcount))
	for n < StealTries; n++ {
		mut i := 0
		mut idx := startIdx
		for i < pcount; i, idx = i+1, (idx+1)%pcount {
			mut pp2 := sched.pps[idx]
			if pp == pp2 {
				continue
			}
			ok := runqsteal(pp, pp2, cp)
			if ok {
				ret true
			}
		}
	}
	ret false
}

// Selects a runnable coroutine for execution.
//
// This function is one of the core components of the coroutine scheduler.
// It attempts to find exactly one coroutine to run,
// following a strict priority order:
//
// The function guarantees that at most one coroutine is returned via
// `polled`; any additional runnable coroutines are enqueued back into
// appropriate run queues.
//
// Invariants:
//	- eventpoll() never returns more entries than a local run queue can hold.
//	- Queue corruption is considered a fatal runtime error.
//
// The `poll` flag is a scheduler hint used to amortize eventpoll costs
// under high I/O traffic.
//
// Assumes that the calling thread is counted as spinning.
// It will exit the spinning state before return.
#disable nilptr boundary
fn findRunnable(spinning: bool): (c: c, ok: bool) {
	mut pp := gett().pp

	ok = runqget(pp, &c)
	if ok {
		if spinning {
			panic("runtime: spinning when local runnable queue is not empty")
		}
		ret c, true
	}

	// Enter spinning.
	if !spinning {
		sched.enterspin()
	}
	// Before the look for global runq, check for the lastpoll enforcement.
	// If the lastpoll value is -1, check eventpoll.
	if atomic::CompareAndSwap(&sched.lastpoll, pollCheckRequested, pollCheckIn, atomic::AcqRel, atomic::Relaxed) {
		// eventpoll guarantees len(toRun) <= local run queue capacity.
		let mut toRun: [prunqsize]c
		bn := eventpoll(0, &toRun) // Use 0-delay to perform a non-blocking poll.
		now := nanotime()
		atomic::Store(&sched.lastpoll, now, atomic::Release)
		// If any coroutines are ready, return the first and enqueue the rest.
		// Timers possibly expired here will be handled in a later iteration.
		if bn > 0 {
			sched.exitspin()
			c = toRun[0]
			runqputbatch(pp, &toRun, 1, bn)
			ret c, true
		}
	}

	// Local queue empty: attempt to fetch a batch from the global run queue.
	//
	// The batch size is capped to prunqsize to preserve
	// locality and reduce contention on the global lock.
	sched.runqmu.lock()
	if sched.runq.head != nil {
		let mut batch: [prunqsize]c
		mut n := u32(0)

		for n < prunqsize; n++ {
			if sched.runq.head == nil {
				break
			}
			batch[n] = sched.runq.head.c
			sched.runq.remove(sched.runq.head)
		}
		sched.runqmu.unlock()

		if n > 0 {
			sched.exitspin()
			c = batch[0]
			runqputbatch(pp, &batch, 1, n)
			ret c, true
		}
	} else {
		sched.runqmu.unlock()
	}

	// No work found locally or globally: attempt work stealing.
	//
	// Stealing is limited to a small fixed number of attempts to
	// avoid excessive contention and cache thrashing.
	ok = stealWork(pp, &c)
	sched.exitspin()
	if ok {
		ret c, true
	}

	// No runnable coroutine found yet.
	// Determine how long we can block based on timers and deadlines.
	// The default is -1, which is blocks indefinitely.
	mut delay := i64(-1)
	mut now := nanotime()
	mut iodeadline := false // Indicates presence of expired I/O deadlines.

	// P's local timers.
	//
	// Expired timers immediately make their coroutines runnable.
	// The first such coroutine is returned; others are enqueued.
	mut n := 0
	if pp.timers.peek() != nil {
		ok = false
		for n < prunqsize {
			mut timer := pp.timers.peek()
			if timer == nil || timer.when > now {
				break
			}
			mut data := pp.timers.pop().data
			if data.used {
				if n == 0 {
					c = data.c
					ok = true
				} else {
					runqput(pp, &data.c, false)
				}
			} else {
				// Timer corresponds to an I/O deadline.
				iodeadline = true
			}
			n++
		}

		// If we already found a runnable coroutine, return immediately
		// without entering eventpoll.
		if ok {
			ret c, true
		}

		// Compute delay until the next timer fires.
		soonest := pp.timers.peek()
		if soonest != nil {
			delay = soonest.when - now
			if delay <= 0 {
				panic("runtime: inconsistent timer state")
			}
		}
	}

	// Handle expired I/O deadlines.
	//
	// Coroutines blocked on I/O with an associated deadline
	// are made runnable once their deadline expires.
	//
	// Coroutines are NOT removed from pd here.
	// The coroutine itself is responsible for verifying whether it is still
	// associated with this pd upon wakeup. If the association is unchanged,
	// the wakeup indicates a deadline-exceeded condition; otherwise, the coroutine
	// was resumed for another reason (e.g. notified by eventpoll).
	if iodeadline && pp.pds.head != nil {
		pp.pdsmu.lock()
		mut q := pp.pds.head
		for {
			if n >= prunqsize || q == nil {
				break
			}
			mut pd := q.pd
			pd.mu.lock()

			if pd.rdeadline > 0 && pd.rdeadline <= now && pd.rcp != nil && pd.rcp != pollrace {
				runqput(pp, &(*pd.rcp), false)
				n++
			}
			if pd.wdeadline > 0 && pd.wdeadline <= now && pd.wcp != nil && pd.wcp != pollrace {
				runqput(pp, &(*pd.wcp), false)
				n++
			}
			if pd.deadline > 0 && pd.deadline <= now {
				if pd.rcp != nil && pd.rcp != pollrace {
					runqput(pp, &(*pd.rcp), false)
					n++
				}
				if pd.wcp != nil && pd.wcp != pollrace {
					runqput(pp, &(*pd.wcp), false)
					n++
				}
			}
			pd.mu.unlock()
			q = q.next
		}
		pp.pdsmu.unlock()

		ok = runqget(pp, &c)
		if !ok {
			panic("runtime: coroutine scheduler queue corruption")
		}
		ret c, true
	}

	// Final step: poll eventpoll.
	if delay != -1 || sched.ncpolling() > 0 && atomic::Swap(&sched.lastpoll, pollCheckIn, atomic::AcqRel) != pollCheckIn {
		// eventpoll guarantees len(toRun) <= local run queue capacity.
		let mut toRun: [prunqsize]c
		bn := eventpoll(delay, &toRun)
		now = nanotime()
		atomic::Store(&sched.lastpoll, now, atomic::Release)
		if bn > 0 {
			c = toRun[0]
			runqputbatch(pp, &toRun, 1, bn)
			ret c, true
		}
	}

	ret c, false
}

// A coroutine scheduler routine.
// Works like and event-loop, looks for a coroutine and executes it.
#disable nilptr
fn schedule(mut spinning: bool) {
	mut m := gett()
Sched:
	const MaxSpin = 4
	mut tried := 0
	for {
		// Poll a coroutine to run.
		mut c, ok := findRunnable(spinning)
		spinning = false
		// findRunnable failed to find a runnable coroutine.
		if !ok {
			// There are timers in this worker, clean them.
			// Normally, eventpoll returns a timer if exist.
			// Somehow, a spurious wakeup occurred.
			if m.pp.timers.len() > 0 {
				continue
			}
			// There are coroutines polling for I/O or waiting for a blocking job,
			// and there is no M looking for eventpoll.
			if atomic::Load(&sched.lastpoll, atomic::Acquire) != pollCheckIn &&
				(sched.ncpolling() > 0 || sched.ncblocking() > 0) {
				continue
			}
			// No coroutine to run, no timer, no eventpoll.
			// Under high contention, new coroutines may be available since last poll.
			// Try again if reasonable.
			if tried < MaxSpin && sched.ncrunning() > 0 {
				tried++
				continue
			}
			// We tried enough.
			// Nothing to do, break the loop.
			// Park or destroy the worker thread.
			break
		}
		tried = 0

		m.c = c
		m.c.state |= coroRunning
		sched.enterrun()
		resume(&m.c)
		sched.exitrun()
		trampolinerun()
		retiredrain() // Destroy the retire coroutines.

		// In general, `m.c` will not be updated by `park`.
		// But empty select statements yield coroutine indefinitely,
		// so it will not be runnable again. To prevent memory leak,
		// because of coroutine handle allocations, it uses `m.c` as
		// the park coroutine. So we can catch it here.
		if m.c.state&reasonSelectEmpty == reasonSelectEmpty {
			close(&m.c)
		}
	}
	schedAgain := stopm(m)
	if schedAgain {
		// We must have a P by waker.
		goto Sched
	}
}

// Start point of a scheduler thread.
// When a scheduler thread is started, it must be counted as spinning.
//
// This function handled like an anonymous function.
// Therefore the compiler will add *unsafe argument to
// the head of the argument list implicitly for anonymous function context.
// The operating system thread API passes a pointer to the function when calling it.
// In the final stage, this function's signature fits with the OS thread API.
fn schedthread() {
	// Set thread-local data before scheduling.
	sett(acquireThread())
	threadMutex.unlock()

	schedule(true)
}

// A hint function for the compiler.
// At the call site of this function, the compiler emits a runtime park call
// and performs the parking operation for the cp. It also releases
// the mutex mu via `mutexunlock`. This is a low-level mechanism and must be
// used with great care.
async fn runtimepark(cp: *c, mu: u64) {}

// Suspends the current cp and yields the CPU.
// The cp should be the coroutine will be used for resume(cp).
// If the mu is not zero, assumes it already locked and releases before yield.
// The coroutine can be made runnable again by calling [unpark].
//
// The first parameters must be the resume-coroutime,
// where park handle will be written, and mutex address.
// Following parameters may be change.
//
// After park, the `gett().c` will not be affected.
// So, scheduler cannot reach to up-to-date information of the coroutine.
#disable nilptr
async fn park(mut &cp: *c, mu: uintptr, mut reason: u32) {
	cp.state |= coroSuspended | reason
	runtimepark(cp, u64(mu)).await
}

// Same as park, but takes a tagged pointer for mu.
#disable nilptr
async fn park2(mut &cp: *c, mu: taggedPointer, mut reason: u32) {
	cp.state |= coroSuspended | reason
	runtimepark(cp, u64(mu)).await
}

// Makes cp runnable.
#disable nilptr
fn unpark(mut &cp: *c) {
	cp.state &= ^reasonMask
	mut m := gett()
	runqput(m.pp, cp, true)
	// Try to wake up a new M. If there is an idle M, this is something that can be
	// used to increase concurrency. If we only try to wake up a new M in more
	// restricted situations, for example only when a runqput overflow occurs,
	// we may not achieve sufficient concurrency. For instance, a small number of
	// coroutines that could run concurrently may never cause an overflow and thus
	// end up behaving effectively synchronously or single-threaded concurrency.
	// Therefore, wake up an M so that runnable work can be executed multi-threaded.
	wakep()
}

// Puts the current coroutine sleep for duration dur.
#disable nilptr
async fn sleep(dur: sleepDuration) {
	mut m := gett()
	mut timer := new(timer)
	timer.data.c = m.c
	timer.data.used = true
	timer.when = nanotime() + dur
	m.pp.timers.push(timer)
	park(&timer.data.c, 0, reasonSleep).await
}

// Destroys all retired coroutine frames for the current worker thread.
fn retiredrain() {
	extern.__jule_retireDrain()
}

// Run all queued coroutines until the queue is empty.
// This must be called by the scheduler to make progress after resume.
fn trampolinerun() {
	extern.__jule_trampolineRun()
}

// Yields the processor, allowing other coroutiens to run. It does not
// suspend the current coroutine, so execution resumes automatically.
async fn Yield() {
	// Push the coroutine to the global queue, otherwise the next
	// scheduler iteration after park may run it from the local queue.
	mut coroq := new(coroqc)
	coroq.c = gett().c
	sched.runqmu.lock()
	sched.runq.pushq(coroq)
	park(&coroq.c, uintptr(&sched.runqmu), reasonNA).await
}

#disable nilptr boundary
fn schedinit() {
	// Initialize the global scheduler instance.
	sched = scheduler{}
	sched.runq = new(coroq)

	// We mutate sched here but mutex is not need to be acquired.
	// Because we are in the program initialize state, no concurrency risk.

	// Initialize corotuine processors.
	maxprocs := u32(COMAXPROCS())
	sched.pps = make([]&p, maxprocs)
	sched.idlepps = mpmcQueue[&p].new(u64(maxprocs))
	mut i := u32(0)
	for i < maxprocs; i++ {
		mut pp := new(p)
		sched.pps[i] = pp
		if i == 0 {
			// Set P of the main program thread.
			threadHead.pp = pp
		} else {
			sched.idlepps.enqueue(&pp)
		}
	}
}

// The sysmon (system-monitor) thread.
//
// This function handled like an anonymous function.
// Therefore the compiler will add *unsafe argument to
// the head of the argument list implicitly for anonymous function context.
// The operating system thread API passes a pointer to the function when calling it.
// In the final stage, this function's signature fits with the OS thread API.
fn sysmon() {
	for {
		// Sleep for the next period.
		threadsleep(_Millisecond * 10)
		// eventpoll if not polled for more than 10ms and
		// there is a coroutine waiting for eventpoll.
		// If eventpoll is not triggered, all coroutines may accumulate in
		// local and global queues, leading to starvation.
		// The eventpoll must be checked regularly.
		if sched.ncpolling() > 0 {
			lastpoll := atomic::Load(&sched.lastpoll, atomic::Acquire)
			if lastpoll >= 0 && nanotime()-lastpoll > _Millisecond*10 {
				atomic::CompareAndSwap(&sched.lastpoll, lastpoll, pollCheckRequested, atomic::AcqRel, atomic::Relaxed)
			}
		}
	}
}

// Spawns a M with the P.
// The thread-mutex must be acquired before the call, it will release before return.
#disable nilptr
fn startm(mut pp: &p) {
	countm++
	// We have a cached M. Notify it.
	if cachedm > 0 {
		mut t := pushNewThread(roleM) // It will return a cached thread instance.
		t.pp = pp
		cachedm--
		threadMutex.unlock()
		t.parker.unpark()
		ret
	}
	// Spawn a new one.
	unsafe { threadSpawn(roleM, pp, (*unsafe)(uintptr(schedthread)), nil) }
}

// Stops the m, but it may be cached.
#disable nilptr
fn stopm(mut m: &thread): (schedAgain: bool) {
	// Thread have a P.
	// We have to put back it to idle processes.
	if m.pp != nil {
		for {
			ok, _, full := sched.idlepps.enqueue(&m.pp)
			if !ok && full {
				panic("malformed internal schedule queue")
			}
			if ok {
				break
			}
		}
	}

	threadMutex.lock()
	countm--
	if countm <= 0 {
		panic("runtime: all coroutines are asleep - deadlock!")
	}
	// We already cached a lot M.
	// Do not cache it, but keeps it if it is the main thread of the program.
	if m != threadHead && cachedm >= COMAXPROCS() {
		removeThread(m)
		m = nil
		threadMutex.unlock()
		ret false
	}
	// There is space for caching, cache it.
	// Park the thread and report true, it will be continue to run when notified.
	cachedm++
	m.pp = nil
	m.state = threadClosed
	threadMutex.unlock()
	m.parker.park()
	ret true
}
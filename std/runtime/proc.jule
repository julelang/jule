// Copyright 2025 The Jule Project Contributors. All rights reserved.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

// SCHEDULER
//
// This scheduler implements a *cooperative*, *single-threadâ€“owned* coroutine
// execution loop for the Jule runtime. It is designed as a low-level runtime
// component, not a user-facing abstraction.
//
// The scheduler operates similarly to an event loop: it repeatedly polls a
// runnable coroutine, resumes it, and re-integrates it into the system based on
// its post-resume state.
//
// The scheduler does NOT implement coroutines itself. Instead, it operates on
// opaque coroutine handles provided by the runtime ABI.
//
// The scheduler is designed according to the C:M:P model:
//
//	C (Coroutine)
//		A suspendable state machine generated by the compiler.
//		Conceptually, it is an async function, but it is not part of the user's
//		ordinary control flow. It is detached and it may execute concurrently at
//		any time, and its scheduling is fully managed by the scheduler.
//		It behaves similarly to a typical thread.
//
//	M (Machine)
//		A real operating system thread.
//		It is responsible for executing a coroutine.
//		Only as many M instances may be created as permitted by COMAXPROCS.
//
//	P (Processor)
//		A scheduler processor. It owns its own local state.
//		An M must be paired with a P in order to execute a coroutine and perform
//		scheduling. An M can be paired with only one P at a time, and a P can be
//		paired with only one M at a time.
//
// Async functions may call other async functions and, through an await chain,
// form a chain of state machines. The scheduler does not take responsibility
// for lifetime management of each individual state machine. It is only
// responsible for managing the lifetime of coroutines (detached state machines).
//
// A coroutine may only be resumed by the scheduler.
// When a parked coroutine is woken up, it must be enqueued into the runnable
// coroutine queue. The scheduler decides when it will actually execute.

use "std/internal/runtime/atomic"

extern fn __jule_retireDrain()
extern fn __jule_trampolineRun()

// Returns the maximum number of CPUs that can be executing simultaneously.
fn COMAXPROCS(): int { ret numcpu }

const (
	// The maximum count of the runnable coroutine queue of a coroutine processor.
	// Must be power of two.
	coroprocRunQueueSize = 256
)

// A coroutine processor.
struct coroproc {
	// Prioritized coroutine, should run at next scheduler loop.
	runnext: &coro

	// List of the runnable coroutines.
	runq: deque

	// Poll descriptors that are still alive.
	pdsmu: mutex
	pds:   polldescq

	// Local timers of the coroutine processor.
	timers: timerheap
}

// A scheduler envrionment for the whole Jule program.
struct scheduler {
	// All coroutine processors.
	procs: []&coroproc

	// Idle coroutine processors of the scheduler.
	idleprocs: mpmcQueue[&coroproc]

	// Global queue for runnable coroutines with guard.
	runqmu: mutex
	runq:   &coroq

	// Count of the coroutines actively polling for I/O.
	// Atomic if used with relevant scheduler functions.
	_ncpolling: int

	// Count of the worker threads actively polling for a coroutine.
	// Atomic if used with relevant scheduler functions.
	_nmspinning: int
}

impl scheduler {
	// A coroutine should call this before polling for I/O.
	fn enterpoll(mut *self) {
		atomic::Add(&self._ncpolling, 1, atomic::Relaxed)
	}

	// A coroutine should call this after polling for I/O.
	fn exitpoll(mut *self) {
		atomic::Add(&self._ncpolling, -1, atomic::Relaxed)
	}

	// Count of the coroutines actively polling for I/O.
	fn ncpolling(*self): int {
		ret atomic::Load(&self._ncpolling, atomic::Relaxed)
	}

	// A worker thread should call this before polling for a corouitine.
	fn enterspin(mut *self) {
		atomic::Add(&self._nmspinning, 1, atomic::Relaxed)
	}

	// A worker thread should call this after polled a corouitine.
	fn exitspin(mut *self) {
		atomic::Add(&self._nmspinning, -1, atomic::Relaxed)
	}

	// Returns count of the worker threads actively polling for a coroutine.
	fn nmspinning(*self): int {
		ret atomic::Load(&self._nmspinning, atomic::Relaxed)
	}
}

// A global scheduler instance of the program.
// Just a declaration, compiler will not initialize it by default.
// It will be initalized by the runtime.
let mut gsched = scheduler{}

// Should be called when a corotuine enqueued to the global runnable queue.
#disable nilptr
fn onglobalrunqput() {
	// There is not actively spinning worker thread.
	// Try to wake or create worker thread, if an idle coroutine processor available.
	if gsched.nmspinning() < 1 {
		let mut rproc: &coroproc
		ok, _ := gsched.idleprocs.dequeue(&rproc)
		if ok {
			// We found an idle coroutine processor.
			// Wake a thread or spawn a new one with the coroutine processor.
			threadMutex.lock()
			spawnSchedThread(rproc)
			ret
		}
	}
}

// Tries to put coroutine c to the
// runnable coroutine queue of the current processor.
#disable nilptr boundary
fn runqputl(mut &c: *&coro): (ok: bool) {
	mut t := gett()
	ok = t.proc.runq.push(*c)
	if ok {
		ret
	}
LoadBalance:
	// Load balancing.
	// Move half of the local queue to the global queue, if possible.
	let mut batch: [coroprocRunQueueSize / 2]&coro
	mut n := 0
	for n < len(batch); n++ {
		batch[n], ok = t.proc.runq.pop()
		if !ok {
			break
		}
	}
	gsched.runqmu.lock()
	mut i := 0
	for i < n; i++ {
		gsched.runq.push(batch[i])
	}
	gsched.runqmu.unlock()

	// The queue is not full, now the enqueue must succeed.
	ok = t.proc.runq.push(*c)
	if !ok {
		goto LoadBalance
	}

	onglobalrunqput()
	ret
}

// Puts coroutine c to the runnable coroutine queue.
// If next is true, runnext field of the current processor is prioritized.
#disable nilptr
fn runqput(mut &c: *&coro, next: bool) {
	// We have a coroutine processor on the current worker thread.
	mut t := gett()
	if t != nil && t.proc != nil {
		if t.proc.runnext == nil {
			// The runnext slot is empty, use it.
			t.proc.runnext = *c
			ret
		} else if next && runqputl(&t.proc.runnext) {
			// The runnext slot is not empty, but it is prioritized.
			// So we tried moving it to local queue and done.
			// Now we can use the runnext slot.
			t.proc.runnext = *c
			ret
		}
		// The runnext slot is full, try the local queue.
		if runqputl(c) {
			ret
		}
	}

	// No other option, push it directly to the global queue.
	gsched.runqmu.lock()
	gsched.runq.push(*c)
	gsched.runqmu.unlock()

	onglobalrunqput()
}

// A coroutine scheduler routine.
// Works like and event-loop, looks for a coroutine and execues it.
#disable nilptr
fn sched() {
	sett(acquireThread())
	threadMutex.unlock()
	mut t := gett()
Sched:
	// If poll is set, attempt a non-blocking eventpoll first.
	mut poll := false
	for {
		// Poll a coroutine to run.
		mut c := coropoll(&poll)
		// coropoll failed to find a runnable coroutine.
		if c == nil {
			// There are timers in this worker, clean them.
			// Normally, eventpoll returns a timer if exist.
			// Somehow, a spurious wakeup occured.
			if t.proc.timers.len() > 0 {
				continue
			}
			// There are coroutines polling for I/O.
			if gsched.ncpolling() > 0 {
				continue
			}
			// No coroutine to run, no timer, not poll.
			// Nothing to do, break the loop.
			// Park or destroy the worker thread.
			break
		}

		// This coroutine is done, poll a new one.
		if done(c) {
			continue
		}

		t.coro = c
		c.state &= ^reasonSched // Remove sched-reason flag, if exist.
		c.state |= coroRunning
		resume(c)
		trampolinerun()
		retiredrain() // Destroy the retire coroutines.
		t.coro = nil

		// If coropoll return true for poll, save it.
		// Otherwise make it true if coroutine is yield
		// because of scheduler starvation risk.
		poll = poll || c.state&reasonSched == reasonSched
	}
	schedAgain := closeThread(t)
	if schedAgain {
		// We must have a coroutine processor by waker.
		goto Sched
	}
}

// Start point of a scheduler thread.
//
// This function handled like an anonymous function.
// Therefore the compiler will add *unsafe argument to
// the head of the argument list implicitly for anonymous function context.
// The operating system thread API passes a pointer to the function when calling it.
// In the final stage, this function's signature fits with the OS thread API.
fn schedthread() {
	sched()
}

// Suspends the current coroutine and yields the CPU.
// If the mu is not zero, assumes it already locked and releases before yield.
// The coroutine can be made runnable again by calling [unpark].
//
// The coroutine rc is the resume coroutine.
// Compiler will write the handle to it, resume(rc) must be used to resume.
//
// This function will compile inlinely by the compiler.
// The function must implement additional behavior before the park.
// Compiler will park the coroutine at the placeholder comment.
//
// The first parameters must be the resume-coroutine,
// where park handle will be written, and mutex address.
// Following parameters may be change.
#disable nilptr
async fn park(mut rc: &coro, mu: uintptr, mut reason: u32) {
	mut c := gett().coro
	c.state |= coroSuspended | reason
	// park(c)
}

// Makes coroutine c runnable.
#disable nilptr
fn unpark(mut &c: *&coro) {
	(*c).state &= ^reasonMask
	runqput(c, true)
}

// Puts the current coroutine sleep for duration dur.
#disable nilptr
async fn sleep(dur: sleepDuration) {
	mut t := gett()
	mut timer := new(timer)
	timer.coro = t.coro
	timer.when = nanotime() + dur
	t.proc.timers.push(timer)
	park(t.coro, 0, reasonSleep).await
}

// Destroys all retired coroutine frames for the current worker thread.
fn retiredrain() {
	extern.__jule_retireDrain()
}

// Run all queued coroutines until the queue is empty.
// This must be called by the scheduler to make progress after resume.
fn trampolinerun() {
	extern.__jule_trampolineRun()
}

// Yields the processor, allowing other coroutines to run. It does not
// suspend the current coroutine, so execution resumes automatically.
async fn Yield() {
	// Push the coroutine to the global queue, otherwise the next
	// scheduler iteration after park may run it from the local queue.
	mut coro := gett().coro
	gsched.runqmu.lock()
	gsched.runq.push(coro)
	// Do not call onglobalrunqput here.
	// We are doing this out of necessity because
	// when it gets stuck in the local queue,
	// it can be immediately retrieved and resumed.
	// We need to prevent that. This is not an actual overflow.
	park(coro, uintptr(&gsched.runqmu), reasonSched).await
}

#disable nilptr boundary
fn schedinit() {
	// Initialize the global scheduler instance.
	gsched = scheduler{}
	gsched.runq = new(coroq)

	// We mutate gsched here but mutex is not need to be acquired.
	// Becase we are in the program initialize state, no concurreny risk.

	// Initialize corotuine processors.
	maxprocs := u64(COMAXPROCS())
	gsched.procs = make([]&coroproc, maxprocs)
	gsched.idleprocs = mpmcQueue[&coroproc].new(maxprocs)
	mut i := u64(0)
	for i < maxprocs; i++ {
		mut p := new(coroproc)
		p.runq.init(coroprocRunQueueSize)
		gsched.procs[i] = p
		if i == 0 {
			// Set proc of the main program thread.
			threadHead.proc = p
		} else {
			gsched.idleprocs.enqueue(&p)
		}
	}
}
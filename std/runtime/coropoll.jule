// Copyright 2025 The Jule Project Contributors. All rights reserved.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

use "std/internal/runtime/atomic"

// Selects a runnable coroutine for execution.
//
// This function is one of the core components of the coroutine scheduler.
// It attempts to find exactly one coroutine to run,
// following a strict priority order:
//
// The function guarantees that at most one coroutine is returned via
// `polled`; any additional runnable coroutines are enqueued back into
// appropriate run queues.
//
// Invariants:
//	- eventpoll() never returns more entries than a local run queue can hold.
//	- Queue corruption is considered a fatal runtime error.
//
// The `poll` flag is a scheduler hint used to amortize eventpoll costs
// under high I/O traffic.
//
// Assumes that the calling thread is counted as spinning.
// It will exit the spinning state before return.
#disable nilptr boundary
fn coropoll(): (polled: coro, ok: bool) {
	mut t := gett()

	ok = runqget(t.proc, &polled)
	if ok {
		sched.exitspin()
		ret polled, true
	}

	// Before the look for global runq, check for the lastpoll enforcement.
	// If the lastpoll value is -1, check eventpoll.
	if atomic::CompareAndSwap(&sched.lastpoll, pollCheckRequested, pollCheckIn, atomic::AcqRel, atomic::Relaxed) {
		mut toRun := eventpoll(0) // Use 0-delay to perform a non-blocking poll.
		now := nanotime()
		atomic::Store(&sched.lastpoll, now, atomic::Release)
		// If any coroutines are ready, return the first and enqueue the rest.
		// Timers possibly expired here will be handled in a later iteration.
		if toRun.len > 0 {
			sched.exitspin()
			injecteventpollrunq(t.proc, &polled, &toRun)
			ret polled, true
		}
	}

	// Local queue empty: attempt to fetch a batch from the global run queue.
	//
	// The batch size is capped to prunqsize to preserve
	// locality and reduce contention on the global lock.
	sched.runqmu.lock()
	if sched.runq.head != nil {
		let mut batch: [prunqsize]coro
		mut n := u32(0)

		for n < prunqsize; n++ {
			if sched.runq.head == nil {
				break
			}
			batch[n] = sched.runq.head.coro
			sched.runq.remove(sched.runq.head)
		}
		sched.runqmu.unlock()

		if n > 0 {
			sched.exitspin()
			polled = batch[0]
			runqputbatch(t.proc, &batch, 1, n)
			ret polled, true
		}
	} else {
		sched.runqmu.unlock()
	}

	// No work found locally or globally: attempt work stealing.
	//
	// Stealing is limited to a small fixed number of attempts to
	// avoid excessive contention and cache thrashing.
	ok = stealWork(t.proc, &polled)
	sched.exitspin()
	if ok {
		ret polled, true
	}

	// No runnable coroutine found yet.
	// Determine how long we can block based on timers and deadlines.
	// The default is -1, which is blocks indefinitely.
	mut delay := i64(-1)

	mut now := nanotime()
	mut iodeadline := false // Indicates presence of expired I/O deadlines.

	// Processor local timers.
	//
	// Expired timers immediately make their coroutines runnable.
	// The first such coroutine is returned; others are enqueued.
	mut n := 0
	if t.proc.timers.peek() != nil {
		ok = false
		for n < prunqsize {
			mut timer := t.proc.timers.peek()
			if timer == nil || timer.when > now {
				break
			}
			mut data := t.proc.timers.pop().data
			if data.used {
				if n == 0 {
					polled = data.coro
					ok = true
				} else {
					runqput(t.proc, &data.coro, false)
				}
			} else {
				// Timer corresponds to an I/O deadline.
				iodeadline = true
			}
			n++
		}

		// If we already found a runnable coroutine, return immediately
		// without entering eventpoll.
		if ok {
			ret polled, true
		}

		// Compute delay until the next timer fires.
		soonest := t.proc.timers.peek()
		if soonest != nil {
			delay = soonest.when - now
			if delay <= 0 {
				panic("runtime: inconsistent timer state")
			}
		}
	}

	// Handle expired I/O deadlines.
	//
	// Coroutines blocked on I/O with an associated deadline
	// are made runnable once their deadline expires.
	//
	// Coroutines are NOT removed from pd here.
	// The coroutine itself is responsible for verifying whether it is still
	// associated with this pd upon wakeup. If the association is unchanged,
	// the wakeup indicates a deadline-exceeded condition; otherwise, the coroutine
	// was resumed for another reason (e.g. notified by eventpoll).
	if iodeadline && t.proc.pds.head != nil {
		t.proc.pdsmu.lock()
		mut q := t.proc.pds.head
		for {
			if n >= prunqsize || q == nil {
				break
			}
			mut pd := q.pd
			pd.mu.lock()

			if pd.rdeadline > 0 && pd.rdeadline <= now && pd.rcoro != nil && pd.rcoro != pollrace {
				runqput(t.proc, &(*pd.rcoro), false)
				n++
			}
			if pd.wdeadline > 0 && pd.wdeadline <= now && pd.wcoro != nil && pd.wcoro != pollrace {
				runqput(t.proc, &(*pd.wcoro), false)
				n++
			}
			if pd.deadline > 0 && pd.deadline <= now {
				if pd.rcoro != nil && pd.rcoro != pollrace {
					runqput(t.proc, &(*pd.rcoro), false)
					n++
				}
				if pd.wcoro != nil && pd.wcoro != pollrace {
					runqput(t.proc, &(*pd.wcoro), false)
					n++
				}
			}
			pd.mu.unlock()
			q = q.next
		}
		t.proc.pdsmu.unlock()

		ok = runqget(t.proc, &polled)
		if !ok {
			panic("runtime: coroutine scheduler queue corruption")
		}
		ret polled, true
	}

	// Final step: poll eventpoll.
	if delay != -1 || sched.ncpolling() > 0 && atomic::Swap(&sched.lastpoll, pollCheckIn, atomic::AcqRel) != pollCheckIn {
		mut toRun := eventpoll(delay)
		now = nanotime()
		atomic::Store(&sched.lastpoll, now, atomic::Release)
		if toRun.len > 0 {
			injecteventpollrunq(t.proc, &polled, &toRun)
			ret polled, true
		}
	}

	ret polled, false
}

// Dispatches a coroq returned from eventpoll.
// Sets polled to a runnable coroutine from toRun.
fn injecteventpollrunq(mut p: &coroproc, mut &polled: *coro, mut &toRun: *coroq) {
	if toRun.len < 1 {
		ret
	}
	// eventpoll guarantees len(toRun) <= local run queue capacity.
	// If P's local queue is empty, all corotoutines will be enqueued in there.
	*polled = toRun.head.coro
	toRun.remove(toRun.head)
	for toRun.head != nil {
		runqput(p, &toRun.head.coro, false)
		toRun.remove(toRun.head)
	}
}
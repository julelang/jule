// Copyright 2025 The Jule Project Contributors. All rights reserved.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

// Selects a runnable coroutine for execution.
//
// This function is one of the core components of the coroutine scheduler.
// It attempts to find exactly one coroutine to run,
// following a strict priority order:
//
// The function guarantees that at most one coroutine is returned via
// `polled`; any additional runnable coroutines are enqueued back into
// appropriate run queues.
//
// Invariants:
//	- eventpoll() never returns more entries than a local run queue can hold.
//	- Queue corruption is considered a fatal runtime error.
//
// The `poll` flag is a scheduler hint used to amortize eventpoll costs
// under high I/O traffic.
#disable nilptr boundary
fn coropoll(mut &poll: *bool): (polled: &coro) {
	// Fast-path: if poll is set, attempt a non-blocking eventpoll first.
	//
	// This is used to avoid starvation under high I/O load by allowing
	// the scheduler to re-check pending events on the next iteration.
	if *poll {
		// Clear the flag immediately; this attempt is sufficient.
		*poll = false

		// Use 0-delay to perform a non-blocking poll.
		mut toRun := eventpoll(0)

		// If any coroutines are ready, return the first and enqueue the rest.
		// Timers possibly expired here will be handled in a later iteration.
		if toRun.len > 0 {
			polled = toRun.head.coro
			toRun.remove(toRun.head)
			for toRun.head != nil {
				runqput(&toRun.head.coro, false)
				toRun.remove(toRun.head)
			}
			ret
		}
	}

	mut t := gett()
	gsched.enterspin()

	// Highest priority: runnext slot.
	//
	// This is used for coroutine handoff and latency-sensitive scheduling.
	if t.proc.runnext != nil {
		polled = t.proc.runnext
		t.proc.runnext = nil
		gsched.exitspin()
		ret
	}

	// Second priority: local run queue.
	let mut ok: bool
	polled, ok = t.proc.runq.pop()
	if ok {
		gsched.exitspin()
		ret
	}

	// Local queue empty: attempt to fetch a batch from the global run queue.
	//
	// The batch size is capped to coroprocRunQueueSize to preserve
	// locality and reduce contention on the global lock.
	gsched.runqmu.lock()
	if gsched.runq.head != nil {
		let mut batch: [coroprocRunQueueSize]&coro
		mut n := 0

		for n < len(batch); n++ {
			if gsched.runq.head == nil {
				break
			}
			batch[n] = gsched.runq.head.coro
			gsched.runq.remove(gsched.runq.head)
		}
		gsched.runqmu.unlock()

		if n > 0 {
			gsched.exitspin()
			polled = batch[0]

			// Push remaining coroutines into the local run queue.
			mut i := 1
			for i < n; i++ {
				ok = t.proc.runq.push(batch[i])
				if !ok {
					panic("runtime: coroutine scheduler queue corruption")
				}
			}
			ret
		}
	} else {
		gsched.runqmu.unlock()
	}

	// No work found locally or globally: attempt work stealing.
	//
	// Stealing is limited to a small fixed number of attempts to
	// avoid excessive contention and cache thrashing.
	const StealTries = 4
	mut n := 0
	for n < StealTries; n++ {
		idx := absint(int(rand() % u64(len(gsched.procs))))
		mut proc := gsched.procs[idx]

		// Do not steal from ourselves.
		if proc == t.proc {
			continue
		}

		polled, ok = proc.runq.steal()
		if ok {
			gsched.exitspin()
			ret
		}
	}
	gsched.exitspin()

	// No runnable coroutine found yet.
	// Determine how long we can block based on timers and deadlines.
	// The default is -1, which is blocks indefinitely.
	mut delay := i64(-1)

	mut now := nanotime()
	mut iodeadline := false // Indicates presence of expired I/O deadlines.

	// Processor local timers.
	//
	// Expired timers immediately make their coroutines runnable.
	// The first such coroutine is returned; others are enqueued.
	if t.proc.timers.peek() != nil {
		n = 0
		for n < coroprocRunQueueSize {
			mut timer := t.proc.timers.peek()
			if timer == nil || timer.when > now {
				break
			}
			mut coro := t.proc.timers.pop().coro
			if coro != nil {
				if polled == nil {
					polled = coro
				} else {
					ok = t.proc.runq.push(coro)
					if !ok {
						panic("runtime: coroutine scheduler queue corruption")
					}
				}
			} else {
				// Timer corresponds to an I/O deadline.
				iodeadline = true
			}
			n++
		}

		// If we already found a runnable coroutine, return immediately
		// without entering event polling.
		if polled != nil {
			ret
		}

		// Compute delay until the next timer fires.
		soonest := t.proc.timers.peek()
		if soonest != nil {
			delay = soonest.when - now
			if delay <= 0 {
				panic("runtime: inconsistent timer state")
			}
		}
	}

	// Handle expired I/O deadlines.
	//
	// Waiting coroutines with read/write deadlines are made runnable
	// when their deadlines expire.
	if iodeadline && t.proc.pds.head != nil {
		t.proc.pdsmu.lock()
		mut q := t.proc.pds.head
		for {
			if n >= coroprocRunQueueSize || q == nil {
				break
			}
			mut pd := q.pd
			pd.mu.lock()

			if pd.rdeadline > 0 && pd.rdeadline <= now && pd.rcoro != nil {
				ok = t.proc.runq.push(pd.rcoro)
				if !ok {
					panic("runtime: coroutine scheduler queue corruption")
				}
				n++
			}
			if pd.wdeadline > 0 && pd.wdeadline <= now && pd.wcoro != nil {
				ok = t.proc.runq.push(pd.wcoro)
				if !ok {
					panic("runtime: coroutine scheduler queue corruption")
				}
				n++
			}
			if pd.deadline > 0 && pd.deadline <= now {
				if pd.rcoro != nil {
					ok = t.proc.runq.push(pd.rcoro)
					if !ok {
						panic("runtime: coroutine scheduler queue corruption")
					}
					n++
				}
				if pd.wcoro != nil {
					ok = t.proc.runq.push(pd.wcoro)
					if !ok {
						panic("runtime: coroutine scheduler queue corruption")
					}
					n++
				}
			}
			pd.mu.unlock()
			q = q.next
		}
		t.proc.pdsmu.unlock()

		polled, ok = t.proc.runq.pop()
		if !ok {
			panic("runtime: coroutine scheduler queue corruption")
		}
		ret
	}

	// Final step: perform eventpoll.
	mut toRun := eventpoll(delay)

	if toRun.len > 0 {
		// Set poll flag so the next scheduler iteration performs
		// a fast non-blocking eventpoll. This is critical under
		// sustained high I/O load.
		*poll = true

		// eventpoll guarantees len(toRun) <= local run queue capacity.
		polled = toRun.head.coro
		toRun.remove(toRun.head)
		for toRun.head != nil {
			ok = t.proc.runq.push(toRun.head.coro)
			if !ok {
				panic("runtime: coroutine scheduler queue corruption")
			}
			toRun.remove(toRun.head)
		}
	}

	ret
}
// Copyright 2025 The Jule Project Contributors. All rights reserved.
// Use of this source code is governed by a BSD 3-Clause
// license that can be found in the LICENSE file.

use "std/internal/runtime/atomic"

// Selects a runnable coroutine for execution.
//
// This function is one of the core components of the coroutine scheduler.
// It attempts to find exactly one coroutine to run,
// following a strict priority order:
//
// The function guarantees that at most one coroutine is returned via
// `polled`; any additional runnable coroutines are enqueued back into
// appropriate run queues.
//
// Invariants:
//	- eventpoll() never returns more entries than a local run queue can hold.
//	- Queue corruption is considered a fatal runtime error.
//
// The `poll` flag is a scheduler hint used to amortize eventpoll costs
// under high I/O traffic.
#disable nilptr boundary
fn coropoll(): (polled: &coro) {
	mut t := gett()
	sched.enterspin()

	// Highest priority: runnext slot.
	//
	// This is used for coroutine handoff and latency-sensitive scheduling.
	if t.proc.runnext != nil {
		polled = t.proc.runnext
		t.proc.runnext = nil
		sched.exitspin()
		ret
	}

	// Second priority: local run queue.
	let mut ok: bool
	polled, ok = t.proc.runq.pop()
	if ok {
		sched.exitspin()
		ret
	}

	// Before the look for global runq, check for the lastpoll enforcement.
	// If the lastpoll value is -1, check eventpoll.
	if atomic::CompareAndSwap(&sched.lastpoll, pollCheckRequested, pollCheckIn, atomic::AcqRel, atomic::Relaxed) {
		mut toRun := eventpoll(0) // Use 0-delay to perform a non-blocking poll.
		now := nanotime()
		atomic::Store(&sched.lastpoll, now, atomic::Release)
		// If any coroutines are ready, return the first and enqueue the rest.
		// Timers possibly expired here will be handled in a later iteration.
		if toRun.len > 0 {
			sched.exitspin()
			injecteventpollrunq(&polled, &toRun)
			ret
		}
	}

	// Local queue empty: attempt to fetch a batch from the global run queue.
	//
	// The batch size is capped to coroprocRunQueueSize to preserve
	// locality and reduce contention on the global lock.
	sched.runqmu.lock()
	if sched.runq.head != nil {
		let mut batch: [coroprocRunQueueSize]&coro
		mut n := 0

		for n < len(batch); n++ {
			if sched.runq.head == nil {
				break
			}
			batch[n] = sched.runq.head.coro
			sched.runq.remove(sched.runq.head)
		}
		sched.runqmu.unlock()

		if n > 0 {
			sched.exitspin()
			polled = batch[0]

			// Push remaining coroutines into the local run queue.
			mut i := 1
			for i < n; i++ {
				ok = t.proc.runq.push(batch[i])
				if !ok {
					panic("runtime: coroutine scheduler queue corruption")
				}
			}
			ret
		}
	} else {
		sched.runqmu.unlock()
	}

	// No work found locally or globally: attempt work stealing.
	//
	// Stealing is limited to a small fixed number of attempts to
	// avoid excessive contention and cache thrashing.
	const StealTries = 4
	mut n := 0
	for n < StealTries; n++ {
		idx := absint(int(rand() % u64(len(sched.procs))))
		mut proc := sched.procs[idx]

		// Do not steal from ourselves.
		if proc == t.proc {
			continue
		}

		polled, ok = proc.runq.steal()
		if ok {
			sched.exitspin()
			ret
		}
	}
	sched.exitspin()

	// No runnable coroutine found yet.
	// Determine how long we can block based on timers and deadlines.
	// The default is -1, which is blocks indefinitely.
	mut delay := i64(-1)

	mut now := nanotime()
	mut iodeadline := false // Indicates presence of expired I/O deadlines.

	// Processor local timers.
	//
	// Expired timers immediately make their coroutines runnable.
	// The first such coroutine is returned; others are enqueued.
	if t.proc.timers.peek() != nil {
		n = 0
		for n < coroprocRunQueueSize {
			mut timer := t.proc.timers.peek()
			if timer == nil || timer.when > now {
				break
			}
			mut coro := t.proc.timers.pop().coro
			if coro != nil {
				if polled == nil {
					polled = coro
				} else {
					ok = t.proc.runq.push(coro)
					if !ok {
						panic("runtime: coroutine scheduler queue corruption")
					}
				}
			} else {
				// Timer corresponds to an I/O deadline.
				iodeadline = true
			}
			n++
		}

		// If we already found a runnable coroutine, return immediately
		// without entering eventpoll.
		if polled != nil {
			ret
		}

		// Compute delay until the next timer fires.
		soonest := t.proc.timers.peek()
		if soonest != nil {
			delay = soonest.when - now
			if delay <= 0 {
				panic("runtime: inconsistent timer state")
			}
		}
	}

	// Handle expired I/O deadlines.
	//
	// Coroutines blocked on I/O with an associated deadline
	// are made runnable once their deadline expires.
	//
	// Coroutines are NOT removed from pd here.
	// The coroutine itself is responsible for verifying whether it is still
	// associated with this pd upon wakeup. If the association is unchanged,
	// the wakeup indicates a deadline-exceeded condition; otherwise, the coroutine
	// was resumed for another reason (e.g. notified by eventpoll).
	if iodeadline && t.proc.pds.head != nil {
		t.proc.pdsmu.lock()
		mut q := t.proc.pds.head
		for {
			if n >= coroprocRunQueueSize || q == nil {
				break
			}
			mut pd := q.pd
			pd.mu.lock()

			if pd.rdeadline > 0 && pd.rdeadline <= now && pd.rcoro != nil && pd.rcoro != pollrace {
				ok = t.proc.runq.push(pd.rcoro)
				if !ok {
					panic("runtime: coroutine scheduler queue corruption")
				}
				n++
			}
			if pd.wdeadline > 0 && pd.wdeadline <= now && pd.wcoro != nil && pd.wcoro != pollrace {
				ok = t.proc.runq.push(pd.wcoro)
				if !ok {
					panic("runtime: coroutine scheduler queue corruption")
				}
				n++
			}
			if pd.deadline > 0 && pd.deadline <= now {
				if pd.rcoro != nil && pd.rcoro != pollrace {
					ok = t.proc.runq.push(pd.rcoro)
					if !ok {
						panic("runtime: coroutine scheduler queue corruption")
					}
					n++
				}
				if pd.wcoro != nil && pd.wcoro != pollrace {
					ok = t.proc.runq.push(pd.wcoro)
					if !ok {
						panic("runtime: coroutine scheduler queue corruption")
					}
					n++
				}
			}
			pd.mu.unlock()
			q = q.next
		}
		t.proc.pdsmu.unlock()

		polled, ok = t.proc.runq.pop()
		if !ok {
			panic("runtime: coroutine scheduler queue corruption")
		}
		ret
	}

	// Final step: poll eventpoll.
	if delay != -1 || atomic::Swap(&sched.lastpoll, pollCheckIn, atomic::AcqRel) != pollCheckIn {
		mut toRun := eventpoll(delay)
		now = nanotime()
		atomic::Store(&sched.lastpoll, now, atomic::Release)
		if toRun.len > 0 {
			injecteventpollrunq(&polled, &toRun)
		}
	}

	ret
}

// Dispatches a coroq returned from eventpoll.
// Sets polled to a runnable coroutine from toRun.
fn injecteventpollrunq(mut &polled: *&coro, mut &toRun: *coroq) {
	if toRun.len < 1 {
		ret
	}
	// eventpoll guarantees len(toRun) <= local run queue capacity.
	// If P's local queue is empty, all corotoutines will be enqueued in there.
	*polled = toRun.head.coro
	toRun.remove(toRun.head)
	for toRun.head != nil {
		runqput(&toRun.head.coro, false)
		toRun.remove(toRun.head)
	}
}